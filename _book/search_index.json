[["index.html", "Data Analysis in Medicine and Health using R Preface", " Data Analysis in Medicine and Health using R Kamarul Imran, Wan Nor Arifin, Tengku Muhammad Hanis Tengku Mukhtar 2022-01-11 Preface We write this book to help new R users who have limited programming and statistical background. The main audience for this book will be medical epidemiologists, public health researchers and analysts and post-graduate students in public health and epidemiology. The book aims to help R users to quickly able to make plots, explore data and to perform regression analysis using the R programming language in RStudio IDE setting. We used Rmarkdown to write this book using the bookdown package and because of that we are entirely grateful for people who have developed this platform at RStudio and outside RStudio. All in all, we hope you enjoy this book!   Kamarul Imran Musa   Wan Nor Arifin Wan Mansor   Tengku Muhammad Hanis Tengku Mokhtar   School of Medical Sciences, Universiti Sains Malaysia 2022-01-11 "],["introduction-to-r-rstudio-and-rstudio-cloud.html", "Chapter 1 Introduction to R, RStudio and RStudio Cloud 1.1 RStudio Cloud 1.2 Point and click R GUI 1.3 RStudio Server 1.4 Installing R and RStudio on Your Local Machine 1.5 Starting your RStudio", " Chapter 1 Introduction to R, RStudio and RStudio Cloud In this chapter, we would like to achieve these objectives: To introduce R To introduce RStudio To introduce RStudio Cloud. This is a platform where we can run RStudio on the cloud To guide to install R in your local machine To guide to install RStudio in your local machine To show readers to install optional; latex editor (Miktex or Texlive and MacTex) To demonstrate to readers how R scripts work To describe R packages and R Taskview 1.1 RStudio Cloud RStudio cloud facilitates the learning of R. Anyone can sign up and start using RStudio on the cloud.It is one of the the quickest way to learn R. By using RStudio Cloud, we do not need to install R on our local machine. RStudio Cloud also allows collaboration between R teachers and students. It also helps colleagues working together on R codes. On its webpage, RStudio is described as a lightweight, cloud-based solution that allows anyone to do, share, teach and learn data science online. And it also adds that by using this exciting platform, it means we can analyze our data using the RStudio IDE, directly from our browser. share projects with our team, class, workshop or the world. teach data science with R to our students or colleagues. learn data science in an instructor-led environment or with interactive tutorials. RStudio Cloud has the free and the commercial version (which is fortunately very affordable). To start using RStudio Cloud, visit https://rstudio.cloud/. On the page, click Sign Up. Sign Up Page for RStudio Cloud With RStudio Cloud, there is almost nothing to configure and you do not need dedicated hardware, installation or annual purchase contract required. Individual users, instructors and students only need a browser to do, share, teach and learn data science. 1.1.1 The RStudio Cloud Registration This is the registration and login webpage for RStudio Cloud. RStudio Cloud webpage 1.1.2 Register and log in You can register now. After registration, you can log in. RStudio Cloud Registration 1.2 Point and click R GUI There are a number of the so-called SPSS-like GUI for R. For example Bluesky statistics https://www.blueskystatistics.com/ JAMOVI - https://www.jamovi.org/ This is the Bluesky statistics software ] And this is jamovi software jamovi software jamovi is an interesting software. It is a new 3rd generation statistical spreadsheet. It is designed from the ground up to be easy to use, it is a compelling alternative to costly statistical products such as SPSS and SAS. jamovi is built on top of the R statistical language, giving you access to the best the statistics community has to offer. jamovi will always be free and open because jamovi is made by the scientific community, for the scientific community. 1.3 RStudio Server You can run R and RStudio on the server. To do this you have to install RStudio Server. By having RStudio Server, it enables you to do analysis on the server. Using RStudio server can give you a taste of working on BIG DATA ON THE CLOUD. There are two versions of RStudio Server RStudio Server: This is the Open Source edition RStudio Workbench: This is the Professional edition. At our medical school. we have RStudio Server Professional Edition (courtesy of RStudio, of course) running on our server here https://healthdata.usm.my/rstudio/auth-sign-in 1.4 Installing R and RStudio on Your Local Machine To install R on your local machine, you have to have Admin Right to your machine. We recommend that you install R first, then RStudio 1.4.1 Installing R Though you can use the native R software (that you just installed) to run R codes, we highly encourage you to use RStudio Integrated Desktop Environment (IDE). We will show this step by step. First, let us install R on your machine. To install R, go to cran. Then choose the R version thats correct for your machine OS. For example, for Windows OS the link is https://cran.r-project.org/bin/windows/base/R-3.6.1-win.exe. And for Mac OS, the download link is https://cran.r-project.org/bin/macosx/R-3.6.1.pkg. Similarly, if you are using Linux, follow the steps as listed before. CRAN website It is always recommended that you install the latest version of R. And you can have multiple R version on the same local machines. So you do not need to uninstall the old R version in order to install a new R version. 1.4.2 Installing RStudio IDE Now, to install RStudio IDE, go here https://www.rstudio.com/products/rstudio/download/#download. Choose the supported platforms correct for your machine OS. The size of download will be around 90-110 MB. RStudio website 1.4.3 Checking R and RStudio Installations Now, we assume you have installed both R and RStudio. To make sure they work perfectly (or at least for the first time), check: Does your machine can load R? Depending on your OS, go and start R. what version of R do you have? When R loads, look for the version of R. Do you have RStudio? Depending on your OS, go and start RStudio. what version of RStudio do you have? When RStudio loads, look for the version of R. If you have multiple R version, you can choose the R version of your choice by going to Tools then Global Options then General Do you need to update R and RStudio? By knowing the versions of R and RStudio, now you know if you need to update both or one of them. 1.4.4 Installation of MiKTeX, TeXLive and MacTex It is necessary to install Latex editor if you want to convert the outputs you generated in R into PDF format. But if you do not need to produce PDF document, then you do not have to install it. Based on experience, as you go along, you may find it is very attractive to convert your analysis into PDF document. And because of that, you need to install the Latex editor. ] This is MiKTeX, for Window OS MikTeX webpage And this is MacTeX, for Mac OS MacTeX webpage 1.5 Starting your RStudio You can either login to RStudio Cloud and automatically see the RStudio interface OR you can start RStudio on your local machine by loading it. Remember, to login to RStudio Cloud, go to https://rstudio.cloud. You will be asked for your username and password. Click this link https://rstudio.cloud/spaces/156361/join?access_code=WtlSxNuTm%2Fz7E%2BLb%2FW2XnOw480%2BBTmL4B%2FqjYRIg RStudio Cloud space for this book To start R on your machine, and if you are using Windows, find the Rstudio program in your start bar in your machine. And start it. You will see an interface like below. This is definitely different with what you see on your screen because I am using the Vibrant Ink Theme. To choose the theme of your choice, click Global Options then, click Apperance. There are a number of themes available for you to choose. Rstudio Interface with Vibrant Ink theme What you see on RStudio now? You should see three panes if you start Rstudio for the first time or four panes if you have used RStudio before. RStudio Panes 1.5.1 Console tab In Console tab, this is where we will see most of the results generated from codes in RStudio. Console Pane 1.5.2 Files, Plots, Packages, Help and Viewer Pane In this console, you will see List of objects (Remember, R is an object-oriented-programming or oop) R files, datasets, tables, list etc File, Plots and Viewer Pane 1.5.3 Environment, History, Connection and Build Pane In the environment, history, connection and build pane, you will see this Environment Pane 1.5.4 Source Pane In the Source pane, you can create R files and write your R codes Source Pane "],["r-scripts-and-r-packages.html", "Chapter 2 R Scripts and R Packages 2.1 Open a new R script 2.2 Packages 2.3 Directory 2.4 Upload data to RStudio Cloud 2.5 More resources on RStudio Cloud 2.6 Need help? 2.7 Bookdown", " Chapter 2 R Scripts and R Packages 2.1 Open a new R script For beginner, you may start by writing some simple codes. To do this, go to File, then click R Script File -&gt; R Script In Window OS, CTRL-SHIFT-N New R Script 2.1.1 Our first R script Let us write our very first R codes inside an R script. In Line 1, type 2 + 3 click CTRL-ENTER or CMD-ENTER see the outputs in the Console Pane 2 + 3 ## [1] 5 After writing your codes inside the R script, you can save the R script file. This will allow you to open it up again to continue your work. And to save R script, go to File -&gt; Save As -&gt; Choose folder -&gt; Name the file Now, types this to check the version of R version[6:7] ## _ ## major 4 ## minor 1.1 The current version for R is 4, 1.1 If you lower version, then you want to upgrade. To upgrade for Windows, you can use installr package for Mac OS, you can use some functions More info here https://www.linkedin.com/pulse/3-methods-update-r-rstudio-windows-mac-woratana-ngarmtrakulchol/ 2.1.2 Function, Argument and Parameters R codes contain function argument parameters f &lt;- function(&lt;arguments&gt;) { ## Do something interesting } For example, for the function lm() to estimate parameters for linear regression model args(lm) ## function (formula, data, subset, weights, na.action, method = &quot;qr&quot;, ## model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, ## contrasts = NULL, offset, ...) ## NULL For example: lm(weight ~ Time, data = ChickWeight) ## ## Call: ## lm(formula = weight ~ Time, data = ChickWeight) ## ## Coefficients: ## (Intercept) Time ## 27.467 8.803 2.1.3 Need more help? Then type the ? before the function ?lm ## starting httpd help server ... done See what will be displayed in Help Pane Help Pane 2.2 Packages R is a programming language. And R software runs on packages. R packages are collections of functions and data sets developed by the community. They increase the power of R by improving existing base R functionalities, or by adding new ones. A package is a suitable way to organize your own work and, if you want to, share it with others. Typically, a package will include code (not only R code!), documentation for the package and the functions inside, some tests to check everything works as it should, and data sets.1 2.2.1 Packages on CRAN https://cran.r-project.org/ Currently, the CRAN package repository features 12784 available packages Cran Task Views Task Views 2.2.2 Check if the package you need is available in your R library Type this inside your console library(tidyverse) ## -- Attaching packages --------------------------------------- tidyverse 1.3.1 -- ## v ggplot2 3.3.5 v purrr 0.3.4 ## v tibble 3.1.6 v dplyr 1.0.7 ## v tidyr 1.1.4 v stringr 1.4.0 ## v readr 2.1.1 v forcats 0.5.1 ## Warning: package &#39;tibble&#39; was built under R version 4.1.2 ## -- Conflicts ------------------------------------------ tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() You should not receive any error message. If you have not installed the package, you will receive and error message. And it tells you that the package is not available in your R. By default the package is stored in the R folder in your My Document or HOME directory .libPaths() ## [1] &quot;C:/Users/tengk/Documents/R/win-library/4.1&quot; ## [2] &quot;C:/Program Files/R/R-4.1.1/library&quot; 2.2.3 Install an R package To install an R package, there are two ways: you can type below (without the # tag) # install.packages(tidyverse, dependencies = TRUE) using the GUI Packages pane Now, type the package you want to install. For example you want to install the tidyverse package Typing the package to install And then click the Install button. And you need to have internet access to do this. You can also install packages from: a zip file (from your machine or USB), from github repository other repository 2.3 Directory This is important. Not knowing your working directory will make you lost (you do not know where your R codes, R outputs, datasets etc) You must know where your folder is located. The folder can contain many sub folders. The folder should contain dataset (if you want to analyze your data). It will later store the objects created during R session getwd() ## [1] &quot;C:/Tengku/Sync_PC_Laptop/tulis-buku/multivariable-analysis/fork-profKIM/multivar_data_analysis&quot; You have to know to write file path. It is written differently for Window OS and other OS 2.3.1 Starting your R job There are 2 ways to start your job: create a new project (recommended) setting your working directory using setwd() (not recommended) 2.3.2 Create new project Always create a new project (This is the recommended way). This can be by Go to File -&gt; New Project [] When you see project type, click New Project Project Type 2.3.3 Where is my data? Datasets for analysis in R and usually in data frame format. You can see the datasets in the environment pane. Your data is read from the original dataset to a memory. SO you must know the size of your computer RAM. How much your RAM for your machine? The bigger the RAM, thelarger R can read and store your data. The data that is read (in memory) will dissaper once you close RStudio. But the original stays in its location. This will not change your original data (so be happy!) My Data 2.4 Upload data to RStudio Cloud You have to upload data to RStudio Cloud Or link data to dropbox folder Upload Data in RStudio Cloud 2.5 More resources on RStudio Cloud You can learn more about RStudio Cloud here on YouTube : RStudio Cloud for educationn https://www.youtube.com/watch?v=PviVimazpz8 YouTube: Working with R in Cloud https://www.youtube.com/watch?v=SFpzr21Pavg 2.6 Need help? If you need help you can Type a question mark infront of a function ?plot Other options are these: register and join RStudio Community here https://community.rstudio.com/ Ask questions on Stack Overflow https://stackoverflow.com/ Search for mailing list and subscribe to it Books on R https://bookdown.org/ 2.7 Bookdown This webpage contains many useful books that use R codes https://bookdown.org/. Bookdown https://www.datacamp.com/community/tutorials/r-packages-guide "],["rstudio-project.html", "Chapter 3 RStudio Project 3.1 Objectives 3.2 Dataset repository on GitHub 3.3 RStudio project on RStudio Cloud 3.4 RStudio project on local machine", " Chapter 3 RStudio Project In this chapter, we will guide readers to have a similar folder and file structure. This will help readers to run the codes with minimal risk for errors. And we do this by creating RStudio Project. From the RStudio Project webpage https://support.rstudio.com/hc/en-us/articles/200526207-Using-RStudio-Projects , it says that RStudio projects make it straightforward to divide your work into multiple contexts, each with their own working directory, workspace, history, and source documents.RStudio projects are associated with R working directories. You can create an RStudio project: on RStudio Cloud or RStudio on your local machine In a brand new directory In an existing directory where you already have R code and data By cloning a version control (Git or Subversion) repository 3.1 Objectives The objectives of the chapters are to share the link for the dataset repository on GitHub to teach readers to create a RStudio Cloud project based on the dataset repository on GitHub to teach readers to create a RStudio project on local machine based on the dataset repository on GitHub 3.2 Dataset repository on GitHub We will use the our book repository on GitHub. The repository is data-for-RMed. To go to the repository, click on this link Dataset Repository on GitHub 3.3 RStudio project on RStudio Cloud Head to RStudio Cloud page Login to RStudio Cloud using your credentials Once inside your workspace, click New Project Rstudio Cloud, Workspace and New Project And click on New Project from Git Repository New Project from Git Repository You will got back to our data-for-RMed repository on GitHub. And you will click on Clone and click the copy button for HTTPS Clone Git Repository to RStudio Cloud Next, we will clone the repository on our RStudio Cloud. This will ensure the file structure is the same with that of on the RStudio Cloud. Following that, just click OK. Clone the GitHub repository on RStudio Cloud project A new Rstudio Cloud project will be created. A new RStudio Cloud Project 3.4 RStudio project on local machine If you want to create a new project on your local machine using the same GitHub repository, then follow these steps. First, open the RStudio. RStudio on Your Machine On the menu, click File, then click New Project New Project Then click Project and then Version Control New Project and Version Control and then click Git Git And remember the HTTPS that we have copied from our dataset-for-RMed GitHub repository Now, paste the HTTPS link the Project directory name will be automatically filled Click on Browse, and you may choose whichever folder that you prefer. We recommend you to use home directory (such as Documents) Clone Git Repository And now RStudio will create a new working directory on your local machine. This working directory contains the same folder and file structures with the GitHub repository RStudio Project "],["data-visualization.html", "Chapter 4 Data Visualization 4.1 Introduction 4.2 History and objectives of Data Visualization 4.3 Ingredients for Good Graphics 4.4 Graphics Packages in R 4.5 The ggplot2 Package 4.6 Preparation 4.7 Basic Plots 4.8 Adding another variable 4.9 Making Subplots 4.10 Overlaying Plots 4.11 Combining Different Plots 4.12 Statistical Transformation 4.13 Customizing Title 4.14 Adjusting Axes 4.15 Choosing Themes 4.16 Saving Plots", " Chapter 4 Data Visualization 4.1 Introduction Data visualization is viewed by many disciplines as a modern equivalent of visual communication. It involves the creation and study of the visual representation of data. Data visualization requires information that has been abstracted in some schematic form, including attributes or variables for the units of information. You can read more about data visualization here https://en.m.wikipedia.org/wiki/Data_visualization and here https://en.m.wikipedia.org/wiki/Michael_Friendly In this chapter, we want to achieve these objectives: To introduce concept of data visualization To describe ingredients for good graphics To generate plots using ggplot packages To save plots in different format and settings 4.2 History and objectives of Data Visualization In his 1983 book which carried the title The Visual Display of Quantitative Information, the author Edward Tufte defines graphical displays and principles for effective graphical display. The book mentioned that Excellence in statistical graphics consists of complex ideas communicated with clarity, precision and efficiency. Visualization is the process of representing data graphically and interacting with these representations. The objective is to gain insight into the data. Some of the processes are outlined here http://researcher.watson.ibm.com/researcher/view_group.php?id=143 4.3 Ingredients for Good Graphics We require these features to make good graphics: Good data Priorities on substance rather than methodology, graphic design, the technology of graphic production or something else No distortion to what the data has to say Presence of many numbers in a small space Coherence for large data sets They encourage the eye to compare different pieces of data They reveal data at several levels of detail, from a broad overview to the fine structure Serve a reasonably clear purpose: description, exploration, tabulation or decoration Be closely integrated with the statistical and verbal descriptions of a data set. 4.4 Graphics Packages in R There are many graphics packages in R. Some packages perform general data visualization or graphical taskss. The others provide specific graphics for certain statistical or data analyses. The popular general purpose graphics packages in R are: graphics : a base R package, which means it is loaded everytime we open R ggplot2 : a user-contributed package by RStudio lattice : a user-contributed package Except for graphics package (a base R package), other packages need to downloaded and installed into your R library. A few examples more specific graphical packages include: survminer::ggsurvlot : The survminer R package provides functions for facilitating survival analysis and visualization. sjPlot : Collection of plotting and table output functions for data visualization 4.5 The ggplot2 Package For this book, we will focus on using the ggplot2 package. The ggplot2 package is an elegant, easy and versatile general graphics package in R. It implements the grammar of graphics concept. The advantage of this concept is that, it fasten the process of learning graphics. It also facilitates the process of creating complex graphics To work with ggplot2, remember to start R codes with ggplot() identify which data to plot: data = Your Data state variables to plot: aes(x = Variable on x-axis, y = Variable on y-axis ) choose type of graph: for example geom_histogram() for histogram, and geom_points() for scatterplots The official website for ggplot2 is here http://ggplot2.org/. It has excellent resources. It states that: ggplot2 is a plotting system for R, based on the grammar of graphics, which tries to take the good parts of base and lattice graphics and none of the bad parts. It takes care of many of the fiddly details that make plotting a hassle (like drawing legends) as well as providing a powerful model of graphics that makes it easy to produce complex multi-layered graphics. 4.6 Preparation 4.6.1 Create a New RStudio Project It is always recommended that to start working on data analysis in RStudio, you create first a new project. Go to File, then click New Project. You can create a new R project based on existing directory. This method is useful because an RStudio project keep your data, your analysis, and outputs in a clean dedicated folder or sets of folders.If you do not want to create a new project, then make sure you are inside the correct directory (the working directory). The working directory is a folder where you store. Type getwd() in your Console to display your working directory. Inside your working directory, you should see and keep dataset or datasets outputs - plots codes (R scripts .R, R markdown files .Rmd) 4.6.2 Important Questions when Making Graphs You must ask yourselves these: Which variable or variables do I want to plot? What is (or are) the type of that variable? Are they factor (categorical) variables ? Are they numerical variables? Am I going to plot a single variable? two variables together? three variables together? 4.6.3 Read Data The common data formats include comma separated files (.csv) MS Excel file (.xlsx) SPSS file (.sav) Stata file (.dta) SAS file Packages that read these data include haven and rio packages. Below are the functions to read SAS, SPSS and Stata file using the haven package. SAS: read_sas() reads .sas7bdat + .sas7bcat files and read_xpt() reads SAS transport files (version 5 and version 8). write_sas() writes .sas7bdat files. SPSS: read_sav() reads .sav files and read_por() reads the older .por files. write_sav() writes .sav files. Stata: read_dta() reads .dta files (up to version 15). write_dta() writes .dta files (versions 8-15). Sometime, we may want to import data from databases. For beginners, this experience is less common. However, the skill to import data from databases are getting more important and more common. Fortunately, R can easily import and read these data. Some examples of common databases format are: MySQL SQLite Postgresql Mariadb 4.6.4 Load the Library The ggplot2 package is one of the core member of tidyverse metapackage (https://www.tidyverse.org/). So, if we load the tidyverse package, it means we are also loading other packages under the tidyverse metapackage: which include dplyr, readr, ggplot2. Loading a package will give you access to help pages of the package functions available in the package sample datasets (not all packages contain this feature) We will also load the here package. This is useful to point to the codes to a specific folder in the project space. We will see this in action later. library(tidyverse) ## -- Attaching packages --------------------------------------- tidyverse 1.3.1 -- ## v ggplot2 3.3.5 v purrr 0.3.4 ## v tibble 3.1.6 v dplyr 1.0.7 ## v tidyr 1.1.4 v stringr 1.4.0 ## v readr 2.1.1 v forcats 0.5.1 ## Warning: package &#39;tibble&#39; was built under R version 4.1.2 ## -- Conflicts ------------------------------------------ tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(here) ## here() starts at C:/Tengku/Sync_PC_Laptop/tulis-buku/multivariable-analysis/fork-profKIM/multivar_data_analysis If you run the code and you get this message there is no package called tidyverse, you need to install the tidyverse package on your R IDE. To install the package, type install.package(\"tidyverse\") in the Console. Once the installation is complete, type library(tidyverse) to load the package. Alternatively, you can use the GUI to install the package: Packages pane Now, type the package you want to install. For example you want to install the tidyverse package Typing the package to install 4.6.5 Read Dataset For now, we will use two datasets: the built-in dataset in the gapminder package. You can read more about gapminder from https://www.gapminder.org/. The gapminder website contains many useful datasets and show wonderful graphics. It is made popular by Dr Hans Rosling. the dataset of patients admitted with peptic ulcer disease peptic_ulcer.xlsx. It is in the MS Excel format. To load the gapminder package, type library(gapminder) call the data gapminder into R and browse the first 6 observations of the gapminder data. The codes below shows assigning gapminder as a dataset a pipe that connects two codes (gapminder and slice) a function called slice() that select rows of the dataset gapminder &lt;- gapminder gapminder %&gt;% slice(1:4) ## # A tibble: 4 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. We can list the variables and look at the type of the variables in the gapminder dataset glimpse(gapminder) ## Rows: 1,704 ## Columns: 6 ## $ country &lt;fct&gt; &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, ~ ## $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, ~ ## $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, ~ ## $ lifeExp &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8~ ## $ pop &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12~ ## $ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, ~ The gapminder data have Six (6) variables A total of 1704 observations Two factor variables, 2 integer variables and 2 numeric (dbl) variables We can examine the basic statistics of the gapminder datasets by using summary(). This function will list the frequencies some descriptive statistics: min, 1st quartile, median, mean, 3rd quartile and max summary(gapminder) ## country continent year lifeExp ## Afghanistan: 12 Africa :624 Min. :1952 Min. :23.60 ## Albania : 12 Americas:300 1st Qu.:1966 1st Qu.:48.20 ## Algeria : 12 Asia :396 Median :1980 Median :60.71 ## Angola : 12 Europe :360 Mean :1980 Mean :59.47 ## Argentina : 12 Oceania : 24 3rd Qu.:1993 3rd Qu.:70.85 ## Australia : 12 Max. :2007 Max. :82.60 ## (Other) :1632 ## pop gdpPercap ## Min. :6.001e+04 Min. : 241.2 ## 1st Qu.:2.794e+06 1st Qu.: 1202.1 ## Median :7.024e+06 Median : 3531.8 ## Mean :2.960e+07 Mean : 7215.3 ## 3rd Qu.:1.959e+07 3rd Qu.: 9325.5 ## Max. :1.319e+09 Max. :113523.1 ## To know more about the package, we can use the \\(?\\) mark ?gapminder ## starting httpd help server ... done 4.7 Basic Plots Let us start by creating a simple plot using these arguments: data : data = gapminder variables : x = year, y = lifeExp graph scatterplot : geom_point() In ggplot2 which is a package under tidyverse package, we may use the \\(+\\) sign to connect the function. And in R, your codes can span multiple lines. This will increase the visibility of the codes. ggplot(data = gapminder) + geom_point(mapping = aes(x = year, y = lifeExp)) Now, we can see a scatterplot the scatterplot shows the relationship between year and life expectancy. as variable year advances, the life expectancy increases. What do the codes tell us? the ggplot() tells R to be ready to make plot from a specified data. And geom_point() tells R to make a scatter plot. More resources about ggplot2 package is available here https://ggplot2.tidyverse.org/reference/ggplot.html 4.8 Adding another variable We can see that the variables we want to plot are specified by aes(). We can add a third variable to make a more complex plot. For example: data : data = gapminder variables : x = year, y = lifeExp, colour = continent For this, the objective to create plot might be to see the relationship between year and life expectancy based on continent. ggplot(data = gapminder) + geom_point(mapping = aes(x = year, y = lifeExp, colour = continent)) What can you see from the scatterplot? You may notice that Europe countries have high life expectancy Africa countries have lower life expectancy One Asia country looks like an outlier (very low life expectancy) One Africa country looks like an outlier (very low life expectancy) Now, we will replace the third variable with Gross Domestic Product (gdpPercap) and make the plot correlates with the size of gdpPerCap ggplot(data = gapminder) + geom_point(mapping = aes(x = year, y = lifeExp, size = gdpPercap)) ggplot2 will automatically assign a unique level of the aesthetic (here a unique color) to each unique value of the variable, a process known as scaling. ggplot2 will also add a legend that explains which levels correspond to which values. The plot suggests that with higher gdpPerCap, there is also longer lifeExp. Instead of using colour, we can use different shapes. This is useful especially in the instances where there is no facility to print out colourful plots. ggplot(data = gapminder) + geom_point(mapping = aes(x = year, y = lifeExp, shape = continent)) But, see what will happen if we set the colour and shape like below but outside the aes parentheses. For example, let set the parameter colour to blue ggplot(data = gapminder) + geom_point(mapping = aes(x = year, y = lifeExp), colour = &#39;blue&#39;) And then parameter shape to plus (which is represented by number 3). ggplot(data = gapminder) + geom_point(mapping = aes(x = year, y = lifeExp), shape = 3) We may wonder what number corresponds to what type of shape. We can type \\(?pch\\). And we will see in the Viewer pane, the explanation about the shape available in R. It also shows what number that corresponds to what shape. 4.9 Making Subplots We can split our plots based on a factor variable and make subplots using the facet(). For example, if we want to make subplots based on continents, then we need to set these parameters: data = gapminder variable year on the x-axis and lifeExp on the y-axis split the plot based on continent set the number of rows for the plot at 3 ggplot(data = gapminder) + geom_point(mapping = aes(x = year, y = lifeExp)) + facet_wrap(~ continent, nrow = 3) Now, what happen if we change the value for the nrow ggplot(data = gapminder) + geom_point(mapping = aes(x = year, y = lifeExp)) + facet_wrap(~ continent, nrow = 2) 4.10 Overlaying Plots Each geom_X() in ggplot2 indicates different visual objects. This is a scatterplot ggplot(data = gapminder) + geom_point(mapping = aes(x = gdpPercap, y = lifeExp)) This is a smooth line plot ggplot(data = gapminder) + geom_smooth(mapping = aes(x = gdpPercap, y = lifeExp)) ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; And we can regenerate the smooth plot based on continent using the linetype(). We will also use log(gdpPercap) to reduce the skewness of the data. ggplot(data = gapminder) + geom_smooth(mapping = aes(x = log(gdpPercap), y = lifeExp, linetype = continent)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Another smooth plot but setting the parameter for colour ggplot(data = gapminder) + geom_smooth(mapping = aes(x = log(gdpPercap), y = lifeExp, colour = continent)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; 4.11 Combining Different Plots We can combine more than one geoms (type of plots) to overlay plots. The trick is to use multiple geoms in a single line of R code ggplot(data = gapminder) + geom_point(mapping = aes(x = log(gdpPercap), y = lifeExp)) + geom_smooth(mapping = aes(x = log(gdpPercap), y = lifeExp)) ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; The codes above show duplication or repetition. To avoid this, we can pass the mapping to ggplot(). ggplot(data = gapminder, mapping = aes(x = log(gdpPercap), y = lifeExp)) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; And we can expand this to make scatterplot shows different colour for continent ggplot(data = gapminder, mapping = aes(x = log(gdpPercap), y = lifeExp)) + geom_point(mapping = aes(colour = continent)) + geom_smooth() ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; Or expand this to make the smooth plot shows different colour for continent ggplot(data = gapminder, mapping = aes(x = log(gdpPercap), y = lifeExp)) + geom_point() + geom_smooth(mapping = aes(colour = continent)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Or both the scatterplot and the smoothplot ggplot(data = gapminder, mapping = aes(x = log(gdpPercap), y = lifeExp)) + geom_point(mapping = aes(shape = continent)) + geom_smooth(mapping = aes(colour = continent)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; 4.12 Statistical Transformation Let us create a bar chart, with y axis as the frequency. ggplot(data = gapminder) + geom_bar(mapping = aes(x = continent)) If we want the y-axis to show proportion, we can use these codes ggplot(data = gapminder) + geom_bar(mapping = aes(x = continent, y = ..prop.., group = 1)) 4.13 Customizing Title We can customize many aspects of the plot using ggplot package. For example, from gapminder dataset, we choose gdpPerCap and log it (to reduce skewness) and lifeExpy, and make a scatterplot. Lets name the plot as mypop mypop &lt;- ggplot(data = gapminder, mapping = aes(x = log(gdpPercap), y = lifeExp)) + geom_point() + geom_smooth(mapping = aes(colour = continent)) mypop ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; You will notice that there is no title in the plot. So we will add a title to the plot. mypop + ggtitle(&quot;GDP (in log) and life expectancy&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; To make the title appear in multiple lines, we can add \\n mypop + ggtitle(&quot;GDP (in log) and life expectancy: \\nData from Gapminder&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; 4.14 Adjusting Axes We can specify the tick marks min = 0 max = 12 interval = 1 mypop + scale_x_continuous(breaks = seq(0,12,1)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; And we can label the x-axis and y-axis mypop + ggtitle(&quot;GDP (in log) and life expectancy: \\nData from Gapminder&quot;) + ylab(&quot;Life Expentancy&quot;) + xlab(&quot;Percapita GDP in log&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; 4.15 Choosing Themes The default is gray theme or theme_gray(). But there are many other themes. One of the popular themes is the the black and white theme. mypop + theme_bw() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; This is the classic theme mypop + theme_classic() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; 4.16 Saving Plots In R, you can save the plot into different graphical format. You can also set other parameters such as the dpi and the size for the plot (height and width). One of the preferred formats for saving a plot is as a PDF format. Here, we will show how to save plots in different format in R. In this example, let us use the object we created before (mypop). We will add title x label y label classic theme And then create a new graphical object myplot myplot &lt;- mypop + ggtitle(&quot;GDP (in log) and life expectancy: \\nData from Gapminder&quot;) + ylab(&quot;Life Expentancy&quot;) + xlab(&quot;Percapita GDP in log&quot;) + scale_x_continuous(breaks = seq(0,12,1)) + theme_classic() myplot ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; We now can see a nice plot. And next, we want to save the plot (currently on the screen) to these formats: pdf format png format jpg format And we want to save the plots in a folder named as plots. To do this create a folder and name it as plot We are using the here() function now. If you notice, we use a new package called here. This is a very useful package that can help get or save files in the correct path (including folder) even when we are using different machines. If we are not clear about this, do you remember any circumstances when the drive name changes automatically (especially when using thumbdrive) in different computers. By using here() from the here package, we will always get to the correct path or folder. To save the plots in the directory named as plots, we can do these: ggsave(plot = myplot, here(&quot;plots&quot;,&quot;my_pdf_plot.pdf&quot;)) ## Saving 7 x 5 in image ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ggsave(plot = myplot, here(&quot;plots&quot;,&quot;my_png_plot.png&quot;)) ## Saving 7 x 5 in image ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ggsave(plot = myplot, here(&quot;plots&quot;,&quot;my_jpg_plot.jpg&quot;)) ## Saving 7 x 5 in image ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; If we want to add more customization before saving the plot, for example, we want to set these parameters: width = 10 cm (or you can use in) height = 6 cm (or you can use in) dpi = 150. dpi is dots per inch Now, you can run these codes ggsave(plot = myplot, here(&#39;plots&#39;,&#39;my_pdf_plot2.pdf&#39;), width = 10, height = 6, units = &quot;in&quot;, dpi = 150, device = &#39;pdf&#39;) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ggsave(plot = myplot, here(&#39;plots&#39;,&#39;my_png_plot2.png&#39;), width = 10, height = 6, units = &quot;cm&quot;, dpi = 150, device = &#39;png&#39;) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ggsave(plot = myplot, here(&quot;plots&quot;,&quot;my_jpg_plot2.jpg&quot;), width = 10, height = 6, units = &quot;cm&quot;, dpi = 150, device = &#39;jpg&#39;) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; "],["data-wrangling.html", "Chapter 5 Data Wrangling 5.1 Definition of data wrangling 5.2 Data wrangling with dplyr package 5.3 Common procedures for doing data transformation 5.4 Some dplyr functions 5.5 Create a new project or set your working directory 5.6 Load the libraries 5.7 Datasets 5.8 Select variables, generate new variable and rename variable 5.9 Sorting data and select observation 5.10 Group data and get summary statistics 5.11 More complicated dplyr verbs 5.12 Data transformation for categorical variables 5.13 Summary 5.14 Self-practice 5.15 References", " Chapter 5 Data Wrangling 5.1 Definition of data wrangling Data wrangling is also known as Data Munging or Data Transformation. It is loosely the process of manually converting or mapping data from one raw form into another format. The process allows for more convenient consumption of the data. In doing so, we will be using semi-automated tools in RStudio. You can find more information here https://community.modeanalytics.com/sql/tutorial/data-wrangling-with-sql/ 5.2 Data wrangling with dplyr package 5.2.1 dplyr package dplyr is a package grouped inside tidyverse collection of packages. dplyr package is a very useful package to munge or wrangle or to tranform your data. It is a grammar of data manipulation. It provides a consistent set of verbs that help you solve the most common data manipulation challenges. This tidyverse webpage https://github.com/tidyverse/dplyr has more information and examples. 5.3 Common procedures for doing data transformation The common data wrangling procedures that data analyst does include: reducing the size of dataset by selecting certain variables (or columns) generating new variable from existing variables sorting observation of a variable grouping observations based on certain criteria reducing variable to groups to in order to estimate summary statistic 5.4 Some dplyr functions For the procedures listed above, the corresponding dplyr functions are dplyr::select() - to select a number of variables from a dataframe dplyr::mutate() - to generate a new variable from existing variables dplyr::arrange() - to sort observation of a variable dplyr::filter() - to group observations that fulfil certain criteria dplyr::group_by() and dplyr::summarize() - to reduce variable to groups in order to provide summary statistic 5.5 Create a new project or set your working directory It is very important to ensure you know where your working directory is. To do so, the best practice is is to create a new project everytime you want to start new analysis with R. To do so, create a new project by File -&gt; New Project. If you do not start with a new project, you still need to know Where is my working directory?. So, again we emphasize that every time you want to start processing your data, please make sure: you use R project. It is much easier and cleaner to start your work with a new R project. Once you have done or need to log off your computer, close the project and reopen the project the next time you need to. that if you are not using R project, you are inside the correct working directory. Type getwd() to display the active working directory. And to set a new working directory use the function setwd(). Once you are know where your working directory is, you can start read or import data into your working directory. Once you are inside the project, you can import your data if necessary. 5.6 Load the libraries Remember, there are a number of packages you can use to read the data into R. R can read almost all (if not all format) types of data format. For example, we know that common data format are the: SPSS (.sav) format, Stata (.dta) format, SAS format, MS Excel (.xlsx) format Comma-separated-values .csv format. But they are other formats too for examples data in DICOM format. DICOM format data includes data from CT scan and MRI images. There are data in shapefile format to store geographical information. Three packages - haven, rio, readr and foreign packages - are very useful to read or import your data into R memory. readr provides a fast and friendly way to read rectangular data (like csv, tsv, and fwf). This is contained inside the tidyverse metapackage rio provides a quick way to read almost all type of spreadsheet and statistical software data readxl reads .xls and .xlsx sheets. haven reads SPSS, Stata, and SAS data. We will use the here package to facilitate us working with the working directory and lubridate to help us wrangle dates. library(tidyverse) ## -- Attaching packages --------------------------------------- tidyverse 1.3.1 -- ## v ggplot2 3.3.5 v purrr 0.3.4 ## v tibble 3.1.6 v dplyr 1.0.7 ## v tidyr 1.1.4 v stringr 1.4.0 ## v readr 2.1.1 v forcats 0.5.1 ## Warning: package &#39;tibble&#39; was built under R version 4.1.2 ## -- Conflicts ------------------------------------------ tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(rio) ## Warning: package &#39;rio&#39; was built under R version 4.1.2 library(here) ## here() starts at C:/Tengku/Sync_PC_Laptop/tulis-buku/multivariable-analysis/fork-profKIM/multivar_data_analysis library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union When we read dataset that have long variable names and spaces - especially after reading MS Excel dataset - we can use the janitor package to generate more R user-friendly variable names. 5.7 Datasets We will use 2 datasets the stroke dataset in csv format the peptic ulcer dataset in xlsx format Lets read the datasets and name it, each as stroke pep stroke &lt;- read_csv(here(&#39;data&#39;, &#39;stroke_data.csv&#39;)) ## Rows: 213 Columns: 12 ## -- Column specification -------------------------------------------------------- ## Delimiter: &quot;,&quot; ## chr (7): doa, dod, status, sex, dm, stroke_type, referral_from ## dbl (5): gcs, sbp, dbp, wbc, time2 ## ## i Use `spec()` to retrieve the full column specification for this data. ## i Specify the column types or set `show_col_types = FALSE` to quiet this message. pep &lt;- import(here(&#39;data&#39;, &#39;peptic_ulcer.xlsx&#39;)) Take a peek at the dataset 219 observations 12 variables glimpse(stroke) ## Rows: 213 ## Columns: 12 ## $ doa &lt;chr&gt; &quot;17/2/2011&quot;, &quot;20/3/2011&quot;, &quot;9/4/2011&quot;, &quot;12/4/2011&quot;, &quot;12/4~ ## $ dod &lt;chr&gt; &quot;18/2/2011&quot;, &quot;21/3/2011&quot;, &quot;10/4/2011&quot;, &quot;13/4/2011&quot;, &quot;13/~ ## $ status &lt;chr&gt; &quot;alive&quot;, &quot;alive&quot;, &quot;dead&quot;, &quot;dead&quot;, &quot;alive&quot;, &quot;dead&quot;, &quot;aliv~ ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;m~ ## $ dm &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;yes&quot;,~ ## $ gcs &lt;dbl&gt; 15, 15, 11, 3, 15, 3, 11, 15, 6, 15, 15, 4, 4, 10, 12, 1~ ## $ sbp &lt;dbl&gt; 151, 196, 126, 170, 103, 91, 171, 106, 170, 123, 144, 23~ ## $ dbp &lt;dbl&gt; 73, 123, 78, 103, 62, 55, 80, 67, 90, 83, 89, 120, 120, ~ ## $ wbc &lt;dbl&gt; 12.5, 8.1, 15.3, 13.9, 14.7, 14.2, 8.7, 5.5, 10.5, 7.2, ~ ## $ time2 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~ ## $ stroke_type &lt;chr&gt; &quot;IS&quot;, &quot;IS&quot;, &quot;HS&quot;, &quot;IS&quot;, &quot;IS&quot;, &quot;HS&quot;, &quot;IS&quot;, &quot;IS&quot;, &quot;HS&quot;, &quot;I~ ## $ referral_from &lt;chr&gt; &quot;non-hospital&quot;, &quot;non-hospital&quot;, &quot;hospital&quot;, &quot;hospital&quot;, ~ 121 observations 34 variables glimpse(pep) ## Rows: 121 ## Columns: 34 ## $ age &lt;dbl&gt; 42, 66, 67, 19, 77, 39, 62, 71, 69, 97, 52, 21, 57~ ## $ gender &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, ~ ## $ epigastric_pain &lt;chr&gt; &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;~ ## $ vomiting &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, ~ ## $ nausea &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;~ ## $ fever &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;yes&quot;, ~ ## $ diarrhea &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, ~ ## $ malena &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;n~ ## $ onset_more_24_hrs &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;no&quot;~ ## $ NSAIDS &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;~ ## $ septic_shock &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;n~ ## $ previous_OGDS &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;~ ## $ ASA &lt;dbl&gt; 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 2, 2,~ ## $ systolic &lt;dbl&gt; 141, 197, 126, 90, 147, 115, 103, 159, 145, 105, 1~ ## $ diastolic &lt;dbl&gt; 98, 88, 73, 40, 82, 86, 55, 68, 75, 65, 74, 50, 86~ ## $ inotropes &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;n~ ## $ pulse &lt;dbl&gt; 109, 126, 64, 112, 89, 96, 100, 57, 86, 100, 109, ~ ## $ tenderness &lt;chr&gt; &quot;generalized&quot;, &quot;generalized&quot;, &quot;generalized&quot;, &quot;loca~ ## $ guarding &lt;chr&gt; &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;n~ ## $ hemoglobin &lt;dbl&gt; 18.0, 12.0, 12.0, 12.0, 11.0, 18.0, 8.1, 13.3, 11.~ ## $ twc &lt;dbl&gt; 6.0, 6.0, 13.0, 20.0, 21.0, 4.0, 5.0, 12.0, 6.0, 2~ ## $ platelet &lt;dbl&gt; 415, 292, 201, 432, 324, 260, 461, 210, 293, 592, ~ ## $ creatinine &lt;dbl&gt; 135, 66, 80, 64, 137, 102, 69, 92, 94, 104, 58, 24~ ## $ albumin &lt;chr&gt; &quot;27&quot;, &quot;28&quot;, &quot;32&quot;, &quot;42&quot;, &quot;38&quot;, &quot;38&quot;, &quot;30&quot;, &quot;41&quot;, &quot;N~ ## $ PULP &lt;dbl&gt; 2, 3, 3, 2, 7, 1, 2, 5, 3, 4, 2, 3, 4, 3, 5, 5, 1,~ ## $ admission_to_op_hrs &lt;dbl&gt; 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 6, 6, 6, 6,~ ## $ perforation &lt;dbl&gt; 0.5, 1.0, 0.5, 0.5, 1.0, 1.0, 3.0, 1.5, 0.5, 1.5, ~ ## $ degree_perforation &lt;chr&gt; &quot;small&quot;, &quot;small&quot;, &quot;small&quot;, &quot;small&quot;, &quot;small&quot;, &quot;smal~ ## $ side_perforation &lt;chr&gt; &quot;distal stomach&quot;, &quot;distal stomach&quot;, &quot;distal stomac~ ## $ ICU &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;, ~ ## $ SSSI &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;n~ ## $ anast_leak &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;n~ ## $ sepsis &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;~ ## $ outcome &lt;chr&gt; &quot;alive&quot;, &quot;alive&quot;, &quot;alive&quot;, &quot;alive&quot;, &quot;alive&quot;, &quot;aliv~ Next, we examine the first 10 observations of the data. There rest are NOT SHOWN. You can also see the types of the variables: chr (character), int (integer), dbl (double) stroke ## # A tibble: 213 x 12 ## doa dod status sex dm gcs sbp dbp wbc time2 stroke_type ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 17/2/2011 18/2/~ alive male no 15 151 73 12.5 1 IS ## 2 20/3/2011 21/3/~ alive male no 15 196 123 8.1 1 IS ## 3 9/4/2011 10/4/~ dead fema~ no 11 126 78 15.3 1 HS ## 4 12/4/2011 13/4/~ dead male no 3 170 103 13.9 1 IS ## 5 12/4/2011 13/4/~ alive fema~ yes 15 103 62 14.7 1 IS ## 6 4/5/2011 5/5/2~ dead fema~ no 3 91 55 14.2 1 HS ## 7 22/5/2011 23/5/~ alive male no 11 171 80 8.7 1 IS ## 8 23/5/2011 24/5/~ alive fema~ yes 15 106 67 5.5 1 IS ## 9 11/7/2011 12/7/~ dead fema~ yes 6 170 90 10.5 1 HS ## 10 4/9/2011 5/9/2~ alive fema~ no 15 123 83 7.2 1 IS ## # ... with 203 more rows, and 1 more variable: referral_from &lt;chr&gt; pep %&gt;% slice_head(n = 10) ## age gender epigastric_pain vomiting nausea fever diarrhea malena ## 1 42 male yes no no no no no ## 2 66 female yes no no no no no ## 3 67 male yes no no no yes no ## 4 19 male yes no no no no no ## 5 77 male yes yes yes no no no ## 6 39 male yes no no yes no no ## 7 62 female yes no no no no no ## 8 71 female yes yes no yes yes no ## 9 69 male yes no no yes no no ## 10 97 female yes no no no no no ## onset_more_24_hrs NSAIDS septic_shock previous_OGDS ASA systolic diastolic ## 1 no no no no 1 141 98 ## 2 no no no no 1 197 88 ## 3 no yes no no 1 126 73 ## 4 yes no no yes 1 90 40 ## 5 yes no no no 2 147 82 ## 6 yes no no no 1 115 86 ## 7 yes no no no 2 103 55 ## 8 no no no no 2 159 68 ## 9 no yes no no 1 145 75 ## 10 yes yes no no 1 105 65 ## inotropes pulse tenderness guarding hemoglobin twc platelet creatinine ## 1 no 109 generalized yes 18.0 6 415 135 ## 2 no 126 generalized yes 12.0 6 292 66 ## 3 no 64 generalized yes 12.0 13 201 80 ## 4 no 112 localized yes 12.0 20 432 64 ## 5 no 89 generalized no 11.0 21 324 137 ## 6 no 96 generalized yes 18.0 4 260 102 ## 7 no 100 generalized yes 8.1 5 461 69 ## 8 no 57 localized no 13.3 12 210 92 ## 9 no 86 generalized yes 11.8 6 293 94 ## 10 no 100 generalized yes 10.0 28 592 104 ## albumin PULP admission_to_op_hrs perforation degree_perforation ## 1 27 2 2 0.5 small ## 2 28 3 2 1.0 small ## 3 32 3 3 0.5 small ## 4 42 2 3 0.5 small ## 5 38 7 3 1.0 small ## 6 38 1 3 1.0 small ## 7 30 2 4 3.0 large ## 8 41 5 4 1.5 moderate ## 9 NA 3 4 0.5 small ## 10 29 4 4 1.5 moderate ## side_perforation ICU SSSI anast_leak sepsis outcome ## 1 distal stomach no no no no alive ## 2 distal stomach no no no no alive ## 3 distal stomach no no no no alive ## 4 distal stomach no no no no alive ## 5 distal stomach yes no no no alive ## 6 duodenum no no no no alive ## 7 distal stomach yes no no yes dead ## 8 distal stomach no no no no alive ## 9 distal stomach no no no no alive ## 10 distal stomach yes no no no alive 5.8 Select variables, generate new variable and rename variable We will work with these functions dplyr::select() dplyr::mutate() and dplyr::rename() 5.8.1 Select variables using dplyr::select() When you work with large datasets with many columns, sometimes it is easier to select only the necessary columns to reduce the size of dataset. This is possible by creating a smaller dataset (less variables). Then you can work on at the initial part of data analysis with this smaller dataset. This will greatly help data exploration. To create smaller datasets, select some of the columns (variables) in the dataset. For example, in pep data, we have 34 variables. Let us generate a new dataset named as pep2 with only 10 variables , pep2 &lt;- pep %&gt;% dplyr::select(age, systolic, diastolic, perforation, twc, gender, vomiting, malena, ASA, outcome) glimpse(pep2) ## Rows: 121 ## Columns: 10 ## $ age &lt;dbl&gt; 42, 66, 67, 19, 77, 39, 62, 71, 69, 97, 52, 21, 57, 58, 84~ ## $ systolic &lt;dbl&gt; 141, 197, 126, 90, 147, 115, 103, 159, 145, 105, 113, 92, ~ ## $ diastolic &lt;dbl&gt; 98, 88, 73, 40, 82, 86, 55, 68, 75, 65, 74, 50, 86, 65, 50~ ## $ perforation &lt;dbl&gt; 0.5, 1.0, 0.5, 0.5, 1.0, 1.0, 3.0, 1.5, 0.5, 1.5, 1.0, 0.5~ ## $ twc &lt;dbl&gt; 6.0, 6.0, 13.0, 20.0, 21.0, 4.0, 5.0, 12.0, 6.0, 28.0, 11.~ ## $ gender &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;female&quot;~ ## $ vomiting &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;n~ ## $ malena &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;~ ## $ ASA &lt;dbl&gt; 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 2, 2, 1, 1, 3~ ## $ outcome &lt;chr&gt; &quot;alive&quot;, &quot;alive&quot;, &quot;alive&quot;, &quot;alive&quot;, &quot;alive&quot;, &quot;alive&quot;, &quot;dea~ The new dataset pep2 is now created. You can see it in the Environment pane. 5.8.2 Generate new variable using mutate() With mutate(), you can generate new variable. For example, in the dataset pep2, we want to create a new variable named as pulse_pressure. This variable equals systolic minus diastolic BP. \\[pulse \\: pressure = systolic \\: BP - diastolic \\: BP \\] pep2 &lt;- pep2 %&gt;% mutate(pulse_pressure = systolic - diastolic) pep2 %&gt;% dplyr::select(systolic, diastolic, pulse_pressure ) ## systolic diastolic pulse_pressure ## 1 141 98 43 ## 2 197 88 109 ## 3 126 73 53 ## 4 90 40 50 ## 5 147 82 65 ## 6 115 86 29 ## 7 103 55 48 ## 8 159 68 91 ## 9 145 75 70 ## 10 105 65 40 ## 11 113 74 39 ## 12 92 50 42 ## 13 122 86 36 ## 14 117 65 52 ## 15 103 50 53 ## 16 70 40 30 ## 17 106 62 44 ## 18 150 82 68 ## 19 72 38 34 ## 20 147 63 84 ## 21 131 63 68 ## 22 121 71 50 ## 23 135 82 53 ## 24 111 57 54 ## 25 129 62 67 ## 26 154 78 76 ## 27 105 67 38 ## 28 117 71 46 ## 29 111 67 44 ## 30 133 72 61 ## 31 88 60 28 ## 32 152 90 62 ## 33 130 95 35 ## 34 161 116 45 ## 35 142 63 79 ## 36 152 80 72 ## 37 102 74 28 ## 38 109 45 64 ## 39 148 69 79 ## 40 129 60 69 ## 41 112 67 45 ## 42 67 50 17 ## 43 112 63 49 ## 44 108 60 48 ## 45 121 81 40 ## 46 190 90 100 ## 47 100 54 46 ## 48 123 59 64 ## 49 125 67 58 ## 50 152 81 71 ## 51 141 87 54 ## 52 136 68 68 ## 53 116 69 47 ## 54 135 76 59 ## 55 135 81 54 ## 56 114 58 56 ## 57 193 93 100 ## 58 160 84 76 ## 59 128 68 60 ## 60 110 82 28 ## 61 140 80 60 ## 62 115 65 50 ## 63 141 82 59 ## 64 105 66 39 ## 65 143 68 75 ## 66 110 68 42 ## 67 112 57 55 ## 68 115 76 39 ## 69 118 74 44 ## 70 110 66 44 ## 71 122 75 47 ## 72 128 84 44 ## 73 117 85 32 ## 74 136 72 64 ## 75 163 67 96 ## 76 105 51 54 ## 77 120 70 50 ## 78 145 79 66 ## 79 121 71 50 ## 80 129 100 29 ## 81 121 81 40 ## 82 120 80 40 ## 83 84 46 38 ## 84 131 84 47 ## 85 134 61 73 ## 86 114 56 58 ## 87 173 70 103 ## 88 124 66 58 ## 89 95 50 45 ## 90 157 73 84 ## 91 197 81 116 ## 92 143 93 50 ## 93 149 66 83 ## 94 140 67 73 ## 95 128 75 53 ## 96 130 68 62 ## 97 183 84 99 ## 98 123 80 43 ## 99 154 80 74 ## 100 139 97 42 ## 101 151 71 80 ## 102 165 92 73 ## 103 132 80 52 ## 104 163 105 58 ## 105 160 90 70 ## 106 140 90 50 ## 107 161 99 62 ## 108 148 81 67 ## 109 101 57 44 ## 110 115 75 40 ## 111 130 72 58 ## 112 130 67 63 ## 113 123 53 70 ## 114 118 60 58 ## 115 100 67 33 ## 116 112 74 38 ## 117 104 65 39 ## 118 130 80 50 ## 119 129 63 66 ## 120 133 80 53 ## 121 119 66 53 Now for stroke dataset, we will convert doa and dod, both a character variable to variable of date type stroke &lt;- stroke %&gt;% mutate(doa = dmy(doa), dod = dmy(dod)) stroke ## # A tibble: 213 x 12 ## doa dod status sex dm gcs sbp dbp wbc time2 ## &lt;date&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2011-02-17 2011-02-18 alive male no 15 151 73 12.5 1 ## 2 2011-03-20 2011-03-21 alive male no 15 196 123 8.1 1 ## 3 2011-04-09 2011-04-10 dead female no 11 126 78 15.3 1 ## 4 2011-04-12 2011-04-13 dead male no 3 170 103 13.9 1 ## 5 2011-04-12 2011-04-13 alive female yes 15 103 62 14.7 1 ## 6 2011-05-04 2011-05-05 dead female no 3 91 55 14.2 1 ## 7 2011-05-22 2011-05-23 alive male no 11 171 80 8.7 1 ## 8 2011-05-23 2011-05-24 alive female yes 15 106 67 5.5 1 ## 9 2011-07-11 2011-07-12 dead female yes 6 170 90 10.5 1 ## 10 2011-09-04 2011-09-05 alive female no 15 123 83 7.2 1 ## # ... with 203 more rows, and 2 more variables: stroke_type &lt;chr&gt;, ## # referral_from &lt;chr&gt; 5.8.3 Rename variable using rename() Now, we want to rename variable gender to sex variable ASA to asa pep2 &lt;- pep2 %&gt;% rename(sex = gender, asa = ASA) 5.9 Sorting data and select observation The function arrange() can sort the data. And the function filter() allows you to select observations based on your criteria. 5.9.1 Sorting data using arrange() We can sort data in ascending or descending order. To do that, we will use arrange(). It will sort the observation based on the values of the specified variable. For dataset stroke, let us sort the doa from the earliest. stroke %&gt;% arrange(doa) ## # A tibble: 213 x 12 ## doa dod status sex dm gcs sbp dbp wbc time2 ## &lt;date&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2011-01-01 2011-01-05 dead female yes 15 150 87 12.5 4 ## 2 2011-01-03 2011-01-06 alive male no 15 152 108 7.4 3 ## 3 2011-01-06 2011-01-22 alive female yes 15 231 117 22.4 16 ## 4 2011-01-16 2011-02-08 alive female no 11 110 79 9.6 23 ## 5 2011-01-18 2011-01-23 alive male no 15 199 134 18.7 5 ## 6 2011-01-20 2011-01-24 dead female no 7 190 101 11.3 4 ## 7 2011-01-25 2011-02-16 alive female yes 5 145 102 15.8 22 ## 8 2011-01-28 2011-02-11 dead female yes 13 161 96 8.5 14 ## 9 2011-01-29 2011-02-02 alive male no 15 222 129 9 4 ## 10 2011-01-31 2011-02-02 alive male no 15 161 107 9.5 2 ## # ... with 203 more rows, and 2 more variables: stroke_type &lt;chr&gt;, ## # referral_from &lt;chr&gt; 5.9.2 Select observation using filter() To select observations based on certain criteria, we use the filter() function. Here, in this example, we will create a new dataset (which we will name as stroke_m_40) that contains patients that have sex as male and Glasgow Coma Scale (gcs) at 7 or higher: gender is male gcs at 7 or higher stroke_m_7 &lt;- stroke %&gt;% filter(sex == &#39;male&#39;, gcs &gt;= 7) stroke_m_7 ## # A tibble: 85 x 12 ## doa dod status sex dm gcs sbp dbp wbc time2 ## &lt;date&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2011-02-17 2011-02-18 alive male no 15 151 73 12.5 1 ## 2 2011-03-20 2011-03-21 alive male no 15 196 123 8.1 1 ## 3 2011-05-22 2011-05-23 alive male no 11 171 80 8.7 1 ## 4 2011-11-28 2011-11-29 dead male no 10 207 128 10.8 1 ## 5 2012-02-22 2012-02-23 dead male no 7 150 80 15.5 1 ## 6 2012-03-25 2012-03-26 alive male no 14 128 79 10.3 1 ## 7 2012-04-02 2012-04-03 alive male no 15 143 59 7.1 1 ## 8 2011-01-31 2011-02-02 alive male no 15 161 107 9.5 2 ## 9 2011-02-06 2011-02-08 alive male no 15 153 61 11.2 2 ## 10 2011-02-20 2011-02-22 alive male no 15 143 93 15.6 2 ## # ... with 75 more rows, and 2 more variables: stroke_type &lt;chr&gt;, ## # referral_from &lt;chr&gt; Next, we will create a new dataset (named as stroke_high_BP) that contain sbp above 130 OR dbp above 90, AND stroke_high_BP &lt;- stroke %&gt;% filter(sbp &gt; 130 | dbp &gt; 90) stroke_high_BP ## # A tibble: 173 x 12 ## doa dod status sex dm gcs sbp dbp wbc time2 ## &lt;date&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2011-02-17 2011-02-18 alive male no 15 151 73 12.5 1 ## 2 2011-03-20 2011-03-21 alive male no 15 196 123 8.1 1 ## 3 2011-04-12 2011-04-13 dead male no 3 170 103 13.9 1 ## 4 2011-05-22 2011-05-23 alive male no 11 171 80 8.7 1 ## 5 2011-07-11 2011-07-12 dead female yes 6 170 90 10.5 1 ## 6 2011-10-12 2011-10-13 alive female no 15 144 89 5.7 1 ## 7 2011-10-21 2011-10-22 alive male no 4 230 120 12.7 1 ## 8 2011-10-26 2011-10-27 dead female no 4 207 120 16.5 1 ## 9 2011-11-28 2011-11-29 dead male no 10 207 128 10.8 1 ## 10 2011-12-29 2011-12-30 alive female no 12 178 100 8.8 1 ## # ... with 163 more rows, and 2 more variables: stroke_type &lt;chr&gt;, ## # referral_from &lt;chr&gt; 5.10 Group data and get summary statistics Thegroup_by() function allows us to group data based on categorical variable. Using the summarize we do summary statistics for the overall data or for groups created using group_by() function. 5.10.1 Group data using group_by() The group_by function will prepare the data for group analysis. For example, to get summary values for mean sbp, mean dbp and mean gcs for sex stroke_sex &lt;- stroke %&gt;% group_by(sex) 5.10.2 Summary statistic using summarize() Now that we have a group data named stroke_sex, now, we would summarize our data using the mean and standard deviation (SD) for the groups specified above. stroke_sex %&gt;% summarise(meansbp = mean(sbp, na.rm = TRUE), meandbp = mean(dbp, na.rm = TRUE), meangcs = mean(gcs, na.rm = TRUE)) ## # A tibble: 2 x 4 ## sex meansbp meandbp meangcs ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 female 166. 91.5 11.9 ## 2 male 159. 91.6 13.3 To calculate the frequencies for two variables for pep dataset sex outcome pep2 %&gt;% group_by(sex) %&gt;% count(outcome, sort = TRUE) ## # A tibble: 4 x 3 ## # Groups: sex [2] ## sex outcome n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 male alive 70 ## 2 male dead 26 ## 3 female alive 13 ## 4 female dead 12 or pep2 %&gt;% count(sex, outcome, sort = TRUE) ## sex outcome n ## 1 male alive 70 ## 2 male dead 26 ## 3 female alive 13 ## 4 female dead 12 5.11 More complicated dplyr verbs To be more efficent, use multiple dplyr (a package inside tidyverse metapackage) functions in one line of R code. For example, pep2 %&gt;% filter(sex == &quot;male&quot;, diastolic &gt;= 60, systolic &gt;= 80) %&gt;% dplyr::select(age, systolic, diastolic, perforation, outcome) %&gt;% group_by(outcome) %&gt;% summarize(mean_sbp = mean(systolic, na.rm = TRUE), mean_dbp = mean(diastolic, na.rm = TRUE), mean_perf = mean(perforation, na.rm = TRUE), freq = n()) ## # A tibble: 2 x 5 ## outcome mean_sbp mean_dbp mean_perf freq ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 alive 135. 77.2 0.920 61 ## 2 dead 130. 75.5 1.80 23 5.12 Data transformation for categorical variables 5.12.1 forcats package Data transformation for categorical variables (factor variables) can be facilitated using the forcats package. 5.12.2 Conversion from numeric to factor variables Now, we will convert the integer (numerical) variable to a factor (categorical) variable. For example, we will generate a new factor (categorical) variable named as high_bp from variable sbp and dbp (which both are double variable). We will label high_bpas High or Not High. The criteria: if sbp \\(sbp \\geq 130\\) or \\(dbp \\geq 90\\) then labelled as High, else is Not High stroke &lt;- stroke %&gt;% mutate(high_bp = if_else(sbp &gt;= 130 | dbp &gt;= 90, &quot;High&quot;, &quot;Not High&quot;)) stroke %&gt;% count(high_bp) ## # A tibble: 2 x 2 ## high_bp n ## &lt;chr&gt; &lt;int&gt; ## 1 High 177 ## 2 Not High 36 of by using cut() stroke &lt;- stroke %&gt;% mutate(cat_sbp = cut(sbp, breaks = c(-Inf, 120, 130, Inf), labels = c(&#39;&lt;120&#39;, &#39;121-130&#39;, &#39;&gt;130&#39;))) stroke %&gt;% count(cat_sbp) ## # A tibble: 3 x 2 ## cat_sbp n ## &lt;fct&gt; &lt;int&gt; ## 1 &lt;120 25 ## 2 121-130 16 ## 3 &gt;130 172 stroke %&gt;% group_by(cat_sbp) %&gt;% summarize(minsbp = min(sbp), maxsbp = max(sbp)) ## # A tibble: 3 x 3 ## cat_sbp minsbp maxsbp ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 &lt;120 75 120 ## 2 121-130 122 130 ## 3 &gt;130 132 290 5.12.3 Recoding variables We use this function to recode variables from the old levels to the new levels. For example: stroke &lt;- stroke %&gt;% mutate(cat_sbp2 = recode(cat_sbp, &quot;&lt;120&quot; = &quot;120 or less&quot;, &quot;121-130&quot; = &quot;121 to 130&quot;, &quot;&gt;130&quot; = &quot;131 or higher&quot;)) stroke %&gt;% count(cat_sbp2) ## # A tibble: 3 x 2 ## cat_sbp2 n ## &lt;fct&gt; &lt;int&gt; ## 1 120 or less 25 ## 2 121 to 130 16 ## 3 131 or higher 172 5.12.4 Changing the level of categorical variable Variable cat_sbp will be ordered as less or 120, then 121 - 130, then 131 or higher levels(stroke$cat_sbp) ## [1] &quot;&lt;120&quot; &quot;121-130&quot; &quot;&gt;130&quot; To change in reverse for example, we can use fct_relevel stroke &lt;- stroke %&gt;% mutate(relevel_cat_sbp = fct_relevel(cat_sbp, levels = c(&quot;131 or higher&quot;, &quot;121 - 130&quot;, &quot;less or 120&quot;))) ## Warning: Outer names are only allowed for unnamed scalar atomic inputs ## Warning: Unknown levels in `f`: 131 or higher, 121 - 130, less or 120 levels(stroke$relevel_cat_sbp) ## [1] &quot;&lt;120&quot; &quot;121-130&quot; &quot;&gt;130&quot; stroke %&gt;% count(relevel_cat_sbp) ## # A tibble: 3 x 2 ## relevel_cat_sbp n ## &lt;fct&gt; &lt;int&gt; ## 1 &lt;120 25 ## 2 121-130 16 ## 3 &gt;130 172 5.13 Summary dplyr package is a very useful package that encourages users to use proper verb when manipulating variables (columns) and observations (rows). We have learned to use 5 functions but there are more functions available. Other useful functions include: dplyr::distinct() dplyr::transmutate() dplyr::sample_n() and dplyr::sample_frac() Also note that, package dplyr is very useful when it is combined with another function that is group_by If you working with database, you can use dbplyr which has been developed to perform very effectively with databases. For categorical variables, you can use forcats package. 5.14 Self-practice If you have completed the tutorial above, you may: Read your own data (hints: haven, foreign) or you can download data from https://www.kaggle.com/datasets . Maybe can try this dataset https://www.kaggle.com/blastchar/telco-customer-churn Create a smaller dataset by selecting some variable (hints: dplyr::select()) Creating a dataset with some selection (hints: dplyr::filter()) Generate a new variable (hintsdplyr::mutate()) Creata an object using pipe and combining dplyr::select(), dplyr::filter() and dplyr::mutate() in one single line of R code Summarise the mean, standard deviation and median for numerical variables dplyr::group_by() and dplyr::summarize() Calculare the number of observations for categorical variables (hints: dplyr::count()) Recode a categorical variable (hints: forcats::fct_recode()) 5.15 References dplyr vignettes here https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html forcats examples here http://r4ds.had.co.nz/factors.html reading data into R https://garthtarr.github.io/meatR/rio.html "],["r-markdown.html", "Chapter 6 R Markdown 6.1 Motivation 6.2 R Markdown Basics 6.3 Table 6.4 Plot, figure and image 6.5 Image 6.6 Inline output 6.7 Equation", " Chapter 6 R Markdown 6.1 Motivation 6.1.1 What is R Markdown? In short, R Markdown document allows reproducible report for data science. You can run R code and generate quality report. Source: https://rmarkdown.rstudio.com/lesson-1.html 6.1.2 Why should we bother learning R Markdown? A few questions: Have you recently made some changes to the data? Just generate a new report in seconds! Do you need to include some more analyses and plots? Just add in some R code and generate a new report in seconds! Do need to share the report but too lazy to edit? Compile and share nicely prepared report! 6.1.3 Sources to learn R Markdown R Markdown resources: Main website: https://rmarkdown.rstudio.com/ Most important, cheatsheet!: https://github.com/rstudio/cheatsheets/raw/master/rmarkdown.pdf Part of R for Data Science book: https://r4ds.had.co.nz/r-markdown-formats.html The definitive guide by the masters: https://bookdown.org/yihui/rmarkdown/ And a more advanced guide by the masters: https://bookdown.org/yihui/rmarkdown-cookbook/ Because R Markdown is based on Markdown, it is important to know the syntax: A good introduction to Markdown: https://daringfireball.net/projects/markdown/ Basic syntax: https://www.markdownguide.org/basic-syntax and extended syntax: https://www.markdownguide.org/extended-syntax 6.2 R Markdown Basics 6.2.1 Install rmarkdown package install.packages(&quot;rmarkdown&quot;) 6.2.2 Install LaTeX package for PDF R Markdown relies on LaTeX package, which can be installed on the respective OSs, Windows &amp; MacOS  MikTeX @ https://miktex.org/download Linux  texlive. LaTeX package is important so that you can generate PDF output. 6.2.3 New .Rmd file From menu, select File &gt; New file &gt; R script Edit the Title and Author fields. Choose any of the Default Output Format. A basic template will be generated. 6.2.4 Knit it! Click on Knit menu. You may click on the dropdown menu for more options. Compare the contents in the .Rmd file and your output. 6.2.5 YAML header Anything that are included in between the two ---. Basically we have the title, author, date and output. This can be further customized. 6.2.6 Code chunks In between the opening three backticks ``` and closing three backticks ```. It can be any code here. Include the opening as ```{r} to specify the chunk as R code. 6.2.7 Header levels First level, second level and third level are preceded by #, ## and ###. Add more # for more header sublevel. 6.2.8 Exercise 1 Customize the template with your own analyses and plots. Explore chunk options (refer to R Markdown Cheatsheet). More important ones are: echo eval comment 6.3 Table 6.3.1 Basic table A sample table like this: | Right | Left | Default | Center | |---:|:---|---|:---:| | 12 | 12 | 12 | 12 | | 123 | 123 | 123 | 123 | | 1.0 | 1.0 | 1.0 | 1.0 | Table: Sample table becomes like this: Sample table Right Left Default Center 12 12 12 12 123 123 123 123 1.0 1.0 1.0 1.0 6.3.2 Table (from data frame) ```{r} library(knitr) kable(head(iris)) ``` becomes: ## Warning: package &#39;knitr&#39; was built under R version 4.1.2 Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 6.3.3 More on table You can explore the following packages for table: kableExtra stargazer 6.4 Plot, figure and image 6.4.1 Plot caption  fig.cap ```{r, fig.cap=&quot;Sample caption&quot;, echo=FALSE} plot(pressure) ``` becomes: FIGURE 6.1: Sample caption 6.4.2 Plot size  fig.height(in inch) ```{r, fig.cap=&quot;Sample caption&quot;, echo=FALSE, fig.height=3} plot(pressure) ``` becomes: FIGURE 6.2: Sample caption 6.4.3 Plot size  fig.width (in inch) ```{r, fig.cap=&quot;Sample caption&quot;, echo=FALSE, fig.width=3} plot(pressure) ``` becomes: FIGURE 6.3: Sample caption 6.4.4 Plot aspect ratio  fig.asp ```{r, fig.cap=&quot;Sample caption&quot;, echo=FALSE, fig.width=3, fig.asp=1} plot(pressure) ``` becomes: FIGURE 6.4: Sample caption 6.5 Image To add image is as simple as: ![caption](image_path} For example, ![Sample image](smile.png) becomes: Sample image We may add width to change the size of the image, ![caption](image_path){width=} For example, ![Sample image](smile.png){width=25%} becomes: Sample image 6.5.1 Exercise 2 Create a report containing tables, figures and images. 6.6 Inline output R output can be included in an R Markdown document using a code chunk, for example, mean(iris$Petal.Width) ## [1] 1.199333 The code chunck can be included in between the text, for example The mean of petal width is `r mean(iris$Petal.Width)`. becomes: The mean of petal width is 1.1993333. 6.7 Equation 6.7.1 Mathematical equation We need the knowledge of mathematical expressions and symbols used in LaTeX to be able to utilize it in R Markdown. This can be learned from these good sources: A good starter: https://www.overleaf.com/learn/latex/Mathematical_expressions More advanced reference: https://en.wikibooks.org/wiki/LaTeX/Mathematics A (quite) an extensive list of mathematical symbols: https://oeis.org/wiki/List_of_LaTeX_mathematical_symbols 6.7.2 Inline equation Inline equation is included between two $ signs, for example, The equation is $y = \\beta_0 + \\beta_1 x$ The equation is \\(y = \\beta_0 + \\beta_1 x\\) 6.7.3 Display equation Display equation is included between two $$ signs, for example, The equation is, $$y = \\beta_0 + \\beta_1 x$$ The equation is, \\[y = \\beta_0 + \\beta_1 x\\] 6.7.4 Exercise 3 Create a report containing inline outputs and equations. "],["exploratory-data-analysis-eda.html", "Chapter 7 Exploratory data analysis (EDA) 7.1 Motivation 7.2 Questions to ask before making graphs 7.3 EDA using ggplot2 package 7.4 Loading the library 7.5 The dataset 7.6 Exploratory data analysis 7.7 One variable: Distribution of a categorical variable 7.8 One variable: Distribution of a numerical variable 7.9 Two variables: Plotting a numerical and a categorical variable 7.10 Line plot 7.11 Plotting means and error bars 7.12 Scatterplot with fit line", " Chapter 7 Exploratory data analysis (EDA) 7.1 Motivation In statistics, exploratory data analysis (EDA) is an approach in data analysis in order to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task. Exploratory data analysis was promoted by John Tukey to encourage statisticians to explore the data, and possibly formulate hypotheses that could lead to new data collection Source: https://en.wikipedia.org/wiki/Exploratory_data_analysis 7.2 Questions to ask before making graphs You must ask yourselves these: Which data do I want to use? data = Which variable or variables do I want to plot? mapping = aes() What is (or are) the type of that variable? Are they factor (categorical) variables ? Are they numerical variables? Am I going to plot a single variable? two variables together? three variables together? What plot? geom_ 7.3 EDA using ggplot2 package 7.3.1 Usage of ggplot2 start with ggplot() supply a dataset data = and aesthetic mapping (with aes()) - variables add on layers geom_X : geom_point(), geom_histogram() scales (like scale_colour_brewer()) faceting specifications (like facet_wrap()) coordinate systems (like coord_flip()). 7.4 Loading the library We will use two packages for data exploration: tidyverse package for plotting and data wrangling summarytools package for summary statistics patchwork to combine plots here to point to the correct working directory We will load the tidyverse and summarytools package The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. We may find more information about the package here https://www.tidyverse.org/ library(tidyverse) ## -- Attaching packages --------------------------------------- tidyverse 1.3.1 -- ## v ggplot2 3.3.5 v purrr 0.3.4 ## v tibble 3.1.6 v dplyr 1.0.7 ## v tidyr 1.1.4 v stringr 1.4.0 ## v readr 2.1.1 v forcats 0.5.1 ## Warning: package &#39;tibble&#39; was built under R version 4.1.2 ## -- Conflicts ------------------------------------------ tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(summarytools) ## ## Attaching package: &#39;summarytools&#39; ## The following object is masked from &#39;package:tibble&#39;: ## ## view library(patchwork) library(here) ## here() starts at C:/Tengku/Sync_PC_Laptop/tulis-buku/multivariable-analysis/fork-profKIM/multivar_data_analysis 7.5 The dataset Lets read the peptic ulcer data which in MS Excel format. To do this we will load the readxl package library(readxl) use the function read_xlsx() to read data into R pep &lt;- read_xlsx(here(&#39;data&#39;, &#39;peptic_ulcer.xlsx&#39;)) And examine the data number of observations type of variables name of variables glimpse(pep) ## Rows: 121 ## Columns: 34 ## $ age &lt;dbl&gt; 42, 66, 67, 19, 77, 39, 62, 71, 69, 97, 52, 21, 57~ ## $ gender &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, ~ ## $ epigastric_pain &lt;chr&gt; &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;~ ## $ vomiting &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, ~ ## $ nausea &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;~ ## $ fever &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;yes&quot;, ~ ## $ diarrhea &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, ~ ## $ malena &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;n~ ## $ onset_more_24_hrs &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;no&quot;~ ## $ NSAIDS &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;~ ## $ septic_shock &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;n~ ## $ previous_OGDS &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;~ ## $ ASA &lt;dbl&gt; 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 2, 2,~ ## $ systolic &lt;dbl&gt; 141, 197, 126, 90, 147, 115, 103, 159, 145, 105, 1~ ## $ diastolic &lt;dbl&gt; 98, 88, 73, 40, 82, 86, 55, 68, 75, 65, 74, 50, 86~ ## $ inotropes &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;n~ ## $ pulse &lt;dbl&gt; 109, 126, 64, 112, 89, 96, 100, 57, 86, 100, 109, ~ ## $ tenderness &lt;chr&gt; &quot;generalized&quot;, &quot;generalized&quot;, &quot;generalized&quot;, &quot;loca~ ## $ guarding &lt;chr&gt; &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;n~ ## $ hemoglobin &lt;dbl&gt; 18.0, 12.0, 12.0, 12.0, 11.0, 18.0, 8.1, 13.3, 11.~ ## $ twc &lt;dbl&gt; 6.0, 6.0, 13.0, 20.0, 21.0, 4.0, 5.0, 12.0, 6.0, 2~ ## $ platelet &lt;dbl&gt; 415, 292, 201, 432, 324, 260, 461, 210, 293, 592, ~ ## $ creatinine &lt;dbl&gt; 135, 66, 80, 64, 137, 102, 69, 92, 94, 104, 58, 24~ ## $ albumin &lt;chr&gt; &quot;27&quot;, &quot;28&quot;, &quot;32&quot;, &quot;42&quot;, &quot;38&quot;, &quot;38&quot;, &quot;30&quot;, &quot;41&quot;, &quot;N~ ## $ PULP &lt;dbl&gt; 2, 3, 3, 2, 7, 1, 2, 5, 3, 4, 2, 3, 4, 3, 5, 5, 1,~ ## $ admission_to_op_hrs &lt;dbl&gt; 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 6, 6, 6, 6,~ ## $ perforation &lt;dbl&gt; 0.5, 1.0, 0.5, 0.5, 1.0, 1.0, 3.0, 1.5, 0.5, 1.5, ~ ## $ degree_perforation &lt;chr&gt; &quot;small&quot;, &quot;small&quot;, &quot;small&quot;, &quot;small&quot;, &quot;small&quot;, &quot;smal~ ## $ side_perforation &lt;chr&gt; &quot;distal stomach&quot;, &quot;distal stomach&quot;, &quot;distal stomac~ ## $ ICU &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;, ~ ## $ SSSI &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;n~ ## $ anast_leak &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;n~ ## $ sepsis &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;~ ## $ outcome &lt;chr&gt; &quot;alive&quot;, &quot;alive&quot;, &quot;alive&quot;, &quot;alive&quot;, &quot;alive&quot;, &quot;aliv~ The are quite a number of variables. For the sake of simplicity, we will select variables which we think might be important for simple data exploration. To do that, we will use select() function pep &lt;- pep %&gt;% select(age, systolic, diastolic, hemoglobin, twc, ASA, PULP, perforation, gender, epigastric_pain, malena, tenderness, degree_perforation, outcome) 7.6 Exploratory data analysis 7.6.1 Statistics Overall results pep %&gt;% descr() ## Non-numerical variable(s) ignored: gender, epigastric_pain, malena, tenderness, degree_perforation, outcome ## Descriptive Statistics ## pep ## N: 121 ## ## age ASA diastolic hemoglobin perforation PULP systolic twc ## ----------------- -------- -------- ----------- ------------ ------------- -------- ---------- -------- ## Mean 60.43 1.55 72.07 12.32 1.22 3.53 128.56 13.03 ## Std.Dev 18.05 0.62 13.99 3.33 0.91 2.28 24.51 6.66 ## Min 19.00 1.00 38.00 3.30 0.30 0.00 67.00 2.00 ## Q1 49.00 1.00 63.00 10.00 0.50 2.00 112.00 9.00 ## Median 64.00 1.00 71.00 12.00 1.00 3.00 128.00 12.00 ## Q3 75.00 2.00 81.00 15.00 1.50 5.00 143.00 16.00 ## Max 97.00 3.00 116.00 19.40 5.00 9.00 197.00 37.00 ## MAD 17.79 0.00 13.34 2.97 0.74 2.97 22.24 5.93 ## IQR 26.00 1.00 18.00 5.00 1.00 3.00 31.00 7.00 ## CV 0.30 0.40 0.19 0.27 0.74 0.65 0.19 0.51 ## Skewness -0.55 0.66 0.12 -0.22 1.66 0.25 0.36 0.78 ## SE.Skewness 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 ## Kurtosis -0.53 -0.55 0.22 0.02 3.34 -0.91 0.58 0.60 ## N.Valid 121.00 121.00 121.00 121.00 121.00 121.00 121.00 121.00 ## Pct.Valid 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 pep %&gt;% freq() ## Variable(s) ignored: age, systolic, diastolic, hemoglobin, twc ## Frequencies ## pep$ASA ## Type: Numeric ## ## Freq % Valid % Valid Cum. % Total % Total Cum. ## ----------- ------ --------- -------------- --------- -------------- ## 1 63 52.07 52.07 52.07 52.07 ## 2 50 41.32 93.39 41.32 93.39 ## 3 8 6.61 100.00 6.61 100.00 ## &lt;NA&gt; 0 0.00 100.00 ## Total 121 100.00 100.00 100.00 100.00 ## ## pep$PULP ## Type: Numeric ## ## Freq % Valid % Valid Cum. % Total % Total Cum. ## ----------- ------ --------- -------------- --------- -------------- ## 0 10 8.26 8.26 8.26 8.26 ## 1 20 16.53 24.79 16.53 24.79 ## 2 11 9.09 33.88 9.09 33.88 ## 3 22 18.18 52.07 18.18 52.07 ## 4 21 17.36 69.42 17.36 69.42 ## 5 10 8.26 77.69 8.26 77.69 ## 6 8 6.61 84.30 6.61 84.30 ## 7 16 13.22 97.52 13.22 97.52 ## 8 2 1.65 99.17 1.65 99.17 ## 9 1 0.83 100.00 0.83 100.00 ## &lt;NA&gt; 0 0.00 100.00 ## Total 121 100.00 100.00 100.00 100.00 ## ## pep$perforation ## Type: Numeric ## ## Freq % Valid % Valid Cum. % Total % Total Cum. ## ----------- ------ --------- -------------- --------- -------------- ## 0.3 4 3.31 3.31 3.31 3.31 ## 0.5 43 35.54 38.84 35.54 38.84 ## 0.7 2 1.65 40.50 1.65 40.50 ## 0.8 2 1.65 42.15 1.65 42.15 ## 1 25 20.66 62.81 20.66 62.81 ## 1.5 18 14.88 77.69 14.88 77.69 ## 2 13 10.74 88.43 10.74 88.43 ## 2.5 3 2.48 90.91 2.48 90.91 ## 3 9 7.44 98.35 7.44 98.35 ## 5 2 1.65 100.00 1.65 100.00 ## &lt;NA&gt; 0 0.00 100.00 ## Total 121 100.00 100.00 100.00 100.00 ## ## pep$gender ## Type: Character ## ## Freq % Valid % Valid Cum. % Total % Total Cum. ## ------------ ------ --------- -------------- --------- -------------- ## female 25 20.66 20.66 20.66 20.66 ## male 96 79.34 100.00 79.34 100.00 ## &lt;NA&gt; 0 0.00 100.00 ## Total 121 100.00 100.00 100.00 100.00 ## ## pep$epigastric_pain ## Type: Character ## ## Freq % Valid % Valid Cum. % Total % Total Cum. ## ----------- ------ --------- -------------- --------- -------------- ## no 5 4.13 4.13 4.13 4.13 ## yes 116 95.87 100.00 95.87 100.00 ## &lt;NA&gt; 0 0.00 100.00 ## Total 121 100.00 100.00 100.00 100.00 ## ## pep$malena ## Type: Character ## ## Freq % Valid % Valid Cum. % Total % Total Cum. ## ----------- ------ --------- -------------- --------- -------------- ## no 117 96.69 96.69 96.69 96.69 ## yes 4 3.31 100.00 3.31 100.00 ## &lt;NA&gt; 0 0.00 100.00 ## Total 121 100.00 100.00 100.00 100.00 ## ## pep$tenderness ## Type: Character ## ## Freq % Valid % Valid Cum. % Total % Total Cum. ## ----------------- ------ --------- -------------- --------- -------------- ## generalized 84 69.42 69.42 69.42 69.42 ## localized 37 30.58 100.00 30.58 100.00 ## &lt;NA&gt; 0 0.00 100.00 ## Total 121 100.00 100.00 100.00 100.00 ## ## pep$degree_perforation ## Type: Character ## ## Freq % Valid % Valid Cum. % Total % Total Cum. ## -------------- ------ --------- -------------- --------- -------------- ## large 26 21.49 21.49 21.49 21.49 ## moderate 20 16.53 38.02 16.53 38.02 ## small 75 61.98 100.00 61.98 100.00 ## &lt;NA&gt; 0 0.00 100.00 ## Total 121 100.00 100.00 100.00 100.00 ## ## pep$outcome ## Type: Character ## ## Freq % Valid % Valid Cum. % Total % Total Cum. ## ----------- ------ --------- -------------- --------- -------------- ## alive 83 68.60 68.60 68.60 68.60 ## dead 38 31.40 100.00 31.40 100.00 ## &lt;NA&gt; 0 0.00 100.00 ## Total 121 100.00 100.00 100.00 100.00 Stratified for outcome pep %&gt;% group_by(outcome) %&gt;% descr() ## Non-numerical variable(s) ignored: gender, epigastric_pain, malena, tenderness, degree_perforation ## Descriptive Statistics ## pep ## Group: outcome = alive ## N: 83 ## ## age ASA diastolic hemoglobin perforation PULP systolic twc ## ----------------- -------- -------- ----------- ------------ ------------- -------- ---------- -------- ## Mean 58.20 1.48 73.25 12.74 0.95 3.16 130.95 12.75 ## Std.Dev 18.53 0.63 14.02 3.14 0.60 2.35 25.06 6.00 ## Min 19.00 1.00 40.00 4.50 0.30 0.00 70.00 3.70 ## Q1 43.00 1.00 65.00 11.00 0.50 1.00 113.00 8.00 ## Median 62.00 1.00 72.00 12.00 0.70 3.00 129.00 12.00 ## Q3 73.00 2.00 82.00 15.00 1.50 5.00 147.00 16.00 ## Max 97.00 3.00 116.00 19.40 3.00 9.00 197.00 32.00 ## MAD 19.27 0.00 13.34 2.97 0.30 2.97 25.20 5.93 ## IQR 28.50 1.00 16.50 4.00 0.75 4.00 33.50 8.00 ## CV 0.32 0.43 0.19 0.25 0.63 0.74 0.19 0.47 ## Skewness -0.45 0.93 0.15 0.07 1.41 0.43 0.34 0.69 ## SE.Skewness 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 ## Kurtosis -0.71 -0.24 0.35 -0.21 1.66 -0.87 0.06 0.29 ## N.Valid 83.00 83.00 83.00 83.00 83.00 83.00 83.00 83.00 ## Pct.Valid 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 ## ## Group: outcome = dead ## N: 38 ## ## age ASA diastolic hemoglobin perforation PULP systolic twc ## ----------------- -------- -------- ----------- ------------ ------------- -------- ---------- -------- ## Mean 65.29 1.68 69.50 11.41 1.82 4.34 123.34 13.64 ## Std.Dev 16.14 0.57 13.75 3.57 1.16 1.89 22.71 7.96 ## Min 23.00 1.00 38.00 3.30 0.30 1.00 67.00 2.00 ## Q1 57.00 1.00 60.00 10.00 1.00 3.00 111.00 9.00 ## Median 68.50 2.00 67.00 11.25 1.50 4.00 122.00 12.00 ## Q3 78.00 2.00 80.00 15.00 2.50 6.00 139.00 20.00 ## Max 92.00 3.00 100.00 17.00 5.00 8.00 197.00 37.00 ## MAD 15.57 0.00 12.60 3.71 0.74 1.48 18.53 7.41 ## IQR 21.00 1.00 19.50 4.75 1.50 2.75 26.50 11.00 ## CV 0.25 0.34 0.20 0.31 0.64 0.44 0.18 0.58 ## Skewness -0.71 0.11 0.04 -0.50 0.91 0.24 0.30 0.71 ## SE.Skewness 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.38 ## Kurtosis -0.17 -0.76 -0.32 -0.50 0.61 -0.99 1.94 0.11 ## N.Valid 38.00 38.00 38.00 38.00 38.00 38.00 38.00 38.00 ## Pct.Valid 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 with(pep, stby(outcome, degree_perforation, freq)) ## Frequencies ## pep$outcome ## Type: Character ## Group: degree_perforation = large ## ## Freq % Valid % Valid Cum. % Total % Total Cum. ## ----------- ------ --------- -------------- --------- -------------- ## alive 9 34.62 34.62 34.62 34.62 ## dead 17 65.38 100.00 65.38 100.00 ## &lt;NA&gt; 0 0.00 100.00 ## Total 26 100.00 100.00 100.00 100.00 ## ## Group: degree_perforation = moderate ## ## Freq % Valid % Valid Cum. % Total % Total Cum. ## ----------- ------ --------- -------------- --------- -------------- ## alive 13 65.00 65.00 65.00 65.00 ## dead 7 35.00 100.00 35.00 100.00 ## &lt;NA&gt; 0 0.00 100.00 ## Total 20 100.00 100.00 100.00 100.00 ## ## Group: degree_perforation = small ## ## Freq % Valid % Valid Cum. % Total % Total Cum. ## ----------- ------ --------- -------------- --------- -------------- ## alive 61 81.33 81.33 81.33 81.33 ## dead 14 18.67 100.00 18.67 100.00 ## &lt;NA&gt; 0 0.00 100.00 ## Total 75 100.00 100.00 100.00 100.00 with(pep, stby(outcome, gender, freq)) ## Frequencies ## pep$outcome ## Type: Character ## Group: gender = female ## ## Freq % Valid % Valid Cum. % Total % Total Cum. ## ----------- ------ --------- -------------- --------- -------------- ## alive 13 52.00 52.00 52.00 52.00 ## dead 12 48.00 100.00 48.00 100.00 ## &lt;NA&gt; 0 0.00 100.00 ## Total 25 100.00 100.00 100.00 100.00 ## ## Group: gender = male ## ## Freq % Valid % Valid Cum. % Total % Total Cum. ## ----------- ------ --------- -------------- --------- -------------- ## alive 70 72.92 72.92 72.92 72.92 ## dead 26 27.08 100.00 27.08 100.00 ## &lt;NA&gt; 0 0.00 100.00 ## Total 96 100.00 100.00 100.00 100.00 Another useful package is the gt package 7.6.2 Plots 7.7 One variable: Distribution of a categorical variable 7.7.1 Bar chart The frequency of the outcome pep %&gt;% group_by(outcome) %&gt;% summarise(freq = n()) ## # A tibble: 2 x 2 ## outcome freq ## &lt;chr&gt; &lt;int&gt; ## 1 alive 83 ## 2 dead 38 To plot the distribution of a categorical variable, we can use a bar chart. ggplot(data = pep) + geom_bar(mapping = aes(x = outcome)) But we can also pass the aes() to ggplot ggplot(data = pep, mapping = aes(x = outcome)) + geom_bar() Combining dplyr and ggplot (both are packages inside the tidyverse metapackage) dplyr part: pep_age &lt;- pep %&gt;% group_by(outcome) %&gt;% summarize(mean_age = mean(age)) pep_age ## # A tibble: 2 x 2 ## outcome mean_age ## &lt;chr&gt; &lt;dbl&gt; ## 1 alive 58.2 ## 2 dead 65.3 ggplot part: ggplot(pep_age, mapping = aes(x = outcome, y = mean_age)) + geom_col() dplyr and ggplot in action: pep %&gt;% group_by(outcome) %&gt;% summarize(mean_age = mean(age)) %&gt;% ggplot(mapping = aes(x = outcome, y = mean_age, fill = outcome)) + geom_col() + ylab(&quot;Mean age (Years)&quot;) Excellent resource from this website http://www.cookbook-r.com/Graphs/Bar_and_line_graphs_(ggplot2)/. It is a must go to website!! 7.8 One variable: Distribution of a numerical variable Plot distribution of values of a numerical variable 7.8.1 Histogram To see the distribution of a numerical variable, we can plot a histogram. To specify the number of bin, we can use bindwidth and add some customization ggplot(data = pep, mapping = aes(x = systolic)) + geom_histogram(binwidth = 10, fill = &quot;blue&quot;) + ylab(&quot;Frequency&quot;) + xlab(&quot;Systolic Blood Pressure&quot;) + ggtitle(&quot;Systolic BP distribution&quot;) ggplot2 has lots of flexibility and personalization. For example, the histogram above is very plain. We can improve it by setting the line color and fill color, the theme, the size, the symbols and many other parameters. 7.8.2 Density curve Let us create a density curve. Density curve is useful to examine the distribution of observations. ggplot(data = pep, mapping = aes(x = diastolic)) + geom_density() + xlab(&quot;Diastolic BP (mmHg)&quot;) + ylab(&quot;Density&quot;) + labs(title = &quot;Density distribution for diastolic BP&quot;, caption = &quot;Source : Peptic ulcer disease data&quot;) 7.8.3 Histogram and density curve together If we want to display both the histogram and the density curve, we can use geom_histogram() and geom_density() in the single line of codes. ggplot(pep, mapping = aes(x = diastolic)) + geom_histogram(mapping = aes(y = ..density..), binwidth = 10) + geom_density(colour = &#39;red&#39;) + xlab(&quot;Diastolic BP (mmHg)&quot;) + ylab(&quot;Density&quot;) + labs(title = &quot;Density distribution for diastolic BP&quot;, caption = &quot;Source : Peptic ulcer disease data&quot;) + theme_bw() 7.9 Two variables: Plotting a numerical and a categorical variable 7.9.1 Overlaying histogramss and boxplot Now, examine the distribution of a numerical variable (var age) based on variable outcome by overlaying histograms. hist_age &lt;- ggplot(data = pep, aes(x = age, fill = outcome)) + geom_histogram(binwidth = 5, aes(y = ..density..), position = &quot;identity&quot;, alpha = 0.75) + geom_density(alpha = 0.25) + xlab(&quot;Age&quot;) + ylab(&quot;Density&quot;) + labs(title = &quot;Density distribution&quot;, caption = &quot;Source : Peptic ulcer disease data&quot;) + theme_bw() box_age &lt;- ggplot(data = pep, aes(x = outcome, y = age)) + geom_boxplot(outlier.shape = NA) + geom_dotplot(binaxis = &quot;y&quot;, binwidth = 1, fill = NA, colour = &quot;blue&quot;, alpha = 0.75) + xlab(&#39;Outcome&#39;) + ylab(&#39;Age&#39;) + labs(title = &quot;Box-plot&quot;, caption = &quot;Source : Peptic ulcer disease data&quot;) + theme_bw() hist_age | box_age ## Three variables: Plotting a numerical and two categorical variables It is hard to visualize three variables in a single histogram plot. Perhaps we can use facet_.() function to split the plots. 7.9.2 Faceting the plots We can see better plots if we split the histogram based on certain grouping. In this example, we stratify the distribution of variable age (a numerical variable) based on outcome and gender (both are categorical variables) ggplot(data = pep, aes(x = age, fill = gender)) + geom_histogram(binwidth = 5, aes(y = ..density..), position = &quot;identity&quot;, alpha = 0.25) + geom_density(aes(colour = gender), alpha = 0.55) + xlab(&quot;Age&quot;) + ylab(&quot;Density&quot;) + labs(title = &quot;Density distribution of age for outcome and gender&quot;, caption = &quot;Source : Peptic ulcer disease data&quot;) + theme_bw() + facet_wrap( ~ outcome) And the summary statistics for the plots are pep %&gt;% group_by(outcome, gender) %&gt;% summarize(mean_age = mean(age, na.rm = TRUE), sd_age = sd(age, na.rm = TRUE)) ## `summarise()` has grouped output by &#39;outcome&#39;. You can override using the `.groups` argument. ## # A tibble: 4 x 4 ## # Groups: outcome [2] ## outcome gender mean_age sd_age ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 alive female 70.2 11.4 ## 2 alive male 56.0 18.8 ## 3 dead female 70.1 11.2 ## 4 dead male 63.1 17.7 7.10 Line plot Line graphs are typically used for visualizing how one continuous variable, on the y-axis, changes in relation to another continuous variable, on the x-axis. This is very useful for longitudinal data. Load gapminder package so we can use the gapminder data library(gapminder) As with bar graphs, there are exceptions. Line graphs can also be used with a discrete variable on the x-axis. This is appropriate when the variable is ordered (e.g., small, medium, large). How about the life expectancy for Asia continent and also Malaysia in comparison gapminder data continent == Asia life expectancy trend gapminder %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% ggplot(mapping = aes(x = year, y = lifeExp, colour = country)) + geom_line(show.legend = FALSE) And the summary statistics gapminder %&gt;% filter(continent == &#39;Asia&#39;) %&gt;% filter(year == 1962 | year == 1982 | year == 2002) %&gt;% group_by(year) %&gt;% summarise(mean_life = mean(lifeExp, na.rm = TRUE), sd_life = sd(lifeExp, na.rm = TRUE)) ## # A tibble: 3 x 3 ## year mean_life sd_life ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1962 51.6 9.82 ## 2 1982 62.6 8.54 ## 3 2002 69.2 8.37 7.11 Plotting means and error bars We want to error bar for life expectancy for Asia continent only. Error bar that will contain mean standard deviation Our approach is: use filter to choose Asia continent only filter() generate the mean and SD for life expectancy using mutate() plot the scatterplot (country vs mean life expectancy) geom_point() plot errorbar geom_errorbar() gap_continent &lt;- gapminder %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% group_by(country) %&gt;% mutate(mean = mean(lifeExp), sd = sd(lifeExp)) %&gt;% arrange(desc(mean)) Plot them with coord_flip() gap_continent %&gt;% ggplot(mapping = aes(x = country, y = mean)) + geom_point(mapping = aes(x = country, y = mean)) + geom_errorbar(mapping = aes(ymin = mean - sd, ymax = mean + sd), position = position_dodge()) + coord_flip() 7.12 Scatterplot with fit line The steps below shows that: data is pep fit line between age and size of perforation plot scatterplot plot fit line pep_fit &lt;- pep %&gt;% ggplot(mapping = aes(x = age, y = perforation, colour = outcome)) + geom_point(show.legend = FALSE) + geom_smooth(method = lm, se = FALSE) + ylab(&#39;Size of perforation&#39;) + xlab(&#39;Age of patient&#39;) + labs(title = &#39;Distribution and fit line&#39;, caption = &#39;Source: Peptic ulcer data&#39;) + theme_bw() pep_fit ## `geom_smooth()` using formula &#39;y ~ x&#39; Now, let us see if the patterns are similar for men and women. We will use facet_wrap() to split the plots based on variable gender pep_fit + facet_grid(. ~ gender) ## `geom_smooth()` using formula &#39;y ~ x&#39; References: ggplot2: Elegant Graphics for Data Analysis. https://ggplot2-book.org/arranging-plots.html ggplot2. https://ggplot2.tidyverse.org/ gapminder. https://cran.r-project.org/web/packages/gapminder/README.html "],["linear-regression.html", "Chapter 8 Linear Regression 8.1 Simple linear regression (SLR) 8.2 Multiple linear regression (MLR) 8.3 Model fit assessment: Residuals 8.4 Interpretation 8.5 Model equation 8.6 Prediction", " Chapter 8 Linear Regression A statistical method to model relationship between: outcome: numerical variable. predictors/independent variables: numerical, categorical variables. A type of Generalized Linear Models (GLMs), which also includes other outcome types, e.g. categorical and count. Basically, the linear relationship is structured as follows, \\[numerical\\ outcome = numerical\\ predictors + categorical\\ predictors\\] 8.1 Simple linear regression (SLR) 8.1.1 About SLR Model linear (straight line) relationship between: outcome: numerical variable. a predictor: numerical variable (only). Note: What if the predictor is a categorical variable? Remember, we already handled that with one-way ANOVA. Formula, \\[numerical\\ outcome = intercept + coefficient \\times numerical\\ predictor\\] in short, \\[\\hat y = \\beta_0 + \\beta_1x_1\\] where \\(\\hat y\\) is the predicted value of the outcome y. 8.1.2 Analysis 8.1.2.1 Libraries # library library(foreign) library(epiDisplay) ## Loading required package: survival ## Loading required package: MASS ## Loading required package: nnet library(psych) ## ## Attaching package: &#39;psych&#39; ## The following objects are masked from &#39;package:epiDisplay&#39;: ## ## alpha, cs, lookup library(lattice) ## ## Attaching package: &#39;lattice&#39; ## The following object is masked from &#39;package:epiDisplay&#39;: ## ## dotplot library(rsq) library(car) ## Loading required package: carData ## ## Attaching package: &#39;car&#39; ## The following object is masked from &#39;package:psych&#39;: ## ## logit library(broom) library(tidyverse) ## -- Attaching packages --------------------------------------- tidyverse 1.3.1 -- ## v ggplot2 3.3.5 v purrr 0.3.4 ## v tibble 3.1.6 v dplyr 1.0.7 ## v tidyr 1.1.4 v stringr 1.4.0 ## v readr 2.1.1 v forcats 0.5.1 ## Warning: package &#39;tibble&#39; was built under R version 4.1.2 ## -- Conflicts ------------------------------------------ tidyverse_conflicts() -- ## x ggplot2::%+%() masks psych::%+%() ## x ggplot2::alpha() masks psych::alpha(), epiDisplay::alpha() ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() ## x dplyr::recode() masks car::recode() ## x dplyr::select() masks MASS::select() ## x purrr::some() masks car::some() 8.1.2.2 Data set # data coronary = read.dta(here::here(&quot;data&quot;,&quot;coronary.dta&quot;)) str(coronary) ## &#39;data.frame&#39;: 200 obs. of 9 variables: ## $ id : num 1 14 56 61 62 64 69 108 112 134 ... ## $ cad : Factor w/ 2 levels &quot;no cad&quot;,&quot;cad&quot;: 1 1 1 1 1 1 2 1 1 1 ... ## $ sbp : num 106 130 136 138 115 124 110 112 138 104 ... ## $ dbp : num 68 78 84 100 85 72 80 70 85 70 ... ## $ chol : num 6.57 6.33 5.97 7.04 6.66 ... ## $ age : num 60 34 36 45 53 43 44 50 43 48 ... ## $ bmi : num 38.9 37.8 40.5 37.6 40.3 ... ## $ race : Factor w/ 3 levels &quot;malay&quot;,&quot;chinese&quot;,..: 3 1 1 1 3 1 1 2 2 2 ... ## $ gender: Factor w/ 2 levels &quot;woman&quot;,&quot;man&quot;: 1 1 1 1 2 2 2 1 1 2 ... ## - attr(*, &quot;datalabel&quot;)= chr &quot;Written by R. &quot; ## - attr(*, &quot;time.stamp&quot;)= chr &quot;&quot; ## - attr(*, &quot;formats&quot;)= chr [1:9] &quot;%9.0g&quot; &quot;%9.0g&quot; &quot;%9.0g&quot; &quot;%9.0g&quot; ... ## - attr(*, &quot;types&quot;)= int [1:9] 100 108 100 100 100 100 100 108 108 ## - attr(*, &quot;val.labels&quot;)= chr [1:9] &quot;&quot; &quot;cad&quot; &quot;&quot; &quot;&quot; ... ## - attr(*, &quot;var.labels&quot;)= chr [1:9] &quot;id&quot; &quot;cad&quot; &quot;sbp&quot; &quot;dbp&quot; ... ## - attr(*, &quot;version&quot;)= int 7 ## - attr(*, &quot;label.table&quot;)=List of 3 ## ..$ cad : Named int [1:2] 1 2 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;no cad&quot; &quot;cad&quot; ## ..$ race : Named int [1:3] 1 2 3 ## .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;malay&quot; &quot;chinese&quot; &quot;indian&quot; ## ..$ gender: Named int [1:2] 1 2 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;woman&quot; &quot;man&quot; 8.1.3 Data exploration 8.1.3.1 Descriptive statistics summ(coronary[c(&quot;chol&quot;, &quot;dbp&quot;)]) ## ## No. of observations = 200 ## ## Var. name obs. mean median s.d. min. max. ## 1 chol 200 6.2 6.19 1.18 4 9.35 ## 2 dbp 200 82.31 80 12.9 56 120 8.1.3.2 Plots multi.hist(coronary[c(&quot;chol&quot;, &quot;dbp&quot;)], ncol = 2) par(mfrow = c(1, 2)) mapply(boxplot, coronary[c(&quot;chol&quot;, &quot;dbp&quot;)], main = colnames(coronary[c(&quot;chol&quot;, &quot;dbp&quot;)])) ## chol dbp ## stats numeric,5 numeric,5 ## n 200 200 ## conf numeric,2 numeric,2 ## out 9.35 numeric,0 ## group 1 numeric,0 ## names &quot;&quot; &quot;&quot; par(mfrow = c(1, 1)) 8.1.4 Univariable Fit model, # model: chol ~ dbp slr_chol = glm(chol ~ dbp, data = coronary) summary(slr_chol) ## ## Call: ## glm(formula = chol ~ dbp, data = coronary) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9967 -0.8304 -0.1292 0.7734 2.8470 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.995134 0.492092 6.087 5.88e-09 *** ## dbp 0.038919 0.005907 6.589 3.92e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 1.154763) ## ## Null deviance: 278.77 on 199 degrees of freedom ## Residual deviance: 228.64 on 198 degrees of freedom ## AIC: 600.34 ## ## Number of Fisher Scoring iterations: 2 Confint(slr_chol) # 95% CI ## Estimate 2.5 % 97.5 % ## (Intercept) 2.99513427 2.03065127 3.95961727 ## dbp 0.03891876 0.02734161 0.05049591 Important results, Coefficient, \\(\\beta\\). 95% CI. P-value. Obtain \\(R^2\\), % of variance explained, rsq(slr_chol, adj = T) ## [1] 0.1756834 Scatter plot, plot(chol ~ dbp, data = coronary) abline(slr_chol) this allows assessment of normality, linearity and equal variance assumptions. We expect eliptical/oval shape (normality), equal scatter of dots on both sides of the prediction line (equal variance). Both these indicate linear relationship between chol and dbp. 8.1.4.1 Interpretation 1mmHg increase in DBP causes 0.04mmol/L increase in cholestrol. DBP explains 17.6% variance in cholestrol. 8.1.4.2 Model equation \\[chol = 3.0 + 0.04\\times dbp\\] 8.2 Multiple linear regression (MLR) 8.2.1 About MLR Model linear relationship between: outcome: numerical variable. predictors: numerical, categorical variables. Note: MLR is a term that refers to linear regression with two or more numerical variables. Whenever we have both numerical and categorical variables, the proper term for the regression model is General Linear Model. However, we will use the term MLR in this workshop. Formula, \\[\\begin{aligned} numerical\\ outcome = &amp;\\ intercept + coefficients \\times numerical\\ predictors \\\\ &amp; + coefficients \\times categorical\\ predictors \\end{aligned}\\] in a shorter form, \\[\\hat y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_kx_k\\] where we have k predictors. Whenever the predictor is a categorical variable with more than two levels, we use dummy variable(s). This can be easily specified in R using factor() if the variable is not yet properly specified as such. There is no problem with binary categorical variable. For a categorical variable with more than two levels, the number of dummy variables (i.e. once turned into several binary variables) equals number of levels minus one. For example, whenever we have four levels, we will obtain three dummy (binary) variables. 8.2.2 Analysis 8.2.2.1 Review data set # data str(coronary) ## &#39;data.frame&#39;: 200 obs. of 9 variables: ## $ id : num 1 14 56 61 62 64 69 108 112 134 ... ## $ cad : Factor w/ 2 levels &quot;no cad&quot;,&quot;cad&quot;: 1 1 1 1 1 1 2 1 1 1 ... ## $ sbp : num 106 130 136 138 115 124 110 112 138 104 ... ## $ dbp : num 68 78 84 100 85 72 80 70 85 70 ... ## $ chol : num 6.57 6.33 5.97 7.04 6.66 ... ## $ age : num 60 34 36 45 53 43 44 50 43 48 ... ## $ bmi : num 38.9 37.8 40.5 37.6 40.3 ... ## $ race : Factor w/ 3 levels &quot;malay&quot;,&quot;chinese&quot;,..: 3 1 1 1 3 1 1 2 2 2 ... ## $ gender: Factor w/ 2 levels &quot;woman&quot;,&quot;man&quot;: 1 1 1 1 2 2 2 1 1 2 ... ## - attr(*, &quot;datalabel&quot;)= chr &quot;Written by R. &quot; ## - attr(*, &quot;time.stamp&quot;)= chr &quot;&quot; ## - attr(*, &quot;formats&quot;)= chr [1:9] &quot;%9.0g&quot; &quot;%9.0g&quot; &quot;%9.0g&quot; &quot;%9.0g&quot; ... ## - attr(*, &quot;types&quot;)= int [1:9] 100 108 100 100 100 100 100 108 108 ## - attr(*, &quot;val.labels&quot;)= chr [1:9] &quot;&quot; &quot;cad&quot; &quot;&quot; &quot;&quot; ... ## - attr(*, &quot;var.labels&quot;)= chr [1:9] &quot;id&quot; &quot;cad&quot; &quot;sbp&quot; &quot;dbp&quot; ... ## - attr(*, &quot;version&quot;)= int 7 ## - attr(*, &quot;label.table&quot;)=List of 3 ## ..$ cad : Named int [1:2] 1 2 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;no cad&quot; &quot;cad&quot; ## ..$ race : Named int [1:3] 1 2 3 ## .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;malay&quot; &quot;chinese&quot; &quot;indian&quot; ## ..$ gender: Named int [1:2] 1 2 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;woman&quot; &quot;man&quot; We exclude id, cad and age from our data for the purpose of this analysis, keeping only sbp , dbp, bmi, race and gender. We will add age later in the exercise. coronary &lt;- coronary %&gt;% dplyr::select(-id, -cad, -age) # remove id, cad, age from our data since we&#39;re not going to use them, # easier to specifiy multivariable model. 8.2.2.2 Data exploration 8.2.2.2.1 Descriptive statistics summ(coronary[c(&quot;chol&quot;, &quot;sbp&quot;, &quot;dbp&quot;, &quot;bmi&quot;)]) ## ## No. of observations = 200 ## ## Var. name obs. mean median s.d. min. max. ## 1 chol 200 6.2 6.19 1.18 4 9.35 ## 2 sbp 200 130.18 126 19.81 88 187 ## 3 dbp 200 82.31 80 12.9 56 120 ## 4 bmi 200 37.45 37.8 2.68 28.99 45.03 codebook(coronary[c(&quot;race&quot;, &quot;gender&quot;)]) ## ## ## ## race : ## Frequency Percent ## malay 73 36.5 ## chinese 64 32.0 ## indian 63 31.5 ## ## ================== ## gender : ## Frequency Percent ## woman 100 50 ## man 100 50 ## ## ================== 8.2.2.2.2 Plots plot(coronary) multi.hist(coronary[c(&quot;chol&quot;, &quot;sbp&quot;, &quot;dbp&quot;, &quot;bmi&quot;)]) par(mfrow = c(2, 2)) mapply(boxplot, coronary[c(&quot;chol&quot;, &quot;sbp&quot;, &quot;dbp&quot;, &quot;bmi&quot;)], main = colnames(coronary[c(&quot;chol&quot;, &quot;sbp&quot;, &quot;dbp&quot;, &quot;bmi&quot;)])) ## chol sbp dbp bmi ## stats numeric,5 numeric,5 numeric,5 numeric,5 ## n 200 200 200 200 ## conf numeric,2 numeric,2 numeric,2 numeric,2 ## out 9.35 numeric,0 numeric,0 numeric,8 ## group 1 numeric,0 numeric,0 numeric,8 ## names &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; par(mfrow = c(1, 1)) par(mfrow = c(1, 2)) boxplot(chol ~ race, data = coronary) boxplot(chol ~ gender, data = coronary) par(mfrow = c(1, 1)) 8.2.2.3 Variable selection 8.2.2.4 slr We want to choose only variables with P-values &lt; 0.25 to be included in MLR. Obtaining the P-values for each variable is easy by LR test, slr_chol0 = glm(chol ~ 1, data = coronary) summary(slr_chol0) ## ## Call: ## glm(formula = chol ~ 1, data = coronary) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.19854 -0.80854 -0.01104 0.69021 3.15146 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.19854 0.08369 74.06 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 1.400874) ## ## Null deviance: 278.77 on 199 degrees of freedom ## Residual deviance: 278.77 on 199 degrees of freedom ## AIC: 637.99 ## ## Number of Fisher Scoring iterations: 2 8.2.2.4.1 Multivariable Perform MLR with all selected variables, # all mlr_chol = glm(chol ~ sbp + dbp + bmi + race, data = coronary) #mlr_chol = glm(chol ~ ., data = coronary) # shortcut summary(mlr_chol) ## ## Call: ## glm(formula = chol ~ sbp + dbp + bmi + race, data = coronary) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.17751 -0.73860 -0.02674 0.63163 2.90926 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.842338 1.265149 3.827 0.000175 *** ## sbp 0.000975 0.006990 0.139 0.889210 ## dbp 0.028350 0.010327 2.745 0.006615 ** ## bmi -0.038537 0.028170 -1.368 0.172879 ## racechinese 0.354039 0.183169 1.933 0.054710 . ## raceindian 0.716327 0.200346 3.575 0.000441 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 1.089387) ## ## Null deviance: 278.77 on 199 degrees of freedom ## Residual deviance: 211.34 on 194 degrees of freedom ## AIC: 592.61 ## ## Number of Fisher Scoring iterations: 2 rsq(mlr_chol, adj = T) ## [1] 0.2223518 Focus on, Coefficients, \\(\\beta\\)s. 95% CI. P-values. For model fit, \\(R^2\\)  % of variance explained by the model. Akaike Information Criterion, AIC  for comparison with other models. This is not useful alone, but for comparison with other models. The model with the lowest AIC is the best model. Looking at all these results, we choose: chol ~ dbp + race which has the lowest AIC. mlr_chol1 = glm(chol ~ dbp + race, data = coronary) summary(mlr_chol1) ## ## Call: ## glm(formula = chol ~ dbp + race, data = coronary) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1378 -0.7068 -0.0289 0.5997 2.7778 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.298028 0.486213 6.783 1.36e-10 *** ## dbp 0.031108 0.006104 5.096 8.14e-07 *** ## racechinese 0.359964 0.182149 1.976 0.049534 * ## raceindian 0.713690 0.190883 3.739 0.000243 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 1.088777) ## ## Null deviance: 278.77 on 199 degrees of freedom ## Residual deviance: 213.40 on 196 degrees of freedom ## AIC: 590.55 ## ## Number of Fisher Scoring iterations: 2 Our chosen model: mlr_chol1: chol ~ dbp + race summary(mlr_chol1) ## ## Call: ## glm(formula = chol ~ dbp + race, data = coronary) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1378 -0.7068 -0.0289 0.5997 2.7778 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.298028 0.486213 6.783 1.36e-10 *** ## dbp 0.031108 0.006104 5.096 8.14e-07 *** ## racechinese 0.359964 0.182149 1.976 0.049534 * ## raceindian 0.713690 0.190883 3.739 0.000243 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 1.088777) ## ## Null deviance: 278.77 on 199 degrees of freedom ## Residual deviance: 213.40 on 196 degrees of freedom ## AIC: 590.55 ## ## Number of Fisher Scoring iterations: 2 Confint(mlr_chol1) # 95% CI of the coefficients ## Estimate 2.5 % 97.5 % ## (Intercept) 3.29802826 2.345067995 4.25098852 ## dbp 0.03110811 0.019143668 0.04307255 ## racechinese 0.35996365 0.002958566 0.71696873 ## raceindian 0.71369024 0.339566932 1.08781356 Compare this model with the no-variable model and all-variable model by LR test and AIC comparison, # LR test anova(slr_chol0, mlr_chol1, test = &quot;LRT&quot;) # sig. better than no var at all! ## Analysis of Deviance Table ## ## Model 1: chol ~ 1 ## Model 2: chol ~ dbp + race ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 199 278.77 ## 2 196 213.40 3 65.373 5.755e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # model with no var at all is called Null Model anova(mlr_chol, mlr_chol1, test = &quot;LRT&quot;) # no sig. dif with all vars model, ## Analysis of Deviance Table ## ## Model 1: chol ~ sbp + dbp + bmi + race ## Model 2: chol ~ dbp + race ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 194 211.34 ## 2 196 213.40 -2 -2.0593 0.3886 # model with 2 vars (dbp &amp; race) is just as good as full model (with all the vars) # model with all vars is called Saturated Model # AIC AIC(slr_chol0, mlr_chol1, mlr_chol) ## df AIC ## slr_chol0 2 637.9921 ## mlr_chol1 5 590.5459 ## mlr_chol 7 592.6065 # our final model has the lowest AIC 8.2.3 Multicollinearity, MC Multicollinearity is the problem of repetitive/redundant variables  high correlations between predictors. MC is checked by Variance Inflation Factor (VIF). VIF &gt; 10 indicates MC problem. vif(mlr_chol1) # all &lt; 10 ## GVIF Df GVIF^(1/(2*Df)) ## dbp 1.132753 1 1.064309 ## race 1.132753 2 1.031653 8.2.4 Interaction, * Interaction is the predictor variable combination that requires interpretation of regression coefficients separately based on the levels of the predictor (e.g. separate analysis for each race group, Malay vs Chinese vs Indian). This makes interpreting our analysis complicated. So, most of the time, we pray not to have interaction in our regression model. summary(glm(chol ~ dbp*race, data = coronary)) # dbp*race not sig. ## ## Call: ## glm(formula = chol ~ dbp * race, data = coronary) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.10485 -0.77524 -0.02423 0.58059 2.74380 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.11114 0.92803 2.275 0.024008 * ## dbp 0.04650 0.01193 3.897 0.000134 *** ## racechinese 1.95576 1.28477 1.522 0.129572 ## raceindian 2.41530 1.25766 1.920 0.056266 . ## dbp:racechinese -0.02033 0.01596 -1.273 0.204376 ## dbp:raceindian -0.02126 0.01529 -1.391 0.165905 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 1.087348) ## ## Null deviance: 278.77 on 199 degrees of freedom ## Residual deviance: 210.95 on 194 degrees of freedom ## AIC: 592.23 ## ## Number of Fisher Scoring iterations: 2 # in R, it is easy to fit interaction by * # dbp*race will automatically include all vars involved i.e. equal to # glm(chol ~ dbp + race + dbp:race, data = coronary) # use : to just include just the interaction There is no interaction here because the included interaction term was insignificant. 8.3 Model fit assessment: Residuals Histogram Raw residuals: Normality assumption. rraw_chol = resid(mlr_chol1) # unstandardized multi.hist(rraw_chol) Scatter plots Standardized residuals vs Standardized predicted values: Overall  normality, linearity and equal variance assumptions. rstd_chol = rstandard(mlr_chol1) # standardized residuals pstd_chol = scale(predict(mlr_chol1)) # standardized predicted values plot(rstd_chol ~ pstd_chol, xlab = &quot;Std predicted&quot;, ylab = &quot;Std residuals&quot;) abline(0, 0) # normal, linear, equal variance The dots should form elliptical/oval shape (normality) and scattered roughly equal above and below the zero line (equal variance). Both these indicate linearity. Raw residuals vs Numerical predictor by each predictors: Linearity assumption. plot(rraw_chol ~ coronary$dbp, xlab = &quot;DBP&quot;, ylab = &quot;Raw Residuals&quot;) abline(0, 0) 8.4 Interpretation Now we have decided on our final model, rename the model, # rename the selected model mlr_chol_final = mlr_chol1 and interpret the model, summary(mlr_chol_final) ## ## Call: ## glm(formula = chol ~ dbp + race, data = coronary) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1378 -0.7068 -0.0289 0.5997 2.7778 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.298028 0.486213 6.783 1.36e-10 *** ## dbp 0.031108 0.006104 5.096 8.14e-07 *** ## racechinese 0.359964 0.182149 1.976 0.049534 * ## raceindian 0.713690 0.190883 3.739 0.000243 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 1.088777) ## ## Null deviance: 278.77 on 199 degrees of freedom ## Residual deviance: 213.40 on 196 degrees of freedom ## AIC: 590.55 ## ## Number of Fisher Scoring iterations: 2 Confint(mlr_chol_final) # 95% CI of the coefficients ## Estimate 2.5 % 97.5 % ## (Intercept) 3.29802826 2.345067995 4.25098852 ## dbp 0.03110811 0.019143668 0.04307255 ## racechinese 0.35996365 0.002958566 0.71696873 ## raceindian 0.71369024 0.339566932 1.08781356 rsq(mlr_chol_final, adj = T) ## [1] 0.2227869 1mmHg increase in DBP causes 0.03mmol/L increase in cholestrol, controlling for the effect of race. Being Chinese causes 0.36mmol/L increase in cholestrol in comparison to Malay, controlling for the effect of DBP. Being Indian causes 0.71mmol/L increase in cholestrol in comparison to Malay, controlling for the effect of DBP. DBP and race explains 22.3% variance in cholestrol. Turn the results into data frames results using broom, tib_mlr = tidy(mlr_chol_final, conf.int = T); tib_mlr ## # A tibble: 4 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 3.30 0.486 6.78 1.36e-10 2.35 4.25 ## 2 dbp 0.0311 0.00610 5.10 8.14e- 7 0.0191 0.0431 ## 3 racechinese 0.360 0.182 1.98 4.95e- 2 0.00296 0.717 ## 4 raceindian 0.714 0.191 3.74 2.43e- 4 0.340 1.09 Display the results using kable in a nice table, knitr::kable(tib_mlr) term estimate std.error statistic p.value conf.low conf.high (Intercept) 3.2980283 0.4862132 6.783091 0.0000000 2.3450680 4.2509885 dbp 0.0311081 0.0061044 5.095998 0.0000008 0.0191437 0.0430726 racechinese 0.3599636 0.1821488 1.976207 0.0495342 0.0029586 0.7169687 raceindian 0.7136902 0.1908827 3.738893 0.0002425 0.3395669 1.0878136 We can export the results into a .csv file for use later (e.g. to prepare a table for journal articles etc.), write.csv(tib_mlr, &quot;mlr_final.csv&quot;) 8.5 Model equation Cholestrol level in mmol/L can be predicted by its predictors as given by, \\[chol = 3.30 + 0.03\\times dbp + 0.36\\times race\\ (chinese) + 0.71\\times race\\ (indian)\\] 8.6 Prediction It is easy to predict in R using our fitted model above. First we view the predicted values for our sample, coronary$pred_chol = predict(mlr_chol_final) head(coronary) ## sbp dbp chol bmi race gender pred_chol ## 1 106 68 6.5725 38.9 indian woman 6.127070 ## 2 130 78 6.3250 37.8 malay woman 5.724461 ## 3 136 84 5.9675 40.5 malay woman 5.911109 ## 4 138 100 7.0400 37.6 malay woman 6.408839 ## 5 115 85 6.6550 40.3 indian man 6.655908 ## 6 124 72 5.9675 37.6 malay man 5.537812 Now let us try predicting for any values for dbp and race, str(coronary[c(&quot;dbp&quot;, &quot;race&quot;)]) ## &#39;data.frame&#39;: 200 obs. of 2 variables: ## $ dbp : num 68 78 84 100 85 72 80 70 85 70 ... ## $ race: Factor w/ 3 levels &quot;malay&quot;,&quot;chinese&quot;,..: 3 1 1 1 3 1 1 2 2 2 ... # simple, dbp = 90, race = indian predict(mlr_chol_final, list(dbp = 90, race = &quot;indian&quot;)) ## 1 ## 6.811448 More data points new_data = data.frame(dbp = c(90, 90, 90), race = c(&quot;malay&quot;, &quot;chinese&quot;, &quot;indian&quot;)) new_data ## dbp race ## 1 90 malay ## 2 90 chinese ## 3 90 indian predict(mlr_chol_final, new_data) ## 1 2 3 ## 6.097758 6.457722 6.811448 new_data$pred_chol = predict(mlr_chol_final, new_data) new_data ## dbp race pred_chol ## 1 90 malay 6.097758 ## 2 90 chinese 6.457722 ## 3 90 indian 6.811448 detach(&quot;package:car&quot;, unload=TRUE) detach(&quot;package:epiDisplay&quot;, unload=TRUE) detach(&quot;package:psych&quot;, unload=TRUE) detach(&quot;package:rsq&quot;, unload=TRUE) detach(&quot;package:MASS&quot;, unload=TRUE) ## Warning: &#39;MASS&#39; namespace cannot be unloaded: ## namespace &#39;MASS&#39; is imported by &#39;lme4&#39; so cannot be unloaded "],["binary-logistic-regression.html", "Chapter 9 Binary Logistic Regression 9.1 Introduction 9.2 Dataset 9.3 The logit and logistic models 9.4 Prepare environment for analysis 9.5 Read data 9.6 Explore data 9.7 Estimate the regression parameters 9.8 Simple binary logistic regression 9.9 Multiple binary logistic regression 9.10 Convert the log odds to odds ratio 9.11 Making inference 9.12 Model comparison 9.13 Adding an interaction term 9.14 Prediction from binary logistic regression 9.15 Model fitness 9.16 Presentation 9.17 References", " Chapter 9 Binary Logistic Regression 9.1 Introduction 9.1.1 Background The logistic model (or logit model) is used to model the probability of a certain class or event existing such as pass or fail, win or lose, alive or dead or healthy or sick. More specifically, binary logistic regression is used to model the relationship between a covariate or a set of covariates and an outcome variables which is a binary variable. A binary variable is a categorical outcome has two categories or levels. In medical and health research, binary outcome variable is very common. Some example where the outcome is binary include: survival status when the status of cancer patients at the end of treatment are coded as either alive or dead relapse status when the status of a patient is coded as either relapse or not relapse satisfaction level when patients who come to clinics are asked if they are satisfied or not satisfied with the service glucose control when patients were categorized as either good control or poor control based on Hba1c In a binary logistic regression model, the dependent variable has two levels (categorical). When the outcome variable has more than two levels or categories, the analysis are modeled by multinomial logistic regression and, if the multiple categories are ordered, by ordinal logistic regression (for example the proportional odds ordinal logistic model). 9.1.2 Objectives At the end of the chapter, the readers will be to understand the concept of simple and multiple binary logistic regression to perform simple binary logistic regression to perform multiple binary logistic regression to perform model assessment of binary logistic regression to present and interpret results from binary logsitic regression 9.1.3 Further readings There are a number of good references to help readers understand binary logistic regression better. The references that we list below also contains workflow that wil be useful for readers when modelling logistic regression. We highly recommend readers to read the Applied Logistic Regression book (hosmer2013applied?) the Logistic Regression: Self Learning Text (kleinbaum2010logistic?) the workflow from A Handbook of Statistical Analyses Using R (R-HSAUR?) 9.2 Dataset We will use a dataset named stroke.dta which in STATA format. These data come from a study of hospitalized stroke patients. They original dataset contain 12 variables but our main variables of interest are: status : Status of patient during hospitalization (alive or dead) gcs : Glasgow Coma Scale on admission (range from 3 to 15) stroke_type : IS (Ischaemic Stroke) or HS (Haemorrhagic Stroke) sex : female or male dm : History of Diabetes (yes or no) sbp : Systolic Blood Pressure (mmHg) age : age of patient on admission The outcome variable is variable status. It is labelled as either dead or alive which is the outcome of each patient during hospitalization. 9.3 The logit and logistic models The simple binary logit and logistic models refer to a a model with only one covariate (also known as independent variable). For example, if the covariate is gcs (Glasgow Coma Scale), the simple logit model is written as: \\[\\hat{g}(x)= ln\\left[ \\frac{\\hat\\pi(x)}{1 - {\\hat\\pi(x)}} \\right]\\] where \\(\\hat{g}(x)\\) is the log odds for death for a given value of gcs. And the odds for death for a given value of GCS is written as \\(= \\hat\\beta_0 + \\hat\\beta_1(gcs)\\) And the simple logistic model is also written as: \\[\\hat{\\pi}(x) = \\frac{exp^{\\hat{\\beta}_{0} + \\hat{\\beta}_{1}{gcs}}}{1 + exp^{\\hat{\\beta}_{0} + \\hat{\\beta}_{1}{gcs}}}\\] The \\(\\pi(x) = E(Y|x)\\) represents the conditional mean of \\(Y\\) given \\(x\\) when the logistic distribution is used. This is also simply known as the predicted probability of death for given value of gcs. If we have decided (based on our clinical expertise and literature review) that a model that could explain death consists of gcs, stroke type, sex, dm, age and sbp, then the logit model can be expanded to: \\[\\hat{g}(x) = \\hat\\beta_0 + \\hat\\beta_1(gcs) + \\hat\\beta_2(stroke type) + \\hat\\beta_3(sex)+ \\hat\\beta_4(dm) + \\hat\\beta_5(sbp) + \\hat\\beta_6(age)\\] This is the odds for death given certain value of gcs, sbp and age and certain categories of stroke stype, sex and diabetes. While the probability of deaths is \\[\\hat{\\pi}(x) = \\frac{exp^{\\hat\\beta_0 + \\hat\\beta_1(gcs) + \\hat\\beta_2(stroke type) + \\hat\\beta_3(sex)+ \\hat\\beta_4(dm) + \\hat\\beta_5(sbp) + \\hat\\beta_6(age)})}{1 + exp^{\\hat\\beta_0 + \\hat\\beta_1(gcs) + \\hat\\beta_2(stroke type) + \\hat\\beta_3(sex)+ \\hat\\beta_4(dm) + \\hat\\beta_5(sbp) + \\hat\\beta_6(age)}}\\] In many datasets, some of the independent variables are discrete, nominal scale variables such as race, sex, treatment group, and so forth. And because of that it is inappropriate to include them in the model as if they were interval scale variables. Though in many software, they are represented by numbers, but these numbers are used merely identifiers. In this situation, we will use a method called design variables (or dummy variables). Suppose, for example, assuming that one of the independent variables is obesity type, which is now coded as Class 1, Class 2 and Class 3. In this case, there are 3 levels or categories, hence two design variables (\\(D - 1\\)) are necessary, lets say D1 and D2. One possible coding strategy is that when the patient is in Class 1 then the two design variables, for D1 and D2 would both be set equal to zero. In this example, Class 1 is the reference category. When the patient is in Class 2, then D1 is set as 1 and D2 as 0; when the patient is in Class 3, the we will set D1 as 0 and D2 and 1. All these coding assignment can be done automatically in the software. But to interpret, we must know which category is the reference. 9.4 Prepare environment for analysis 9.4.1 Creating a RStudio project Start a new analysis task by creating a new RStudio project. To do this, Go to File Click New Project Choose New Directory or Existing Directory. This directory points to the folder that usually contains the dataset to be analyzed. This is called as the working directory. Make sure there is a folder named as data in the folder. If there is not, create one. Make sure the dataset stroke.dta is inside the data folder in the working directory. 9.4.2 Loading libraries Next, we will load the necessary packages. We will use 5 packages the built in stat package - to run Generalized Linear Model. This is already loaded by default. haven - to read SPSS, STATA and SAS dataset tidyverse - to perform data transformation gtsummary - to provide nice results in a table broom - to tidy up the results LogisticDx - to do model assessment here - to ensure proper directory To load these packages, we will use the function library(): library(haven) library(tidyverse) ## -- Attaching packages --------------------------------------- tidyverse 1.3.1 -- ## v ggplot2 3.3.5 v purrr 0.3.4 ## v tibble 3.1.6 v dplyr 1.0.7 ## v tidyr 1.1.4 v stringr 1.4.0 ## v readr 2.1.1 v forcats 0.5.1 ## Warning: package &#39;tibble&#39; was built under R version 4.1.2 ## -- Conflicts ------------------------------------------ tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(gtsummary) ## #Uighur library(broom) library(LogisticDx) ## Warning: package &#39;LogisticDx&#39; was built under R version 4.1.2 library(here) ## here() starts at C:/Tengku/Sync_PC_Laptop/tulis-buku/multivariable-analysis/fork-profKIM/multivar_data_analysis 9.5 Read data WE will read data in the working directory into our R environment. Remember the dataset is in the STATA format. fatal &lt;- read_dta(here(&#39;data&#39;,&#39;stroke.dta&#39;)) Take a peek at data. Check variable names variable types glimpse(fatal) ## Rows: 226 ## Columns: 7 ## $ sex &lt;dbl+lbl&gt; 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 2, 1, ~ ## $ status &lt;dbl+lbl&gt; 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, ~ ## $ gcs &lt;dbl&gt; 13, 15, 15, 15, 15, 15, 13, 15, 15, 10, 15, 15, 15, 15, 15~ ## $ sbp &lt;dbl&gt; 143, 150, 152, 215, 162, 169, 178, 180, 186, 185, 122, 211~ ## $ dm &lt;dbl+lbl&gt; 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, ~ ## $ age &lt;dbl&gt; 50, 58, 64, 50, 65, 78, 66, 72, 61, 64, 63, 59, 64, 62, 40~ ## $ stroke_type &lt;dbl+lbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~ 9.6 Explore data Variables sex, status, dm and stroke type are labelled variable. This means eventhough they are coded as numbers but the numbers represent the groups or categories or levels of the variables. Basically, they are categorical variables. We will transform all of them to factor variables. We can quickly do this using the function across(). Below, we will transform all labelled variables to factor variables: fatal &lt;- fatal %&gt;% mutate(across(where(is.labelled), as_factor)) Now, we can look at the summary statistics fatal %&gt;% tbl_summary() %&gt;% as_hux_table() TABLE 9.1: Characteristic N = 226 sex male97 (43%) female129 (57%) alive or dead alive171 (76%) dead55 (24%) earliest Glasgow Coma Scale15.0 (10.0, 15.0) earliest systolic BP (mmHg)161 (143, 187) diabetes (yes or no)138 (61%) age in years61 (52, 69) Ischaemic Stroke or Haemorrhagic Ischaemic Stroke149 (66%) Haemorrhagic77 (34%) n (%); Median (IQR) or to get summary statistics for each status category: fatal %&gt;% tbl_summary(by = status) %&gt;% as_hux_table() TABLE 9.2: Characteristic alive, N = 171 dead, N = 55 sex male81 (47%)16 (29%) female90 (53%)39 (71%) earliest Glasgow Coma Scale15.0 (14.0, 15.0)8.0 (5.0, 11.0) earliest systolic BP (mmHg)160 (143, 186)162 (140, 199) diabetes (yes or no)100 (58%)38 (69%) age in years61 (53, 68)62 (50, 73) Ischaemic Stroke or Haemorrhagic Ischaemic Stroke132 (77%)17 (31%) Haemorrhagic39 (23%)38 (69%) n (%); Median (IQR) 9.7 Estimate the regression parameters We now can perform binary logistic regression to estimate the regression parameters \\(\\hat\\beta_s\\) or the log odds. Usually, we can do this in two steps: The simple binary logistic regression or the univariable logistic regression. In this analysis, there is only one independent variable or covariate in the model. This is also known as the crude or unadjusted analysis. The multiple binary logistic regression or the multivariable logistic regression. Here, we expand our model and include two or more independent variables (covariates). This is a adjusted model and we can obtain the estimate of a particular covariate independent of the other covariates in the model. 9.8 Simple binary logistic regression The simple binary logistic regression has a dependent variable and only one independent (covariate) variable. in our dataset, for example, we can have status as the dependent variable. gcs as the independent variable. The independent variable can be a numerical or a categorical variable. To estimate the log odds (the regression parameters, \\(\\beta\\)) for the covariate Glasgow Coma Scale (GCS), we can write the logit model as: \\[log\\frac{p(status = dead)}{1 - p(status = dead)} = \\hat\\beta_0 + \\hat\\beta_1(gcs)\\] In R, we use the glm() function to estimate the regression parameters and other parameters of interest. Lets run the model with gcs as the covariate and name the model as fatal_glm_1 fatal_glm_1 &lt;- glm(status ~ gcs, data = fatal, family = binomial(link = &#39;logit&#39;)) To get the summarized result of the model, we will use the summary() function: summary(fatal_glm_1) ## ## Call: ## glm(formula = status ~ gcs, family = binomial(link = &quot;logit&quot;), ## data = fatal) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1179 -0.3921 -0.3921 -0.3921 2.2820 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.29479 0.60432 5.452 4.98e-08 *** ## gcs -0.38811 0.05213 -7.446 9.64e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 250.83 on 225 degrees of freedom ## Residual deviance: 170.92 on 224 degrees of freedom ## AIC: 174.92 ## ## Number of Fisher Scoring iterations: 5 To get the model summary in a data frame format, so we can edit more easily, we can use the tidy() function from the broom package. The package also contains other functions to provide other parameters useful for us later. The function conf.int() will provide the confidence intervals (CI). The default is set at the \\(95%\\) level: tidy(fatal_glm_1, conf.int = TRUE) ## # A tibble: 2 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 3.29 0.604 5.45 4.98e- 8 2.17 4.55 ## 2 gcs -0.388 0.0521 -7.45 9.64e-14 -0.497 -0.292 The estimates here are the log odds for death for a given value of gcs. In this example, each unit increase in gcs, the crude or unadjusted log odds for death due to stroke change by a factor \\(-0.388\\) with \\(95%\\) CI ranges from \\(-0.497 and -0.292\\). Now, lets use another covariate, stroke_type. Stroke type has 2 levels or categories; Haemorrhagic Stroke (HS) and Ischaemic Stroke (IS). HS is known to cause higher risk for deaths in stroke. We will model stroke type (stroke_type), name the model as fatal_glm_2 and show the result using tidy() fatal_glm_2 &lt;- glm(status ~ stroke_type, data = fatal, family = binomial(link = &#39;logit&#39;)) tidy(fatal_glm_2, conf.int = TRUE) ## # A tibble: 2 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -2.05 0.258 -7.95 1.80e-15 -2.59 -1.57 ## 2 stroke_typeHaemorrha~ 2.02 0.344 5.88 4.05e- 9 1.36 2.72 It seems that patients with Haemorrhagic Stroke (HS) had higher log odds for death during admission - by a factor \\(2.02\\) - than patients with Ischaemic Stroke (IS). 9.9 Multiple binary logistic regression The is strong motivation to include other covariates in the model. This is because It is unlikely that only one variable (gcs or stroke type) that is related with stroke. For example, cardiovascular disease has many factors that affect the outcome. So, it makes more sense to consider adding other seemingly important independent variable in the model. by adding more covariates in the model, we can estimate the adjusted log odds. These are the log odds of a particular covariate independent of other covariates. we can add other covariate to adjust for the confounding effects interaction (the product of two covariates) can also be estimated To add or not to add variables is a big subject on its own. Usually it is governed by clinical experience, subject matter experts and some preliminary analysis. Lets expand our model and include gcs, stroke type, sex, dm, sbp and age in the model. We will name this model as fatal_mv. To run this model and get the estimates in R: fatal_mv1 &lt;- glm(status ~ gcs + stroke_type + sex + dm + sbp + age, data = fatal, family = binomial(link = &#39;logit&#39;)) summary(fatal_mv1) ## ## Call: ## glm(formula = status ~ gcs + stroke_type + sex + dm + sbp + age, ## family = binomial(link = &quot;logit&quot;), data = fatal) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.3715 -0.4687 -0.3280 -0.1921 2.5150 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.1588269 1.6174965 -0.098 0.92178 ## gcs -0.3284640 0.0557574 -5.891 3.84e-09 *** ## stroke_typeHaemorrhagic 1.2662764 0.4365882 2.900 0.00373 ** ## sexfemale 0.4302901 0.4362742 0.986 0.32399 ## dmyes 0.4736670 0.4362309 1.086 0.27756 ## sbp 0.0008612 0.0060619 0.142 0.88703 ## age 0.0242321 0.0154010 1.573 0.11562 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 250.83 on 225 degrees of freedom ## Residual deviance: 159.34 on 219 degrees of freedom ## AIC: 173.34 ## ## Number of Fisher Scoring iterations: 5 We could get a cleaner result in a data frame format (and you can edit in spreadsheet easily) by using tidy(): log_odds &lt;- tidy(fatal_mv1, conf.int = TRUE) log_odds ## # A tibble: 7 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -1.59e-1 1.62 -0.0982 9.22e-1 -3.38 3.01 ## 2 gcs -3.28e-1 0.0558 -5.89 3.84e-9 -0.444 -0.224 ## 3 stroke_typeHaemor~ 1.27e+0 0.437 2.90 3.73e-3 0.411 2.13 ## 4 sexfemale 4.30e-1 0.436 0.986 3.24e-1 -0.420 1.30 ## 5 dmyes 4.74e-1 0.436 1.09 2.78e-1 -0.368 1.35 ## 6 sbp 8.61e-4 0.00606 0.142 8.87e-1 -0.0110 0.0129 ## 7 age 2.42e-2 0.0154 1.57 1.16e-1 -0.00520 0.0555 We could see in the multivariable model, that with one unit increase in Glasgow Coma Scale (GCS), the log odds for death during hospitalization equals to \\(-0.328\\), adjusting for other covariates patients with HS has \\(1.266\\) times the log odds for death as compared to patients with IS, adjusting for other covariates. female patients have \\(0.430\\) times the log odds for death as compared to male patients, adjusting for other covariates patients with diabetes mellitus had \\(0.474\\) times the log odds for deaths as compared to patients with no diabetes mellitus With one mmHg increase in systolic blood pressure, the log odds for deaths change by a factor of \\(0.00086\\), when adjusting for other variables. with an increase in one year of age, the log odds for deaths change by a factor of \\(0.024\\), when adjusting for other variables. 9.10 Convert the log odds to odds ratio For lay person, it is difficult to interpret the log odds. It is easier to interpret using the odds ratio. To do this, we can use the argument exponentiate = TRUE in the tidy() function. However, we also know that the odds ratio can be easily calculate by \\(\\exp^{\\beta_i}\\) odds_ratio &lt;- tidy(fatal_mv1, exponentiate = TRUE, conf.int = TRUE) odds_ratio ## # A tibble: 7 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.853 1.62 -0.0982 9.22e-1 0.0341 20.3 ## 2 gcs 0.720 0.0558 -5.89 3.84e-9 0.641 0.799 ## 3 stroke_typeHaemor~ 3.55 0.437 2.90 3.73e-3 1.51 8.45 ## 4 sexfemale 1.54 0.436 0.986 3.24e-1 0.657 3.69 ## 5 dmyes 1.61 0.436 1.09 2.78e-1 0.692 3.87 ## 6 sbp 1.00 0.00606 0.142 8.87e-1 0.989 1.01 ## 7 age 1.02 0.0154 1.57 1.16e-1 0.995 1.06 9.11 Making inference Let us combine the results from the log odds and the odds ratio and rename the table properly. tab_logistic &lt;- bind_cols(log_odds, odds_ratio) ## New names: ## * term -&gt; term...1 ## * estimate -&gt; estimate...2 ## * std.error -&gt; std.error...3 ## * statistic -&gt; statistic...4 ## * p.value -&gt; p.value...5 ## * ... tab_logistic %&gt;% select(term...1, estimate...2, std.error...3, estimate...9, conf.low...13, conf.high...14 ,p.value...5) %&gt;% rename(covariate = term...1, log_odds = estimate...2, SE = std.error...3, odds_ratio = estimate...9, lower_OR = conf.low...13, upper_OR = conf.high...14, p.val = p.value...5) ## # A tibble: 7 x 7 ## covariate log_odds SE odds_ratio lower_OR upper_OR p.val ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -0.159 1.62 0.853 0.0341 20.3 9.22e-1 ## 2 gcs -0.328 0.0558 0.720 0.641 0.799 3.84e-9 ## 3 stroke_typeHaemorrhagic 1.27 0.437 3.55 1.51 8.45 3.73e-3 ## 4 sexfemale 0.430 0.436 1.54 0.657 3.69 3.24e-1 ## 5 dmyes 0.474 0.436 1.61 0.692 3.87 2.78e-1 ## 6 sbp 0.000861 0.00606 1.00 0.989 1.01 8.87e-1 ## 7 age 0.0242 0.0154 1.02 0.995 1.06 1.16e-1 In the model, it means that: if gcs increases by 1 unit (when stroke type is adjusted), the log odds for death changes by a factor \\(-0.32\\) or the odds for death changes by a factor \\(0.72\\) (odds for death reduces for \\(28\\%\\)). The the \\(95\\%CI\\) are between \\(21\\%,36\\%\\), adjusting for other covariates. patients with HS has \\(3.55\\%\\) times higher odds for stroke deaths - with \\(95\\%CI : 17\\%, 85\\%\\) - as compared to patients with HS, adjusting for other independent variables. female patients have \\(53\\%\\) higher odds for death as compared to female patients (\\(p = 0.154\\)), adjusting for other covariates patients with diabetes mellitus had \\(60.6\\%\\) higher odds for deaths compared to patients with no diabetes mellitus though the p value is above \\(5\\%\\) (\\(p = 0.642\\%\\)) With one mmHg increase in systolic blood pressure, the odds for death change by a factor \\(1.00086\\), when adjusting for other variables. The p value is also larger than \\(5\\%\\). with an increase in one year of age, the odds for deaths increase by a factor of \\(1.025\\), when adjusting for other variables. However, the p value is \\(0.115\\) 9.12 Model comparison It is not advisable to assess the important of variables based on their p-values or the Wald statistics. The better way is to use likelihood ratio to compare models and assess the importance of variables. For example, is there any statistical difference between model 1 (fatal_mv) and model 2 (fatal_glm_1) if we set the level of significance at \\(5\\%\\)? anova( fatal_glm_1, fatal_mv1, test = &#39;Chisq&#39;) ## Analysis of Deviance Table ## ## Model 1: status ~ gcs ## Model 2: status ~ gcs + stroke_type + sex + dm + sbp + age ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 224 170.92 ## 2 219 159.34 5 11.582 0.04098 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Both models are different statistically (at \\(5\\%\\) level). Hence, we prefer to keep model fatal_mv1. Now lets be economical, and just keep gcs, stroke type and age in the model. And lets name this model as fatal_mv2 fatal_mv2 &lt;- glm(status ~ gcs + stroke_type + age, data = fatal, family = binomial(link = &#39;logit&#39;)) And perform model comparison again anova( fatal_mv1, fatal_mv2, test = &#39;Chisq&#39;) ## Analysis of Deviance Table ## ## Model 1: status ~ gcs + stroke_type + sex + dm + sbp + age ## Model 2: status ~ gcs + stroke_type + age ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 219 159.34 ## 2 222 161.51 -3 -2.1743 0.537 The p-value is above the threshold of \\(5\\%\\), so we can not reject the null hypothesis that say both models are not statistically different. So by obeying the Occams razor principle, we will choose a simpler model that is model fatal_mv2 for further exploration. 9.13 Adding an interaction term Interaction effects occur when the effect of one variable depends on the value of another variable. Interaction effects are common in regression analysis, ANOVA, and designed experiments. Interaction involves two risk factors (and their effect on one disease outcome). If the effect of one risk factor is the same within strata defined by the other, then there is NO interaction. When the effect of one risk factor is different within strata defined by the other, then there is an interaction (biological) (Statistical) interaction can be measured based on the ways that risks are calculated (modeling). The presence of interaction based on measurements is called statistical interaction, and inherently it may not reflect the true biological interaction. Lets add an interaction between stroke type and diabetes: fatal_mv2_ia &lt;- glm(status ~ gcs + stroke_type + stroke_type:gcs + age, data = fatal, family = binomial(link = &#39;logit&#39;)) tidy(fatal_mv2_ia) ## # A tibble: 5 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.508 1.37 0.371 0.710 ## 2 gcs -0.320 0.0800 -4.01 0.0000619 ## 3 stroke_typeHaemorrhagic 1.61 1.30 1.24 0.217 ## 4 age 0.0236 0.0147 1.60 0.109 ## 5 gcs:stroke_typeHaemorrhagic -0.0347 0.111 -0.312 0.755 \\[\\hat{g}(x) = \\hat\\beta_0 + \\hat\\beta_1(gcs) + \\hat\\beta_2(stroke type) + \\hat\\beta_3(age)+ \\hat\\beta_4(gcs \\times stroke_type)\\] To decide if an interaction term should stay in the model, we suggest you to consider the biological and statistical significance. If you think the interaction justifies both reasons, then it is preferred you keep the interaction term in the model. For example, for our model: the coefficient for the interaction term for stroke type and gcs is not significant at %5%$ level. after getting further advice from the stroke experts, we believe that the effect of gcs on stroke death is not largely different between different stroke type And because of both reasons, we have decided not to keep the interaction for gcs and stroke type in the model. 9.14 Prediction from binary logistic regression We can use the broom::augment() function to calculate the log odds probability residuals hat values Cooks distance standardized residuals 9.14.1 Predict the log odds To obtain the .fitted column (representing the estimated log odds for death) for each patient, we can run: log_odds_mv2 &lt;- augment(fatal_mv2) log_odds_mv2 %&gt;% slice(1:10) ## # A tibble: 10 x 10 ## status gcs stroke_type age .fitted .resid .std.resid .hat .sigma ## &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 alive 13 Ischaemic Stroke 50 -2.49 -0.398 -0.400 0.00991 0.854 ## 2 alive 15 Ischaemic Stroke 58 -2.98 -0.314 -0.315 0.00584 0.855 ## 3 alive 15 Ischaemic Stroke 64 -2.84 -0.337 -0.338 0.00590 0.855 ## 4 alive 15 Ischaemic Stroke 50 -3.17 -0.287 -0.288 0.00657 0.855 ## 5 alive 15 Ischaemic Stroke 65 -2.82 -0.341 -0.342 0.00599 0.855 ## 6 alive 15 Ischaemic Stroke 78 -2.51 -0.395 -0.397 0.00980 0.854 ## 7 dead 13 Ischaemic Stroke 66 -2.12 2.11 2.12 0.00831 0.843 ## 8 alive 15 Ischaemic Stroke 72 -2.65 -0.369 -0.370 0.00731 0.855 ## 9 alive 15 Ischaemic Stroke 61 -2.91 -0.325 -0.326 0.00579 0.855 ## 10 dead 10 Ischaemic Stroke 64 -1.15 1.69 1.70 0.0173 0.847 ## # ... with 1 more variable: .cooksd &lt;dbl&gt; The slice() gives the snapshot of the data. In this case, we choose the first 10 patients. 9.14.2 Predict the probabilities To obtain the .fitted column (representing the estimated probabilities for death) for each patient, we can run: prob_mv2 &lt;- augment(fatal_mv2, type.predict = &quot;response&quot;) prob_mv2 %&gt;% slice(1:10) ## # A tibble: 10 x 10 ## status gcs stroke_type age .fitted .resid .std.resid .hat .sigma ## &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 alive 13 Ischaemic Stroke 50 0.0763 -0.398 -0.400 0.00991 0.854 ## 2 alive 15 Ischaemic Stroke 58 0.0482 -0.314 -0.315 0.00584 0.855 ## 3 alive 15 Ischaemic Stroke 64 0.0551 -0.337 -0.338 0.00590 0.855 ## 4 alive 15 Ischaemic Stroke 50 0.0403 -0.287 -0.288 0.00657 0.855 ## 5 alive 15 Ischaemic Stroke 65 0.0564 -0.341 -0.342 0.00599 0.855 ## 6 alive 15 Ischaemic Stroke 78 0.0750 -0.395 -0.397 0.00980 0.854 ## 7 dead 13 Ischaemic Stroke 66 0.107 2.11 2.12 0.00831 0.843 ## 8 alive 15 Ischaemic Stroke 72 0.0658 -0.369 -0.370 0.00731 0.855 ## 9 alive 15 Ischaemic Stroke 61 0.0516 -0.325 -0.326 0.00579 0.855 ## 10 dead 10 Ischaemic Stroke 64 0.241 1.69 1.70 0.0173 0.847 ## # ... with 1 more variable: .cooksd &lt;dbl&gt; 9.15 Model fitness We will assess the overall model fitness by checking the the area under the curve the Hosmer-Lemeshow test the modidied Hosmer-Lemeshow test the Oseo Rojek test The p-values of bigger than 0.05 indicates that there is no significant difference between the observed data and the predicted data (from our model). That supports the good fit of the model. fit_m &lt;- gof(fatal_mv2, g = 8) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases The area under the curve is \\(87.2\\%\\). The values of above 80 are considered to have good discriminating effect. fit_m$gof ## test stat val df pVal ## 1: HL chiSq 4.622183 6 0.5930997 ## 2: mHL F 1.071882 7 0.3844230 ## 3: OsRo Z -0.501724 NA 0.6158617 ## 4: SstPgeq0.5 Z 1.348843 NA 0.1773873 ## 5: SstPl0.5 Z 1.516578 NA 0.1293733 ## 6: SstBoth chiSq 4.119387 2 0.1274931 ## 7: SllPgeq0.5 chiSq 1.579811 1 0.2087879 ## 8: SllPl0.5 chiSq 2.311910 1 0.1283862 ## 9: SllBoth chiSq 2.341198 2 0.3101811 The Hosmer Lemeshow, modified Hosmer Lemeshow and Oseo Rojek are all above \\(5\\%\\) values which are supportive of good fit of the model. 9.16 Presentation The gtsummary package has a useful function tbld_regression() to produce a formatted table suitable for publication. This is the table for adjusted log odds ratio: tbl_regression(fatal_mv2) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #coxtqcdgqq .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #coxtqcdgqq .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #coxtqcdgqq .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #coxtqcdgqq .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; border-top-color: #FFFFFF; border-top-width: 0; } #coxtqcdgqq .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #coxtqcdgqq .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #coxtqcdgqq .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #coxtqcdgqq .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #coxtqcdgqq .gt_column_spanner_outer:first-child { padding-left: 0; } #coxtqcdgqq .gt_column_spanner_outer:last-child { padding-right: 0; } #coxtqcdgqq .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #coxtqcdgqq .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #coxtqcdgqq .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #coxtqcdgqq .gt_from_md > :first-child { margin-top: 0; } #coxtqcdgqq .gt_from_md > :last-child { margin-bottom: 0; } #coxtqcdgqq .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #coxtqcdgqq .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #coxtqcdgqq .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #coxtqcdgqq .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #coxtqcdgqq .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #coxtqcdgqq .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #coxtqcdgqq .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #coxtqcdgqq .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #coxtqcdgqq .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #coxtqcdgqq .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #coxtqcdgqq .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #coxtqcdgqq .gt_sourcenote { font-size: 90%; padding: 4px; } #coxtqcdgqq .gt_left { text-align: left; } #coxtqcdgqq .gt_center { text-align: center; } #coxtqcdgqq .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #coxtqcdgqq .gt_font_normal { font-weight: normal; } #coxtqcdgqq .gt_font_bold { font-weight: bold; } #coxtqcdgqq .gt_font_italic { font-style: italic; } #coxtqcdgqq .gt_super { font-size: 65%; } #coxtqcdgqq .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 65%; } Characteristic log(OR)1 95% CI1 p-value earliest Glasgow Coma Scale -0.34 -0.45, -0.24 Ischaemic Stroke or Haemorrhagic Ischaemic Stroke   Haemorrhagic 1.2 0.38, 2.1 0.004 age in years 0.02 0.00, 0.05 0.11 1 OR = Odds Ratio, CI = Confidence Interval And this is the table for adjusted odds ratio: tbl_regression(fatal_mv2, exponentiate = TRUE) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #lqqsiuikmm .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #lqqsiuikmm .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #lqqsiuikmm .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #lqqsiuikmm .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; border-top-color: #FFFFFF; border-top-width: 0; } #lqqsiuikmm .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #lqqsiuikmm .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #lqqsiuikmm .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #lqqsiuikmm .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #lqqsiuikmm .gt_column_spanner_outer:first-child { padding-left: 0; } #lqqsiuikmm .gt_column_spanner_outer:last-child { padding-right: 0; } #lqqsiuikmm .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #lqqsiuikmm .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #lqqsiuikmm .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #lqqsiuikmm .gt_from_md > :first-child { margin-top: 0; } #lqqsiuikmm .gt_from_md > :last-child { margin-bottom: 0; } #lqqsiuikmm .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #lqqsiuikmm .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #lqqsiuikmm .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #lqqsiuikmm .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #lqqsiuikmm .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #lqqsiuikmm .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #lqqsiuikmm .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #lqqsiuikmm .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #lqqsiuikmm .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #lqqsiuikmm .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #lqqsiuikmm .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #lqqsiuikmm .gt_sourcenote { font-size: 90%; padding: 4px; } #lqqsiuikmm .gt_left { text-align: left; } #lqqsiuikmm .gt_center { text-align: center; } #lqqsiuikmm .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #lqqsiuikmm .gt_font_normal { font-weight: normal; } #lqqsiuikmm .gt_font_bold { font-weight: bold; } #lqqsiuikmm .gt_font_italic { font-style: italic; } #lqqsiuikmm .gt_super { font-size: 65%; } #lqqsiuikmm .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 65%; } Characteristic OR1 95% CI1 p-value earliest Glasgow Coma Scale 0.71 0.64, 0.79 Ischaemic Stroke or Haemorrhagic Ischaemic Stroke   Haemorrhagic 3.40 1.46, 7.97 0.004 age in years 1.02 1.00, 1.05 0.11 1 OR = Odds Ratio, CI = Confidence Interval 9.17 References "],["poisson-regression.html", "Chapter 10 Poisson Regression 10.1 Introduction 10.2 Preliminaries 10.3 Simple Poisson regression models 10.4 Rate data 10.5 Multiple Poisson regression model 10.6 References", " Chapter 10 Poisson Regression 10.1 Introduction Multiple Poisson Regression for count is given as \\[ln\\,E(Y|\\mathbf{X})=ln\\,\\mu=\\beta_{0}+\\beta_{1}X_{1}+\\cdots+\\beta_{p-1}X_{p-1}=\\beta_{0}+\\sum\\beta_{p-1}X_{p-1}\\] where the X (in bold) denotes a collection of Xs. p is the number of estimated parameters. Multiple Poisson Regression for rate with offset2 is given as \\[ln\\,E(Y|\\mathbf{X})=ln\\,a(\\mathbf{X})+\\beta_{0}+\\sum\\beta_{p-1}X_{p-1}\\] The rate ratio, RR is \\[RR=e^{\\beta_{p-1}}\\] 10.2 Preliminaries 10.2.1 Load libraries library(epiDisplay) library(car) 10.3 Simple Poisson regression models 10.3.1 Count data 10.3.1.1 X categorical # - UKaccident.csv is modified from builtin data Seatbelts acc = read.csv(here::here(&quot;data&quot;, &quot;UKaccident.csv&quot;)) #- driverskilled: number of death #- law: before seatbelt law = 0, after law = 1 str(acc) ## &#39;data.frame&#39;: 122 obs. of 2 variables: ## $ driverskilled: int 107 97 102 87 119 106 110 106 107 125 ... ## $ law : int 0 0 0 0 0 0 0 0 0 0 ... head(acc); tail(acc) ## driverskilled law ## 1 107 0 ## 2 97 0 ## 3 102 0 ## 4 87 0 ## 5 119 0 ## 6 106 0 ## driverskilled law ## 117 81 1 ## 118 84 1 ## 119 87 1 ## 120 90 1 ## 121 79 1 ## 122 96 1 # - some descriptives tapply(acc$driverskilled, acc$law, sum) # total death before vs after ## 0 1 ## 11826 1294 table(acc$law) # num of observations before vs after ## ## 0 1 ## 107 15 # - mean count, manually 11826/107 # 110.5234, count before law ## [1] 110.5234 1294/15 # 86.26667, count after law ## [1] 86.26667 model.acc = glm(driverskilled ~ law, data = acc, family = poisson) summary(model.acc) # significant p based on Wald test ## ## Call: ## glm(formula = driverskilled ~ law, family = poisson, data = acc) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.16127 -0.72398 0.04531 0.77308 1.89182 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.705227 0.009196 511.681 &lt;2e-16 *** ## law -0.247784 0.029281 -8.462 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 219.17 on 121 degrees of freedom ## Residual deviance: 142.64 on 120 degrees of freedom ## AIC: 940.7 ## ## Number of Fisher Scoring iterations: 4 # - to get CI cbind(coef(model.acc), confint(model.acc)) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 4.7052269 4.6871495 4.7231960 ## law -0.2477837 -0.3056189 -0.1908312 # - ln(count) = 4.71 - 0.25*LAW 4.71 - 0.25 # = 4.46 ## [1] 4.46 exp(4.71) # 111.0522, count before law ## [1] 111.0522 exp(4.46) # 86.48751, count after law ## [1] 86.48751 # - Model fit poisgof(model.acc) # fit well, based on chi-square test on the residual deviance ## $results ## [1] &quot;Goodness-of-fit test for Poisson assumption&quot; ## ## $chisq ## [1] 142.6436 ## ## $df ## [1] 120 ## ## $p.value ## [1] 0.07764771 # - Diagnostics # - standardized residuals sr = rstandard(model.acc) sr[abs(sr) &gt; 1.96] ## 4 54 55 91 113 ## -2.335861 -3.176147 -2.857937 -2.647896 -3.098644 # - predicted count vs fitted values fitted.acc = model.acc$fitted data.frame(acc, fitted.acc)[names(sr[abs(sr) &gt; 1.96]),] # look at the discrepancies ## driverskilled law fitted.acc ## 4 87 0 110.52336 ## 54 79 0 110.52336 ## 55 82 0 110.52336 ## 91 84 0 110.52336 ## 113 60 1 86.26667 # Summary with RR idr.display(model.acc) # easier, also view LR test ## ## Poisson regression predicting driverskilled ## ## IDR(95%CI) P(Wald&#39;s test) P(LR-test) ## law: 1 vs 0 0.78 (0.74,0.83) &lt; 0.001 &lt; 0.001 ## ## Log-likelihood = -468.3481 ## No. of observations = 122 ## AIC value = 940.6963 10.3.1.2 X numerical # - Data from https://stats.idre.ucla.edu/stat/data/poisson_sim.csv aw = read.csv(here::here(&quot;data&quot;, &quot;poisson_sim.csv&quot;)) head(aw); tail(aw) ## id num_awards prog math ## 1 45 0 3 41 ## 2 108 0 1 41 ## 3 15 0 3 44 ## 4 67 0 3 42 ## 5 153 0 3 40 ## 6 51 0 1 42 ## id num_awards prog math ## 195 61 1 2 60 ## 196 100 2 2 71 ## 197 143 2 3 75 ## 198 68 1 2 71 ## 199 57 0 2 72 ## 200 132 3 2 73 str(aw) ## &#39;data.frame&#39;: 200 obs. of 4 variables: ## $ id : int 45 108 15 67 153 51 164 133 2 53 ... ## $ num_awards: int 0 0 0 0 0 0 0 0 0 0 ... ## $ prog : int 3 1 3 3 3 1 3 3 3 3 ... ## $ math : int 41 41 44 42 40 42 46 40 33 46 ... #- num_awards: The number of awards earned by students at one high school. #- math: the score on their final exam in math. model.aw = glm(num_awards ~ math, data = aw, family = poisson) summary(model.aw) # math sig. ## ## Call: ## glm(formula = num_awards ~ math, family = poisson, data = aw) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1853 -0.9070 -0.6001 0.3246 2.9529 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.333532 0.591261 -9.021 &lt;2e-16 *** ## math 0.086166 0.009679 8.902 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 287.67 on 199 degrees of freedom ## Residual deviance: 204.02 on 198 degrees of freedom ## AIC: 384.08 ## ## Number of Fisher Scoring iterations: 6 cbind(coef(model.aw), confint(model.aw)) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) -5.3335321 -6.52038334 -4.200322 ## math 0.0861656 0.06737466 0.105356 poisgof(model.aw) # fit well ## $results ## [1] &quot;Goodness-of-fit test for Poisson assumption&quot; ## ## $chisq ## [1] 204.0213 ## ## $df ## [1] 198 ## ## $p.value ## [1] 0.3695697 sr = rstandard(model.aw) sr[abs(sr) &gt; 1.96] ## 54 120 122 150 157 164 172 181 ## 2.740294 1.975409 2.015236 2.112331 2.963862 2.253872 2.112331 2.451774 ## 199 ## -2.241058 aw_ = data.frame(aw[c(4,2)], predicted = model.aw$fitted); head(aw_); tail(aw_) ## math num_awards predicted ## 1 41 0 0.1651762 ## 2 41 0 0.1651762 ## 3 44 0 0.2139002 ## 4 42 0 0.1800399 ## 5 40 0 0.1515396 ## 6 42 0 0.1800399 ## math num_awards predicted ## 195 60 1 0.8490848 ## 196 71 2 2.1907094 ## 197 75 2 3.0922155 ## 198 71 1 2.1907094 ## 199 72 0 2.3878444 ## 200 73 3 2.6027189 aw_[names(sr[abs(sr) &gt; 1.96]),] # look at the discrepancies ## math num_awards predicted ## 54 50 3 0.3587060 ## 120 49 2 0.3290921 ## 122 58 3 0.7146750 ## 150 57 3 0.6556731 ## 157 61 5 0.9254913 ## 164 62 4 1.0087733 ## 172 57 3 0.6556731 ## 181 69 6 1.8439209 ## 199 72 0 2.3878444 # 1 unit increase in math score idr.display(model.aw) ## ## Poisson regression predicting num_awards ## ## IDR(95%CI) P(Wald&#39;s test) P(LR-test) ## math (cont. var.) 1.09 (1.07,1.11) &lt; 0.001 &lt; 0.001 ## ## Log-likelihood = -190.0381 ## No. of observations = 200 ## AIC value = 384.0762 # 10 unit increase in math score? Manually... b1 = coef(model.aw)[[2]]*10 b1.ll = confint(model.aw)[[2]]*10 ## Waiting for profiling to be done... b1.ul = confint(model.aw)[[4]]*10 ## Waiting for profiling to be done... exp(cbind(&quot;Math RR&quot; = b1, &quot;95% LL&quot; = b1.ll, &quot;95% UL&quot; = b1.ul)) ## Math RR 95% LL 95% UL ## [1,] 2.367077 1.961573 2.867842 10.4 Rate data # - data in Fleiss et al 2003 &quot; Table 12.1 cigar.day person.yrs cases rate pred 1 0.0 1421 0 0.000000000 0.000793326 2 5.2 927 0 0.000000000 0.001170787 3 11.2 988 2 0.002024291 0.001834458 4 15.9 849 2 0.002355713 0.002607843 5 20.4 1567 9 0.005743459 0.003652195 6 27.4 1409 10 0.007097232 0.006167215 7 40.8 556 7 0.012589928 0.016813428 &quot; ## [1] &quot; Table 12.1\\n cigar.day person.yrs cases rate pred\\n1 0.0 1421 0 0.000000000 0.000793326\\n2 5.2 927 0 0.000000000 0.001170787\\n3 11.2 988 2 0.002024291 0.001834458\\n4 15.9 849 2 0.002355713 0.002607843\\n5 20.4 1567 9 0.005743459 0.003652195\\n6 27.4 1409 10 0.007097232 0.006167215\\n7 40.8 556 7 0.012589928 0.016813428\\n&quot; cigar.day = c(0, 5.2, 11.2, 15.9, 20.4, 27.4, 40.8) person.yrs = c(1421, 927, 988, 849, 1567, 1409, 556) cases = c(0, 0, 2, 2, 9, 10, 7) cig = data.frame(cigar.day, person.yrs, cases); cig ## cigar.day person.yrs cases ## 1 0.0 1421 0 ## 2 5.2 927 0 ## 3 11.2 988 2 ## 4 15.9 849 2 ## 5 20.4 1567 9 ## 6 27.4 1409 10 ## 7 40.8 556 7 cig$rate = cig$cases/cig$person.yrs; cig ## cigar.day person.yrs cases rate ## 1 0.0 1421 0 0.000000000 ## 2 5.2 927 0 0.000000000 ## 3 11.2 988 2 0.002024291 ## 4 15.9 849 2 0.002355713 ## 5 20.4 1567 9 0.005743459 ## 6 27.4 1409 10 0.007097232 ## 7 40.8 556 7 0.012589928 model.cig = glm(cases ~ cigar.day, offset = log(person.yrs), data = cig, family = &quot;poisson&quot;) # - it includes offset variable summary(model.cig) ## ## Call: ## glm(formula = cases ~ cigar.day, family = &quot;poisson&quot;, data = cig, ## offset = log(person.yrs)) ## ## Deviance Residuals: ## 1 2 3 4 5 6 7 ## -1.5015 -1.4733 0.1370 -0.1463 1.2630 0.4340 -0.8041 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -7.13928 0.45402 -15.725 &lt; 2e-16 *** ## cigar.day 0.07485 0.01564 4.786 1.7e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 30.9017 on 6 degrees of freedom ## Residual deviance: 6.8956 on 5 degrees of freedom ## AIC: 28.141 ## ## Number of Fisher Scoring iterations: 5 poisgof(model.cig) ## $results ## [1] &quot;Goodness-of-fit test for Poisson assumption&quot; ## ## $chisq ## [1] 6.895581 ## ## $df ## [1] 5 ## ## $p.value ## [1] 0.2285227 cig$pred = model.cig$fitted/cig$person.yrs; cig ## cigar.day person.yrs cases rate pred ## 1 0.0 1421 0 0.000000000 0.000793326 ## 2 5.2 927 0 0.000000000 0.001170787 ## 3 11.2 988 2 0.002024291 0.001834458 ## 4 15.9 849 2 0.002355713 0.002607843 ## 5 20.4 1567 9 0.005743459 0.003652195 ## 6 27.4 1409 10 0.007097232 0.006167215 ## 7 40.8 556 7 0.012589928 0.016813428 idr.display(model.cig) # interpret? ## ## Poisson regression predicting cases with offset = log(person.yrs) ## ## IDR(95%CI) P(Wald&#39;s test) P(LR-test) ## cigar.day (cont. var.) 1.08 (1.05,1.11) &lt; 0.001 &lt; 0.001 ## ## Log-likelihood = -12.0707 ## No. of observations = 7 ## AIC value = 28.1413 # - 5 cigar/day exp(coef(model.cig)[[2]]*5) # interpret? ## [1] 1.453868 # - 10 cigar/day exp(coef(model.cig)[[2]]*10) # interpret? ## [1] 2.113733 10.5 Multiple Poisson regression model # - Again, data from https://stats.idre.ucla.edu/stat/data/poisson_sim.csv aw = read.csv(here::here(&quot;data&quot;, &quot;poisson_sim.csv&quot;)) str(aw) ## &#39;data.frame&#39;: 200 obs. of 4 variables: ## $ id : int 45 108 15 67 153 51 164 133 2 53 ... ## $ num_awards: int 0 0 0 0 0 0 0 0 0 0 ... ## $ prog : int 3 1 3 3 3 1 3 3 3 3 ... ## $ math : int 41 41 44 42 40 42 46 40 33 46 ... head(aw); tail(aw) ## id num_awards prog math ## 1 45 0 3 41 ## 2 108 0 1 41 ## 3 15 0 3 44 ## 4 67 0 3 42 ## 5 153 0 3 40 ## 6 51 0 1 42 ## id num_awards prog math ## 195 61 1 2 60 ## 196 100 2 2 71 ## 197 143 2 3 75 ## 198 68 1 2 71 ## 199 57 0 2 72 ## 200 132 3 2 73 #- num_awards: The number of awards earned by students at one high school. #- prog: 1 = General, 2 = Academic, 3 = Vocational #- math: the score on their final exam in math. #- factor prog &amp; save as a new variable prog1 aw$prog1 = factor(aw$prog, levels = 1:3, labels = c(&quot;General&quot;, &quot;Academic&quot;, &quot;Vocational&quot;)) str(aw) ## &#39;data.frame&#39;: 200 obs. of 5 variables: ## $ id : int 45 108 15 67 153 51 164 133 2 53 ... ## $ num_awards: int 0 0 0 0 0 0 0 0 0 0 ... ## $ prog : int 3 1 3 3 3 1 3 3 3 3 ... ## $ math : int 41 41 44 42 40 42 46 40 33 46 ... ## $ prog1 : Factor w/ 3 levels &quot;General&quot;,&quot;Academic&quot;,..: 3 1 3 3 3 1 3 3 3 3 ... head(aw); tail(aw) ## id num_awards prog math prog1 ## 1 45 0 3 41 Vocational ## 2 108 0 1 41 General ## 3 15 0 3 44 Vocational ## 4 67 0 3 42 Vocational ## 5 153 0 3 40 Vocational ## 6 51 0 1 42 General ## id num_awards prog math prog1 ## 195 61 1 2 60 Academic ## 196 100 2 2 71 Academic ## 197 143 2 3 75 Vocational ## 198 68 1 2 71 Academic ## 199 57 0 2 72 Academic ## 200 132 3 2 73 Academic 10.5.1 Univariable # - Math model.aw.u1 = glm(num_awards ~ math, data = aw, family = poisson) summary(model.aw.u1) # Math sig. ## ## Call: ## glm(formula = num_awards ~ math, family = poisson, data = aw) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1853 -0.9070 -0.6001 0.3246 2.9529 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.333532 0.591261 -9.021 &lt;2e-16 *** ## math 0.086166 0.009679 8.902 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 287.67 on 199 degrees of freedom ## Residual deviance: 204.02 on 198 degrees of freedom ## AIC: 384.08 ## ## Number of Fisher Scoring iterations: 6 # - Prog model.aw.u2 = glm(num_awards ~ prog1, data = aw, family = poisson) summary(model.aw.u2) # Vocational vs General not sig. -&gt; Combine ## ## Call: ## glm(formula = num_awards ~ prog1, family = poisson, data = aw) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.4142 -0.6928 -0.6325 0.0000 3.3913 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.6094 0.3333 -4.828 1.38e-06 *** ## prog1Academic 1.6094 0.3473 4.634 3.59e-06 *** ## prog1Vocational 0.1823 0.4410 0.413 0.679 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 287.67 on 199 degrees of freedom ## Residual deviance: 234.46 on 197 degrees of freedom ## AIC: 416.51 ## ## Number of Fisher Scoring iterations: 6 aw$prog2 = recode(aw$prog1, &quot;c(&#39;General&#39;, &#39;Vocational&#39;) = &#39;General &amp; Vocational&#39;&quot;) levels(aw$prog2) ## [1] &quot;Academic&quot; &quot;General &amp; Vocational&quot; # - Prog2: General &amp; Vocational vs Academic model.aw.u2a = glm(num_awards ~ prog2, data = aw, family = poisson) summary(model.aw.u2a) ## ## Call: ## glm(formula = num_awards ~ prog2, family = poisson, data = aw) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.4142 -0.6649 -0.6649 0.0000 3.3913 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.352e-16 9.759e-02 0.000 1 ## prog2General &amp; Vocational -1.509e+00 2.390e-01 -6.314 2.72e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 287.67 on 199 degrees of freedom ## Residual deviance: 234.63 on 198 degrees of freedom ## AIC: 414.69 ## ## Number of Fisher Scoring iterations: 6 table(No_Award = aw$num_awards, aw$prog2) ## ## No_Award Academic General &amp; Vocational ## 0 48 76 ## 1 32 17 ## 2 11 2 ## 3 9 0 ## 4 2 0 ## 5 2 0 ## 6 1 0 tapply(aw$num_awards, aw$prog2, sum) ## Academic General &amp; Vocational ## 105 21 10.5.2 Multivariable model.aw.m1 = glm(num_awards ~ math + prog2, data = aw, family = poisson) summary(model.aw.m1) # both vars sig. ## ## Call: ## glm(formula = num_awards ~ math + prog2, family = poisson, data = aw) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2020 -0.8346 -0.5115 0.2589 2.6793 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.15050 0.66781 -6.215 5.13e-10 *** ## math 0.06995 0.01068 6.548 5.83e-11 *** ## prog2General &amp; Vocational -0.89129 0.25662 -3.473 0.000514 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 287.67 on 199 degrees of freedom ## Residual deviance: 190.16 on 197 degrees of freedom ## AIC: 372.22 ## ## Number of Fisher Scoring iterations: 6 poisgof(model.aw.m1) # good fit ## $results ## [1] &quot;Goodness-of-fit test for Poisson assumption&quot; ## ## $chisq ## [1] 190.1611 ## ## $df ## [1] 197 ## ## $p.value ## [1] 0.6235879 idr.display(model.aw.m1) ## ## Poisson regression predicting num_awards ## ## crude IDR(95%CI) adj. IDR(95%CI) ## math (cont. var.) 1.09 (1.07,1.11) 1.07 (1.05,1.1) ## ## prog2: General &amp; Vocational vs Academic 0.22 (0.14,0.35) 0.41 (0.25,0.68) ## ## P(Wald&#39;s test) P(LR-test) ## math (cont. var.) &lt; 0.001 &lt; 0.001 ## ## prog2: General &amp; Vocational vs Academic &lt; 0.001 &lt; 0.001 ## ## Log-likelihood = -183.108 ## No. of observations = 200 ## AIC value = 372.216 AIC(model.aw.u1, model.aw.u2a, model.aw.m1) ## df AIC ## model.aw.u1 2 384.0762 ## model.aw.u2a 2 414.6871 ## model.aw.m1 3 372.2160 # - diagnostics sr = rstandard(model.aw.m1) sr[abs(sr) &gt; 1.96] ## 54 154 157 164 181 191 199 ## 2.372000 1.996023 2.693894 2.014175 2.342797 -2.013339 -2.261164 aw$pred = model.aw.m1$fitted aw_diag = data.frame(num_of_awards = aw$num_awards, pred_awards = round(aw$pred, 1)) aw_diag[names(sr[abs(sr) &gt; 1.96]), ] # look at the discrepancies ## num_of_awards pred_awards ## 54 3 0.5 ## 154 2 0.3 ## 157 5 1.1 ## 164 4 1.2 ## 181 6 2.0 ## 191 0 2.0 ## 199 0 2.4 # - model fit: scaled Pearson chi-square statistic quasi = summary(glm(num_awards ~ math + prog2, data = aw, family = quasipoisson)) quasi$dispersion # dispersion parameter = scaled Pearson chi-square statistic ## [1] 1.08969 # - closer to 1, better. 10.5.3 Interaction model.aw.i1 = glm(num_awards ~ math + prog2 + math*prog2, data = aw, family = poisson) summary(model.aw.i1) # interaction term not sig. ## ## Call: ## glm(formula = num_awards ~ math + prog2 + math * prog2, family = poisson, ## data = aw) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2295 -0.8162 -0.5377 0.2528 2.6826 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.30286 0.74810 -5.752 8.83e-09 *** ## math 0.07241 0.01196 6.053 1.42e-09 *** ## prog2General &amp; Vocational -0.19552 1.50706 -0.130 0.897 ## math:prog2General &amp; Vocational -0.01277 0.02742 -0.466 0.641 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 287.67 on 199 degrees of freedom ## Residual deviance: 189.94 on 196 degrees of freedom ## AIC: 374 ## ## Number of Fisher Scoring iterations: 6 AIC(model.aw.m1, model.aw.i1) # increase in AIC, M1 is better ## df AIC ## model.aw.m1 3 372.2160 ## model.aw.i1 4 373.9965 10.5.4 Final model # - Accept model.aw.m1 idr.display(model.aw.m1) ## ## Poisson regression predicting num_awards ## ## crude IDR(95%CI) adj. IDR(95%CI) ## math (cont. var.) 1.09 (1.07,1.11) 1.07 (1.05,1.1) ## ## prog2: General &amp; Vocational vs Academic 0.22 (0.14,0.35) 0.41 (0.25,0.68) ## ## P(Wald&#39;s test) P(LR-test) ## math (cont. var.) &lt; 0.001 &lt; 0.001 ## ## prog2: General &amp; Vocational vs Academic &lt; 0.001 &lt; 0.001 ## ## Log-likelihood = -183.108 ## No. of observations = 200 ## AIC value = 372.216 b1 = coef(model.aw.m1)[[2]]*10 b1.ll = confint(model.aw.m1)[[2]]*10 ## Waiting for profiling to be done... b1.ul = confint(model.aw.m1)[[5]]*10 ## Waiting for profiling to be done... exp(cbind(&quot;Math RR&quot; = b1, &quot;95% LL&quot; = b1.ll, &quot;95% UL&quot; = b1.ul)) ## Math RR 95% LL 95% UL ## [1,] 2.012665 1.63494 2.485884 10.6 References the ln of the denominator/person-years, a(X) "],["survival-analysis.html", "Chapter 11 Survival Analysis 11.1 Introduction 11.2 Prepare environment for analysis 11.3 Data 11.4 Exploratory data 11.5 Kaplan-Meier survival estimates 11.6 Plot the survival probability 11.7 Comparing Kaplan-Meier estimates across groups 11.8 Semi-parametric models in survival analysis 11.9 Estimation from Cox proportional hazards regression 11.10 Inference 11.11 Adding interaction in the model 11.12 The proportional hazard assumption 11.13 Model checking 11.14 Plot the adjusted survival 11.15 Presentation and interpretation 11.16 References", " Chapter 11 Survival Analysis 11.1 Introduction 11.1.1 Background Regression concerns the relationship between a response variable and one or more exploratory variables. The independent variables are also called covariates. Some common regression analysis in health and medicine is linear regression, logistic regression, Poisson regression and Cox proportional hazard regression. Linear regression falls under the general linear model while the other three regression models fall under the generalized linear regression. Survival analysis is also known as duration analysis or time-to-event analysis. This analysis is useful in follow up studies where participants or patients are followed-up until they develop the event of interest. Examples of such studies include the retrospective cohort or prospective cohort studies. Sometime people confuse and wrongly think that the survival analysis is only applicable to the study of survival (where the event of interest is either alive or death condition). It is not. Any study that involves both the duration of follow up (in days or weeks or months) and the event of interest can be considered as suitable for survival analysis. So, survival or duration analysis is performed on data that has duration until an event occurs. In health and medicine studies, this event can be of interest, for example: relapse after treatment death after diagnosis recurrence of disease treatment success And for variable time, it can be in any time form such as days, weeks, years or event minutes. If we combine them together (with the event of interest) then it can become: months to relapse years from diagnosis to death weeks before recurrence days taken from the start of treatment to treatment success 11.1.2 Types of survival analysis There are three main types of survival analysis: the non-parametric survival analysis for example Kaplan-Meier estimates. the semi-parametric survival analysis for example Cox proportional hazard regression. the parametric survival analysis for example Weibull parametric survival analysis. In medical literature, we usually perform the non-parametric analysis for example the Kaplan-Meier estimates at the beginning of the analysis to understand the trend of survival probabilities. Following that, we perform simple (univariable) or multiple (multivariable) Cox proportional hazard (PH) regression model. If we are sure with the distribution of the risk and aim to estimate the survival time, then we can perform parametric survival analysis. 11.1.3 Objectives At the end of the chapter, the readers will be to perform Kaplan-Meier survival analysis to perform the simple (univariable) Cox PH regression analysis to perform the multiple (multivariable) Cox PH regression analysis 11.2 Prepare environment for analysis 11.2.1 RStudio project We recommend your start a new analysis project by creating a new RStudio project. To do this: Go to File Click New Project Choose New Directory or Existing Directory. This directory actually a folder that should contain the R codes, the dataset and the outputs from the analysis. To organize further, we would create a folder and name it as data in the directory. And we will copy the dataset stroke_data.csv in the data folder. 11.2.2 Packages Next, we will load the necessary packages. We will use these packages: gtsummary: a package that give us a nice formatted tables of statistics tidyverse: a package for data wrangling and making plots lubridate : a package to manipulate dates survival: a package to run survival analysis. survminer: a package to plot survival objects broom: a package to make prettier outputs here : a package to manage location fo files In addition to survival package, there are other packages that can perform survival analysis. The details of the package is available here https://cran.r-project.org/web/views/Survival.html. Now, we will load the packages: library(here) ## here() starts at C:/Tengku/Sync_PC_Laptop/tulis-buku/multivariable-analysis/fork-profKIM/multivar_data_analysis library(tidyverse) ## -- Attaching packages --------------------------------------- tidyverse 1.3.1 -- ## v ggplot2 3.3.5 v purrr 0.3.4 ## v tibble 3.1.6 v dplyr 1.0.7 ## v tidyr 1.1.4 v stringr 1.4.0 ## v readr 2.1.1 v forcats 0.5.1 ## Warning: package &#39;tibble&#39; was built under R version 4.1.2 ## -- Conflicts ------------------------------------------ tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union library(survival) library(survminer) ## Warning: package &#39;survminer&#39; was built under R version 4.1.2 ## Loading required package: ggpubr library(broom) library(gtsummary) Remember, check if all packages are available in your R library. If not, you will receive an error message stating that the package is not available. To install the package, use the install.packages() function. It will download and install the packages to your R libraries. Once the packages are installed, load the packages again using the library() function. 11.3 Data The tutorial uses a dataset named stroke_data.csv. This is comma-separated value dataset. Now let us load the data stroke &lt;- read_csv(here(&#39;data&#39;, &#39;stroke_data.csv&#39;)) ## Rows: 213 Columns: 12 ## -- Column specification -------------------------------------------------------- ## Delimiter: &quot;,&quot; ## chr (7): doa, dod, status, sex, dm, stroke_type, referral_from ## dbl (5): gcs, sbp, dbp, wbc, time2 ## ## i Use `spec()` to retrieve the full column specification for this data. ## i Specify the column types or set `show_col_types = FALSE` to quiet this message. And take a peek at our data to check number of observations (n=213) name of variables (12 variables) type of variables (character and double) glimpse(stroke) ## Rows: 213 ## Columns: 12 ## $ doa &lt;chr&gt; &quot;17/2/2011&quot;, &quot;20/3/2011&quot;, &quot;9/4/2011&quot;, &quot;12/4/2011&quot;, &quot;12/4~ ## $ dod &lt;chr&gt; &quot;18/2/2011&quot;, &quot;21/3/2011&quot;, &quot;10/4/2011&quot;, &quot;13/4/2011&quot;, &quot;13/~ ## $ status &lt;chr&gt; &quot;alive&quot;, &quot;alive&quot;, &quot;dead&quot;, &quot;dead&quot;, &quot;alive&quot;, &quot;dead&quot;, &quot;aliv~ ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;m~ ## $ dm &lt;chr&gt; &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;yes&quot;,~ ## $ gcs &lt;dbl&gt; 15, 15, 11, 3, 15, 3, 11, 15, 6, 15, 15, 4, 4, 10, 12, 1~ ## $ sbp &lt;dbl&gt; 151, 196, 126, 170, 103, 91, 171, 106, 170, 123, 144, 23~ ## $ dbp &lt;dbl&gt; 73, 123, 78, 103, 62, 55, 80, 67, 90, 83, 89, 120, 120, ~ ## $ wbc &lt;dbl&gt; 12.5, 8.1, 15.3, 13.9, 14.7, 14.2, 8.7, 5.5, 10.5, 7.2, ~ ## $ time2 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~ ## $ stroke_type &lt;chr&gt; &quot;IS&quot;, &quot;IS&quot;, &quot;HS&quot;, &quot;IS&quot;, &quot;IS&quot;, &quot;HS&quot;, &quot;IS&quot;, &quot;IS&quot;, &quot;HS&quot;, &quot;I~ ## $ referral_from &lt;chr&gt; &quot;non-hospital&quot;, &quot;non-hospital&quot;, &quot;hospital&quot;, &quot;hospital&quot;, ~ These data come from patients who were admitted at a tertiary hospital due to acute stroke. They were treated in the ward and the status (dead or alive) were recorded. The variables are: doa : date of admission dod : date of discharge status : event at discharge (alive or dead) sex : male or female dm : diabetes (yes or no) gcs : Glasgow Coma Scale (value from 3 to 15) sbp : Systolic blood pressure (mmHg) dbp : Diastolic blood pressure (mmHg) wbc : Total white cell count time2 : days in ward stroke_type : stroke type (Ischaemic stroke or Haemorrhagic stroke) referral_from : patient was referred from a hospital or not from a hospital The outcome of interest is time from admission to death. The time variable is time2 (in days) and the event variable is status. The event of interest is dead. We also note that variables dates and other categorical variables are in character format. The rest are in numerical format. 11.4 Exploratory data We will convert the variables doa and doa to a more valid format that is date: stroke &lt;- stroke %&gt;% mutate(doa = dmy(doa), dod = dmy(dod)) We will perform a quick preliminary analysis. First, by looking at the summary statistics: summary(stroke) ## doa dod status ## Min. :2011-01-01 Min. :2011-01-05 Length:213 ## 1st Qu.:2011-06-06 1st Qu.:2011-06-09 Class :character ## Median :2011-10-31 Median :2011-11-02 Mode :character ## Mean :2011-10-16 Mean :2011-10-23 ## 3rd Qu.:2012-03-12 3rd Qu.:2012-03-18 ## Max. :2012-06-23 Max. :2012-06-27 ## sex dm gcs sbp ## Length:213 Length:213 Min. : 3.00 Min. : 75.0 ## Class :character Class :character 1st Qu.:10.00 1st Qu.:142.0 ## Mode :character Mode :character Median :15.00 Median :160.0 ## Mean :12.52 Mean :162.8 ## 3rd Qu.:15.00 3rd Qu.:186.0 ## Max. :15.00 Max. :290.0 ## dbp wbc time2 stroke_type ## Min. : 42.00 Min. : 4.2 Min. : 1.000 Length:213 ## 1st Qu.: 79.00 1st Qu.: 7.5 1st Qu.: 3.000 Class :character ## Median : 90.00 Median : 9.9 Median : 4.000 Mode :character ## Mean : 91.53 Mean :10.4 Mean : 6.474 ## 3rd Qu.:103.00 3rd Qu.:12.5 3rd Qu.: 7.000 ## Max. :160.00 Max. :27.0 Max. :43.000 ## referral_from ## Length:213 ## Class :character ## Mode :character ## ## ## The tbl_summary() function from gtsummary package can produce nice tables. For example, the overall characteristics of patients can be obtained by: stroke %&gt;% tbl_summary() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #tluqhdhyhj .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #tluqhdhyhj .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #tluqhdhyhj .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #tluqhdhyhj .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; border-top-color: #FFFFFF; border-top-width: 0; } #tluqhdhyhj .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #tluqhdhyhj .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #tluqhdhyhj .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #tluqhdhyhj .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #tluqhdhyhj .gt_column_spanner_outer:first-child { padding-left: 0; } #tluqhdhyhj .gt_column_spanner_outer:last-child { padding-right: 0; } #tluqhdhyhj .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #tluqhdhyhj .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #tluqhdhyhj .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #tluqhdhyhj .gt_from_md > :first-child { margin-top: 0; } #tluqhdhyhj .gt_from_md > :last-child { margin-bottom: 0; } #tluqhdhyhj .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #tluqhdhyhj .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #tluqhdhyhj .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #tluqhdhyhj .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #tluqhdhyhj .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #tluqhdhyhj .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #tluqhdhyhj .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #tluqhdhyhj .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #tluqhdhyhj .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #tluqhdhyhj .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #tluqhdhyhj .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #tluqhdhyhj .gt_sourcenote { font-size: 90%; padding: 4px; } #tluqhdhyhj .gt_left { text-align: left; } #tluqhdhyhj .gt_center { text-align: center; } #tluqhdhyhj .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #tluqhdhyhj .gt_font_normal { font-weight: normal; } #tluqhdhyhj .gt_font_bold { font-weight: bold; } #tluqhdhyhj .gt_font_italic { font-style: italic; } #tluqhdhyhj .gt_super { font-size: 65%; } #tluqhdhyhj .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 65%; } Characteristic N = 2131 doa 2011-01-01 to 2012-06-23 dod 2011-01-05 to 2012-06-27 status alive 165 (77%) dead 48 (23%) sex female 122 (57%) male 91 (43%) dm 72 (34%) gcs 15.0 (10.0, 15.0) sbp 160 (142, 186) dbp 90 (79, 103) wbc 9.9 (7.5, 12.5) time2 4 (3, 7) stroke_type HS 69 (32%) IS 144 (68%) referral_from hospital 80 (38%) non-hospital 133 (62%) 1 Range; n (%); Median (IQR) To obtain the patients characteristics based on status: stroke %&gt;% tbl_summary(by = status) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #nnplquzljm .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #nnplquzljm .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #nnplquzljm .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #nnplquzljm .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; border-top-color: #FFFFFF; border-top-width: 0; } #nnplquzljm .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #nnplquzljm .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #nnplquzljm .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #nnplquzljm .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #nnplquzljm .gt_column_spanner_outer:first-child { padding-left: 0; } #nnplquzljm .gt_column_spanner_outer:last-child { padding-right: 0; } #nnplquzljm .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #nnplquzljm .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #nnplquzljm .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #nnplquzljm .gt_from_md > :first-child { margin-top: 0; } #nnplquzljm .gt_from_md > :last-child { margin-bottom: 0; } #nnplquzljm .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #nnplquzljm .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #nnplquzljm .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #nnplquzljm .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #nnplquzljm .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #nnplquzljm .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #nnplquzljm .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #nnplquzljm .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #nnplquzljm .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #nnplquzljm .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #nnplquzljm .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #nnplquzljm .gt_sourcenote { font-size: 90%; padding: 4px; } #nnplquzljm .gt_left { text-align: left; } #nnplquzljm .gt_center { text-align: center; } #nnplquzljm .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #nnplquzljm .gt_font_normal { font-weight: normal; } #nnplquzljm .gt_font_bold { font-weight: bold; } #nnplquzljm .gt_font_italic { font-style: italic; } #nnplquzljm .gt_super { font-size: 65%; } #nnplquzljm .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 65%; } Characteristic alive, N = 1651 dead, N = 481 doa 2011-01-03 to 2012-06-22 2011-01-01 to 2012-06-23 dod 2011-01-06 to 2012-06-27 2011-01-05 to 2012-06-26 sex female 87 (53%) 35 (73%) male 78 (47%) 13 (27%) dm 59 (36%) 13 (27%) gcs 15.0 (14.0, 15.0) 8.5 (5.0, 11.2) sbp 160 (143, 186) 162 (137, 192) dbp 90 (79, 102) 90 (80, 109) wbc 9.0 (7.2, 11.6) 11.4 (10.0, 14.5) time2 4 (3, 6) 5 (2, 12) stroke_type HS 38 (23%) 31 (65%) IS 127 (77%) 17 (35%) referral_from hospital 51 (31%) 29 (60%) non-hospital 114 (69%) 19 (40%) 1 Range; n (%); Median (IQR) To obtain the patients characteristics based on stroke types: stroke %&gt;% tbl_summary(by = stroke_type) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #gcsabxjcyp .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #gcsabxjcyp .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #gcsabxjcyp .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #gcsabxjcyp .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; border-top-color: #FFFFFF; border-top-width: 0; } #gcsabxjcyp .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #gcsabxjcyp .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #gcsabxjcyp .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #gcsabxjcyp .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #gcsabxjcyp .gt_column_spanner_outer:first-child { padding-left: 0; } #gcsabxjcyp .gt_column_spanner_outer:last-child { padding-right: 0; } #gcsabxjcyp .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #gcsabxjcyp .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #gcsabxjcyp .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #gcsabxjcyp .gt_from_md > :first-child { margin-top: 0; } #gcsabxjcyp .gt_from_md > :last-child { margin-bottom: 0; } #gcsabxjcyp .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #gcsabxjcyp .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #gcsabxjcyp .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #gcsabxjcyp .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #gcsabxjcyp .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #gcsabxjcyp .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #gcsabxjcyp .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #gcsabxjcyp .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #gcsabxjcyp .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #gcsabxjcyp .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #gcsabxjcyp .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #gcsabxjcyp .gt_sourcenote { font-size: 90%; padding: 4px; } #gcsabxjcyp .gt_left { text-align: left; } #gcsabxjcyp .gt_center { text-align: center; } #gcsabxjcyp .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #gcsabxjcyp .gt_font_normal { font-weight: normal; } #gcsabxjcyp .gt_font_bold { font-weight: bold; } #gcsabxjcyp .gt_font_italic { font-style: italic; } #gcsabxjcyp .gt_super { font-size: 65%; } #gcsabxjcyp .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 65%; } Characteristic HS, N = 691 IS, N = 1441 doa 2011-01-06 to 2012-06-23 2011-01-01 to 2012-06-16 dod 2011-01-22 to 2012-06-26 2011-01-05 to 2012-06-27 status alive 38 (55%) 127 (88%) dead 31 (45%) 17 (12%) sex female 43 (62%) 79 (55%) male 26 (38%) 65 (45%) dm 19 (28%) 53 (37%) gcs 11.0 (6.0, 14.0) 15.0 (13.8, 15.0) sbp 152 (139, 187) 161 (143, 186) dbp 90 (80, 102) 91 (79, 103) wbc 11.4 (9.4, 14.9) 8.8 (7.2, 11.1) time2 6 (4, 15) 4 (3, 5) referral_from hospital 46 (67%) 34 (24%) non-hospital 23 (33%) 110 (76%) 1 Range; n (%); Median (IQR) 11.5 Kaplan-Meier survival estimates Kaplan-Meier survival estimates is the non-parametric survival estimates. It provides the survival probability estimates at different time. Using survfit(), we can estimate the survival probability based on Kaplan-Meier (KM). Lets estimate the survival probabilities for overall stroke types The survival probabilities for all patients: KM &lt;- survfit(Surv(time = time2, event = status == &quot;dead&quot; ) ~ 1, data = stroke) summary(KM) ## Call: survfit(formula = Surv(time = time2, event = status == &quot;dead&quot;) ~ ## 1, data = stroke) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 1 213 9 0.958 0.0138 0.9311 0.985 ## 2 190 4 0.938 0.0168 0.9053 0.971 ## 3 166 4 0.915 0.0198 0.8770 0.955 ## 4 130 4 0.887 0.0237 0.8416 0.934 ## 5 90 5 0.838 0.0310 0.7790 0.901 ## 6 65 3 0.799 0.0367 0.7301 0.874 ## 7 56 4 0.742 0.0438 0.6608 0.833 ## 9 42 1 0.724 0.0462 0.6391 0.821 ## 10 37 1 0.705 0.0489 0.6150 0.807 ## 12 33 4 0.619 0.0587 0.5142 0.746 ## 14 24 2 0.568 0.0642 0.4548 0.708 ## 18 19 1 0.538 0.0674 0.4206 0.687 ## 22 15 1 0.502 0.0718 0.3792 0.664 ## 25 9 2 0.390 0.0892 0.2494 0.611 ## 28 5 1 0.312 0.0998 0.1669 0.584 ## 29 4 1 0.234 0.1009 0.1007 0.545 ## 41 2 1 0.117 0.0970 0.0231 0.593 Next, we will estimate the survival probabilities for stroke type: KM_str_type2 &lt;- survfit(Surv(time = time2, event = status == &quot;dead&quot; ) ~ stroke_type, data = stroke) summary(KM_str_type2) ## Call: survfit(formula = Surv(time = time2, event = status == &quot;dead&quot;) ~ ## stroke_type, data = stroke) ## ## stroke_type=HS ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 1 69 6 0.913 0.0339 0.8489 0.982 ## 2 61 1 0.898 0.0365 0.8293 0.973 ## 3 58 4 0.836 0.0453 0.7520 0.930 ## 4 52 2 0.804 0.0489 0.7136 0.906 ## 5 47 4 0.736 0.0554 0.6346 0.853 ## 6 38 2 0.697 0.0589 0.5905 0.822 ## 7 34 2 0.656 0.0621 0.5447 0.790 ## 9 30 1 0.634 0.0638 0.5205 0.772 ## 10 27 1 0.611 0.0656 0.4945 0.754 ## 12 24 2 0.560 0.0693 0.4390 0.713 ## 14 19 1 0.530 0.0717 0.4068 0.691 ## 18 15 1 0.495 0.0751 0.3675 0.666 ## 22 11 1 0.450 0.0806 0.3166 0.639 ## 25 6 2 0.300 0.1019 0.1541 0.584 ## 29 2 1 0.150 0.1176 0.0322 0.698 ## ## stroke_type=IS ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 1 144 3 0.979 0.0119 0.956 1.000 ## 2 129 3 0.956 0.0174 0.923 0.991 ## 4 78 2 0.932 0.0241 0.886 0.980 ## 5 43 1 0.910 0.0318 0.850 0.975 ## 6 27 1 0.876 0.0451 0.792 0.970 ## 7 22 2 0.797 0.0676 0.675 0.941 ## 12 9 2 0.620 0.1223 0.421 0.912 ## 14 5 1 0.496 0.1479 0.276 0.890 ## 28 3 1 0.331 0.1671 0.123 0.890 ## 41 1 1 0.000 NaN NA NA 11.6 Plot the survival probability The KM estimate provides the survival probabilites. We can plot these probabilities to look at the trend of survival over time. The plot provides survival probability on the y-axis time on the x-axis ggsurvplot(KM_str_type2, data = stroke, risk.table = TRUE, linetype = c(1,4), tables.height = 0.3, pval = TRUE) We can perform the Kaplan-Meier estimates for variable dm too: KM_dm &lt;- survfit(Surv(time = time2, event = status == &quot;dead&quot; ) ~ dm, data = stroke) summary(KM_dm) ## Call: survfit(formula = Surv(time = time2, event = status == &quot;dead&quot;) ~ ## dm, data = stroke) ## ## dm=no ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 1 141 8 0.943 0.0195 0.9058 0.982 ## 2 122 4 0.912 0.0242 0.8661 0.961 ## 3 102 2 0.894 0.0268 0.8434 0.949 ## 4 82 2 0.873 0.0303 0.8152 0.934 ## 5 54 5 0.792 0.0441 0.7100 0.883 ## 6 40 3 0.732 0.0524 0.6366 0.843 ## 7 34 2 0.689 0.0575 0.5854 0.812 ## 10 24 1 0.661 0.0619 0.5498 0.794 ## 12 20 4 0.529 0.0771 0.3971 0.703 ## 18 13 1 0.488 0.0812 0.3521 0.676 ## 22 9 1 0.434 0.0884 0.2908 0.647 ## 25 4 1 0.325 0.1149 0.1627 0.650 ## 29 3 1 0.217 0.1171 0.0752 0.625 ## ## dm=yes ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 1 72 1 0.986 0.0138 0.9594 1.000 ## 3 64 2 0.955 0.0253 0.9070 1.000 ## 4 48 2 0.915 0.0367 0.8463 0.990 ## 7 22 2 0.832 0.0653 0.7137 0.971 ## 9 15 1 0.777 0.0811 0.6330 0.953 ## 14 9 2 0.604 0.1248 0.4030 0.906 ## 25 5 1 0.483 0.1471 0.2662 0.878 ## 28 2 1 0.242 0.1860 0.0534 1.000 ## 41 1 1 0.000 NaN NA NA And then we can plot the survival estimates for patients with and without diabetes: ggsurvplot(KM_dm, data = stroke, risk.table = TRUE, linetype = c(1,4), tables.height = 0.3, pval = TRUE) 11.7 Comparing Kaplan-Meier estimates across groups There are a number of available tests to compare the survival estimates between groups based on KM. The tests include: log-rank test (default) peto-peto test 11.7.1 Log-rank test From Kaplan-Meier survival curves, we could see the graphical representation of survival probabilities in different group over time. And to answer question if the survival estimates are different between levels or groups we can use statistical tests for example the log rank and the peto-peto tests. For all the test, the null hypothesis is that that the survival estimates between levels or groups are not different. For example, to do that: survdiff(Surv(time = time2, event = status == &quot;dead&quot;) ~ stroke_type, data = stroke, rho = 0) ## Call: ## survdiff(formula = Surv(time = time2, event = status == &quot;dead&quot;) ~ ## stroke_type, data = stroke, rho = 0) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## stroke_type=HS 69 31 24.2 1.92 4.51 ## stroke_type=IS 144 17 23.8 1.95 4.51 ## ## Chisq= 4.5 on 1 degrees of freedom, p= 0.03 The survival estimates between the stroke types (IS vs HS groups) are different at the level of \\(5\\%\\) significance (p = 0.03). And for the survival estimates based on diabetes status: survdiff(Surv(time = time2, event = status == &quot;dead&quot;) ~ dm, data = stroke, rho = 0) ## Call: ## survdiff(formula = Surv(time = time2, event = status == &quot;dead&quot;) ~ ## dm, data = stroke, rho = 0) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## dm=no 141 35 29.8 0.919 2.54 ## dm=yes 72 13 18.2 1.500 2.54 ## ## Chisq= 2.5 on 1 degrees of freedom, p= 0.1 The survival estimates between patients with and without diabetes (dm status yes vs no groups) are not different (p = 0.1). 11.7.2 peto-peto test We will be confident with our results if we obtain almost similar findings from other tests. So, now lets compare survival estimates using the peto-peto test. This is the result for comparing survival estimates for stroke type using peto-peto test. survdiff(Surv(time = time2, event = status == &quot;dead&quot;) ~ stroke_type, data = stroke, rho = 1) ## Call: ## survdiff(formula = Surv(time = time2, event = status == &quot;dead&quot;) ~ ## stroke_type, data = stroke, rho = 1) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## stroke_type=HS 69 25.3 18.7 2.33 6.02 ## stroke_type=IS 144 13.7 20.3 2.15 6.02 ## ## Chisq= 6 on 1 degrees of freedom, p= 0.01 This is the result for comparing survival estimates for diabetes status using peto-peto test. survdiff(Surv(time = time2, event = status == &quot;dead&quot;) ~ dm, data = stroke, rho = 1) ## Call: ## survdiff(formula = Surv(time = time2, event = status == &quot;dead&quot;) ~ ## dm, data = stroke, rho = 1) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## dm=no 141 29.54 24.3 1.11 3.61 ## dm=yes 72 9.41 14.6 1.85 3.61 ## ## Chisq= 3.6 on 1 degrees of freedom, p= 0.06 11.8 Semi-parametric models in survival analysis One advantage of time-to-event data (from a cohort study) is the ability to estimate the hazard or risk to develop the event (outcome) of interest. However, the challenge in the cohort study is the presence of censoring. Censoring can happen due to patients leave the study (loss to follow up) randomly patients do not experience the event even at the termination of the study patients are withdrawn from the study In censored patients, we do not know exactly the time for them to develop the event. To explore how to incorporate a regression model-like structure into the hazard function, we can model the hazard function using: \\[h(t) = \\theta_0\\] The hazard function is a rate, and because of that it must be strictly positive. To constrain \\(\\theta\\) at greater than zero, we can parameterize the hazard function as: \\[h(t) = \\exp^{\\beta_0}\\] So for a covariate \\(x\\) the log-hazard function is: \\[ln[h(t.x)] = \\beta_0 + \\beta_1(x)\\] and the hazard function is \\[h(t.x) = exp^{\\beta_0 + \\beta_1(x)}\\] This is the exponential distribution which is one example of a fully parametric hazard function. Fully parametric models accomplishes two goals simultaneously: It describes the basic underlying distribution of survival time (error component) It characterizes how that distribution changes as a function of the covariates (systematic component). However, even though fully parametric models can be used to accomplish the above goals, the assumptions required for their error components may be unnecessarily stringent or unrealistic. One option is to have a fully parametric regression structure but leave their dependence on time unspecified. The models that utilize this approach are called semiparametric regression models. 11.8.1 Cox proportional hazard regression If we take our dataset for example, where want to compare the survival experience of stroke patients on different types of stroke (Ischaemic stroke vs Haemorrhagic stroke), one form of a regression model for the hazard function that addresses the study goal is: \\[h(t,x,\\beta) = h_0(t)r(x,\\beta)\\] We can see that the hazard function is the product of two functions: The function, \\(h_0(t)\\), characterizes how the hazard function changes as a function of survival time. The function, \\(r(x,\\beta)\\), characterizes how the hazard function changes as a function of subject covariates. The \\(h_0(t)\\) is frequently referred to as the baseline hazard function. Thus the baseline hazard function is, in some sense, a generalization of the intercept or constant term found in parametric regression models. The hazard ratio (HR) depends only on the function \\(r(x,\\beta)\\). If the ratio function \\(HR(t,x_1,x_0)\\) has a clear clinical interpretation then, the actual form of the baseline hazard function is of little importance. With this parameterization the hazard function is \\[h(t,x,\\beta) = h_o(t)exp^{x \\beta}\\] and the hazard ratio is \\[HR(t,x_1, x_0) = exp^{\\beta(x_1 - x_0)}\\] This model is referred to in the literature by a variety of terms, such as the Cox model, the Cox proportional hazards model or simply the proportional hazards model. So for example, if we have a covariate which is a dichomotomous (binary), such as stroke type: coded as a value of \\(x_1 = 1\\) and \\(x_0 = 0\\), for HS and IS, respectively, then the hazard ratio becomes \\[HR(t,x_1, x_0) = exp^\\beta\\] If the value of the coefficient is \\(\\beta = ln(2)\\), then the interpretation is that HS are dying at twice \\((exp^\\beta = 2)\\) the rate of patients with IS. 11.8.2 Advantages of the Cox proportional hazard regression If you remember that by using Kaplan-Meier (KM) analysis, we could estimate the survival probability. And using the log-rank or peto-peto test, we could compare the survival between categorical covariates. However, the disadvantages of KM include: Need to categorize numerical variable to compare survival It is a univariable analysis It is a non-parametric analysis We also acknowledge that the fully parametric regression models in survival analysis have stringent assumptions and distribution requirement. So, to overcome the limitations of the KM analysis and the fully parametric analysis, we can model our survival data using the semi-parametric Cox proportional hazard regression. 11.9 Estimation from Cox proportional hazards regression 11.9.1 Simple Cox PH regression Using our stroke dataset, we will estimate the parameters using the Cox PH regression. Remember, in our data we have the time variable : time2 the event variable : status and the event of interest is dead. Event classified other than dead are considered as censored. date variables : date of admission (doa) and date of discharge (dod) all other covariates Now lets take stroke type as the covariate of interest: stroke_stype &lt;- coxph(Surv(time = time2, event = status == &#39;dead&#39;) ~ stroke_type, data = stroke) summary(stroke_stype) ## Call: ## coxph(formula = Surv(time = time2, event = status == &quot;dead&quot;) ~ ## stroke_type, data = stroke) ## ## n= 213, number of events= 48 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## stroke_typeIS -0.6622 0.5157 0.3172 -2.088 0.0368 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## stroke_typeIS 0.5157 1.939 0.277 0.9602 ## ## Concordance= 0.623 (se = 0.045 ) ## Likelihood ratio test= 4.52 on 1 df, p=0.03 ## Wald test = 4.36 on 1 df, p=0.04 ## Score (logrank) test = 4.48 on 1 df, p=0.03 But for nicer output (in a data frame format), we can use tidy(). This will give us the estimate which is the log hazard. If you exponentiate it, you will get hazard ratio the standard error the p-value the confidence intervals for the log hazard tidy(stroke_stype, conf.int = TRUE) ## # A tibble: 1 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 stroke_typeIS -0.662 0.317 -2.09 0.0368 -1.28 -0.0406 The simple Cox PH model with covariate stroke type shows that the patients with IS has \\(-0.0662\\) times the crude log hazard for death as compared to patients with HS (p-value = 0.0368). The \\(95\\%\\) confidence intervals for the crude log hazards are calculated by: \\[\\hat\\beta \\pm 1.96 \\times \\widehat{SE}(\\hat\\beta)\\] \\[-0.662 \\pm 1.96 \\times 0.317 = -1.284 , -0.041\\] tidy(stroke_stype, exponentiate = TRUE, conf.int = TRUE) ## # A tibble: 1 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 stroke_typeIS 0.516 0.317 -2.09 0.0368 0.277 0.960 Or we can get the crude hazard ratio (HR) by exponentiating the log HR. In this example, the simple Cox PH model with covariate stroke type shows that the patients with IS has \\(49\\%\\) lower risk for stroke death as compared to patients with HS (p-value = 0.0368 and \\(95\\% CI 0.277, 0.960\\)). The \\(95\\%\\) confidence intervals for crude HR are calculated by \\[exp[\\hat\\beta \\pm 1.96 \\times \\widehat{SE}(\\hat\\beta)]\\] Hence, the lower bound for crude HR is \\(exp(-1.284)= 0.277\\) and the upper bound for crude HR is \\(exp(-0.0041) = 0.996\\) at \\(95\\%\\) confidence. Lets model the risk for stroke death for covariate gcs: stroke_gcs &lt;- coxph(Surv(time = time2, event = status == &#39;dead&#39;) ~ gcs, data = stroke) summary(stroke_gcs) ## Call: ## coxph(formula = Surv(time = time2, event = status == &quot;dead&quot;) ~ ## gcs, data = stroke) ## ## n= 213, number of events= 48 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## gcs -0.17454 0.83984 0.03431 -5.087 3.63e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## gcs 0.8398 1.191 0.7852 0.8983 ## ## Concordance= 0.763 (se = 0.039 ) ## Likelihood ratio test= 26.01 on 1 df, p=3e-07 ## Wald test = 25.88 on 1 df, p=4e-07 ## Score (logrank) test = 29.33 on 1 df, p=6e-08 The simple Cox PH model with covariate gcs shows that with each one unit increase in gcs, the crude log hazard for death changes by a factor of \\(-0.175\\). tidy(stroke_gcs, exponentiate = TRUE, conf.int = TRUE) ## # A tibble: 1 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 gcs 0.840 0.0343 -5.09 0.000000363 0.785 0.898 When we exponentiate the log HR, the simple Cox PH model shows that with each one unit increase in gcs, the crude risk for death decreases for about \\(16\\%\\) and the of decrease are between \\(95\\% CI (0.785, 0.898)\\). The relationship between stroke death and gcs is highly significant (p-value \\(&lt; 0.0001\\)) when not adjusting for other covariates. By using tbl_uvregression() we can generate simple univariable model for all covariates in one line of code. In return, we get the crude HR for all the covariates of interest. stroke %&gt;% dplyr::select(time2, status, sex, dm, gcs, sbp, dbp, wbc, stroke_type, referral_from) %&gt;% tbl_uvregression( method = coxph, y = Surv(time2, event = status == &#39;dead&#39;), exponentiate = TRUE, pvalue_fun = ~style_pvalue(.x, digits = 3) ) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #kdodeeibbl .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #kdodeeibbl .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #kdodeeibbl .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #kdodeeibbl .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; border-top-color: #FFFFFF; border-top-width: 0; } #kdodeeibbl .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #kdodeeibbl .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #kdodeeibbl .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #kdodeeibbl .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #kdodeeibbl .gt_column_spanner_outer:first-child { padding-left: 0; } #kdodeeibbl .gt_column_spanner_outer:last-child { padding-right: 0; } #kdodeeibbl .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #kdodeeibbl .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #kdodeeibbl .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #kdodeeibbl .gt_from_md > :first-child { margin-top: 0; } #kdodeeibbl .gt_from_md > :last-child { margin-bottom: 0; } #kdodeeibbl .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #kdodeeibbl .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #kdodeeibbl .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #kdodeeibbl .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #kdodeeibbl .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #kdodeeibbl .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #kdodeeibbl .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #kdodeeibbl .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #kdodeeibbl .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #kdodeeibbl .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #kdodeeibbl .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #kdodeeibbl .gt_sourcenote { font-size: 90%; padding: 4px; } #kdodeeibbl .gt_left { text-align: left; } #kdodeeibbl .gt_center { text-align: center; } #kdodeeibbl .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #kdodeeibbl .gt_font_normal { font-weight: normal; } #kdodeeibbl .gt_font_bold { font-weight: bold; } #kdodeeibbl .gt_font_italic { font-style: italic; } #kdodeeibbl .gt_super { font-size: 65%; } #kdodeeibbl .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 65%; } Characteristic N HR1 95% CI1 p-value sex 213 female   male 0.71 0.37, 1.36 0.299 dm 213 no   yes 0.60 0.31, 1.13 0.112 gcs 213 0.84 0.79, 0.90 sbp 213 1.00 0.99, 1.01 0.617 dbp 213 1.00 0.98, 1.01 0.772 wbc 213 1.04 0.97, 1.11 0.270 stroke_type 213 HS   IS 0.52 0.28, 0.96 0.037 referral_from 213 hospital   non-hospital 0.58 0.32, 1.05 0.074 1 HR = Hazard Ratio, CI = Confidence Interval 11.9.2 Multiple Cox PH regression There are two primary reasons to include more than one covariates in the model. One of the primary reasons for using a regression model is to include multiple covariates to adjust statistically for possible imbalances in the observed data before making statistical inferences. In traditional statistical applications, it is called analysis of covariance, while in clinical and epidemiological investigations it is often called control of confounding. The other reason is a statistically related issue where the inclusion of higher-order terms in a model representing interactions between covariates. These are also called effect modifiers. Lets decide based on our clinical expertise and statistical significance, we would model a Cox PH model with these covariates. stroke_type gcs referral_from The reasons because we found that both gcs and stroke type are statistically significant. We also believe that the way patients are referred to hospital may indicate the severity of stroke. On the other hand, we assumed variables sex, sbp, dbp and wbc are not clinically important risk factors for stroke death. In addition to that, we foresee that stroke type and death is mediated through gcs. For example, patients that suffer from haemorrhagic stroke will suffer more severe bleeding. This will lead to poorer gcs and pooer survival status. So, by adding both gcs and stroke, we can estimate the total effect from stroke type to death and the effect mediated through gcs. To estimate to Cox PH model with stroke_type, gcs and referral_from: stroke_mv &lt;- coxph(Surv(time = time2, event = status == &#39;dead&#39;) ~ stroke_type + gcs + referral_from, data = stroke) tidy(stroke_mv, exponentiate = TRUE, conf.int = TRUE) ## # A tibble: 3 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 stroke_typeIS 0.835 0.345 -0.523 6.01e-1 0.424 1.64 ## 2 gcs 0.847 0.0358 -4.63 3.72e-6 0.790 0.909 ## 3 referral_fromnon-ho~ 0.824 0.322 -0.602 5.47e-1 0.439 1.55 We would like to doubly confirm if the model with covariates stroke_type, gcs and referral_from and really statistically different from model with stroke type and referral from: stroke_mv2 &lt;- coxph(Surv(time = time2, event = status == &#39;dead&#39;) ~ stroke_type + referral_from, data = stroke) tidy(stroke_mv, exponentiate = TRUE, conf.int = TRUE) ## # A tibble: 3 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 stroke_typeIS 0.835 0.345 -0.523 6.01e-1 0.424 1.64 ## 2 gcs 0.847 0.0358 -4.63 3.72e-6 0.790 0.909 ## 3 referral_fromnon-ho~ 0.824 0.322 -0.602 5.47e-1 0.439 1.55 We can confirm this by running the likelihood ratio test between the two Cox PH models: anova(stroke_mv, stroke_mv2, test = &#39;Chisq&#39;) ## Analysis of Deviance Table ## Cox model: response is Surv(time = time2, event = status == &quot;dead&quot;) ## Model 1: ~ stroke_type + gcs + referral_from ## Model 2: ~ stroke_type + referral_from ## loglik Chisq Df P(&gt;|Chi|) ## 1 -187.29 ## 2 -198.08 21.57 1 3.412e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 And true enough, the two Cox PH modes are different (p-value &lt; 0.0001). And we will choose the larger model. 11.10 Inference 11.11 Adding interaction in the model Interaction in the model is an interesting phenomenon. To illustrate how we manage interaction term, lets start with a model with three covariates, gcs, stroke type and dm. In this model, based on clinical knowledge and curiosity, we would develop a two-way interaction term between gcs and stroke type. This interaction term (a product from gcs and stroke type) means the relationship between stroke type and risk for death is heterogenous in different values of gcs. Lets assign stroke type as \\(st\\) and code it as either 1 (HS) and 0 (IS) \\[g(t,st,gsc,dm,\\beta) = ln[h_0(t) + st\\beta_1 + gcs\\beta_2 + (st \\times gcs)\\beta_3 + dm\\beta_4]\\] And the difference in the log hazard function becomes \\[[g(t,st = 1,gcs, dm,\\beta) - g(t,st = 0,gcs, dm,\\beta)]\\] \\[= ln[h_0(t) + 1beta_1 + gcs\\beta_2 + (1gcs)\\beta_3 + dm\\beta_4] -\\] \\[ln[h_0(t) + 0beta_1 + gcs\\beta_2 + (0gcs)\\beta_3 + dm\\beta_4]\\] \\[=beta_1 + gcs\\beta_3\\] To run model with an interaction for Cox PH model, we can: stroke_noia &lt;- coxph(Surv(time = time2, event = status == &#39;dead&#39;) ~ stroke_type + gcs + dm, data = stroke) stroke_ia &lt;- coxph(Surv(time = time2, event = status == &#39;dead&#39;) ~ stroke_type + gcs + stroke_type:gcs + dm, data = stroke) tidy(stroke_ia, exponentiate = TRUE, conf.int = TRUE) ## # A tibble: 4 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 stroke_typeIS 0.770 0.759 -0.345 0.730 0.174 3.41 ## 2 gcs 0.839 0.0480 -3.67 0.000244 0.763 0.921 ## 3 dmyes 0.653 0.342 -1.25 0.212 0.334 1.28 ## 4 stroke_typeIS:gcs 1.01 0.0728 0.187 0.852 0.879 1.17 It appears that the p-value for the interaction term (a product of gcs and stroke type) is large (p value = 0.852). anova(stroke_ia, stroke_noia, test = &#39;Chisq&#39;) ## Analysis of Deviance Table ## Cox model: response is Surv(time = time2, event = status == &quot;dead&quot;) ## Model 1: ~ stroke_type + gcs + stroke_type:gcs + dm ## Model 2: ~ stroke_type + gcs + dm ## loglik Chisq Df P(&gt;|Chi|) ## 1 -186.61 ## 2 -186.63 0.0351 1 0.8514 And when we run likelihood ratio test, we also observe the same result (p value = 0.851). And because of that, we decide not to add the interaction term in the model. However, it is also recommended to confirm this decision after discussion with subject matter expert, in this case, a stroke physician or neurosurgeon. 11.12 The proportional hazard assumption 11.12.1 Risk constant over time The most important assumption in Cox PH regression is the proportionality of the hazards over time. It refers to the requirement that, the hazard functions are multiplicatively related and that their ratio is constant over survival time or it simply says that the estimated HR do not depend on time. A check of the proportional hazards assumption can be done by looking at the parameter estimates \\(\\beta_1, ..., \\beta_q\\) over time. And we can safely assume proportional hazards when the estimates dont vary much over time. In most settings, in order to test the PH assumption, we can employ two-step procedure for assessing: calculate covariate specific tests and plot the scaled and smoothed scaled Schoenfeld residuals obtained from the model. 11.12.2 Test for PH assumption In many statistical software, the null hypothesis of constant regression coefficients can be tested, both globally as well as for each covariate. In R, this can be done using the cox.zph() function from the survival package. stroke_zph &lt;- cox.zph(stroke_mv, transform = &#39;km&#39;) stroke_zph ## chisq df p ## stroke_type 1.38336 1 0.24 ## gcs 0.00764 1 0.93 ## referral_from 1.39355 1 0.24 ## GLOBAL 1.63410 3 0.65 The global test is not significant (p value = 0.65)E and the p-value for each of the covariate is also larger than 0.05. These evidence support that the there risks are proportional over time. stroke_zph_rank &lt;- cox.zph(stroke_mv, transform = &#39;rank&#39;) stroke_zph_rank ## chisq df p ## stroke_type 1.29 1 0.26 ## gcs 1.62 1 0.20 ## referral_from 1.86 1 0.17 ## GLOBAL 3.05 3 0.38 11.12.3 Plots to assess PH assumption We can plots these residuals deviance schoenfeld scaled schoenfeld For example, lets start with plotting the residuals ggcoxdiagnostics(stroke_mv, type = &quot;deviance&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Now, we will use schoenfeld ggcoxdiagnostics(stroke_mv, type = &quot;schoenfeld&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; ggcoxdiagnostics(stroke_mv, type = &quot;deviance&quot;, ox.scale = &#39;time&#39;) ## Warning in ggcoxdiagnostics(stroke_mv, type = &quot;deviance&quot;, ox.scale = &quot;time&quot;): ## ox.scale=&#39;time&#39; works only with type=schoenfeld/scaledsch ## `geom_smooth()` using formula &#39;y ~ x&#39; ggcoxdiagnostics(stroke_mv, type = &quot;deviance&quot;, ox.scale = &quot;linear.predictions&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; ggcoxdiagnostics(stroke_mv, type = &quot;schoenfeld&quot;, ox.scale = &quot;observation.id&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; You may refer to this link for more details http://pdf.medrang.co.kr/CSAM/2017/024/csam-24-583_suppl.pdf plot(stroke_zph, var = &quot;stroke_type&quot;) for gcs plot(stroke_zph, var = &quot;gcs&quot;) The plot for gcs shows possible violation of PH assumption. Even though, the coxzph() shows p value of larger than 0.05, there is a possibility that due to small sample size in the data, the PH test lost its power. However we believe that the violation is not severe. for referral type plot(stroke_zph, var = &quot;referral_from&quot;) In the case of serious violation of proportionality of hazard, we can remedy using stratified cox regression extended cox regression using time-varying dependent variable parametric survival analysis 11.13 Model checking 11.13.1 Prediction from Cox PH model From the Cox PH model, we can predict the linear predictor the risk the expected number of events given the covariates and follow-up time The linear predictor stroke_lp &lt;- augment(stroke_mv, data = stroke) stroke_lp %&gt;% dplyr::select(gcs, stroke_type, referral_from, .fitted:.resid) %&gt;% slice(1:10) ## # A tibble: 10 x 6 ## gcs stroke_type referral_from .fitted .se.fit .resid ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 15 IS non-hospital -0.785 0.365 -0.0192 ## 2 15 IS non-hospital -0.785 0.365 -0.0192 ## 3 11 HS hospital 0.252 0.0545 0.970 ## 4 3 IS hospital 1.40 0.537 0.907 ## 5 15 IS non-hospital -0.785 0.365 -0.0192 ## 6 3 HS hospital 1.58 0.341 0.888 ## 7 11 IS hospital 0.0716 0.361 -0.0453 ## 8 15 IS non-hospital -0.785 0.365 -0.0192 ## 9 6 HS hospital 1.08 0.234 0.932 ## 10 15 IS non-hospital -0.785 0.365 -0.0192 The predicted risk which is the risk score \\(exp(lp)\\) (risk), risks &lt;- augment(stroke_mv, data = stroke, type.predict = &quot;risk&quot;) risks %&gt;% dplyr::select(status, gcs, stroke_type, referral_from, .fitted:.resid) %&gt;% slice(1:10) ## # A tibble: 10 x 7 ## status gcs stroke_type referral_from .fitted .se.fit .resid ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 alive 15 IS non-hospital 0.456 0.247 -0.0192 ## 2 alive 15 IS non-hospital 0.456 0.247 -0.0192 ## 3 dead 11 HS hospital 1.29 0.0618 0.970 ## 4 dead 3 IS hospital 4.05 1.08 0.907 ## 5 alive 15 IS non-hospital 0.456 0.247 -0.0192 ## 6 dead 3 HS hospital 4.85 0.751 0.888 ## 7 alive 11 IS hospital 1.07 0.374 -0.0453 ## 8 alive 15 IS non-hospital 0.456 0.247 -0.0192 ## 9 dead 6 HS hospital 2.95 0.401 0.932 ## 10 alive 15 IS non-hospital 0.456 0.247 -0.0192 The expected is the expected number of events given the covariates and follow-up time (expected). The survival probability for a subject is equal to \\(exp(-expected)\\). expected &lt;- augment(stroke_mv, data = stroke, type.predict = &quot;expected&quot;) expected %&gt;% dplyr::select(status, gcs, stroke_type, referral_from, .fitted:.resid) %&gt;% mutate(surv_prob = exp(-(.fitted))) %&gt;% slice(1:10) ## # A tibble: 10 x 8 ## status gcs stroke_type referral_from .fitted .se.fit .resid surv_prob ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 alive 15 IS non-hospital 0.0192 0.00801 -0.0192 0.981 ## 2 alive 15 IS non-hospital 0.0192 0.00801 -0.0192 0.981 ## 3 dead 11 HS hospital 0.0297 0.0209 0.970 0.971 ## 4 dead 3 IS hospital 0.0933 0.0853 0.907 0.911 ## 5 alive 15 IS non-hospital 0.0192 0.00801 -0.0192 0.981 ## 6 dead 3 HS hospital 0.112 0.0850 0.888 0.894 ## 7 alive 11 IS hospital 0.0453 0.0193 -0.0453 0.956 ## 8 alive 15 IS non-hospital 0.0192 0.00801 -0.0192 0.981 ## 9 dead 6 HS hospital 0.0680 0.0472 0.932 0.934 ## 10 alive 15 IS non-hospital 0.0192 0.00801 -0.0192 0.981 The Cox model is a relative risk model; predictions of type linear predictor, risk, and terms are all relative to the sample from which they came. By default, the reference value for each of these is the mean covariate within strata. Predictions of type expected incorporate the baseline hazard and are thus absolute instead of relative; the reference option has no effect on these. 11.13.2 Residuas from Cox PH model Here, we will generate the martingale residuals deviance schoenfeld dfbeta scaled schoenfeld rmartingale &lt;- residuals(stroke_mv, &#39;martingale&#39;) rdeviance &lt;- residuals(stroke_mv, &#39;deviance&#39;) rschoenfeld &lt;- residuals(stroke_mv, &#39;schoenfeld&#39;) rdfbeta &lt;- residuals(stroke_mv, &#39;dfbeta&#39;) rscaled_sch &lt;- residuals(stroke_mv, &#39;scaledsch&#39;) 11.13.3 Influential observations We may check the \\(dfbetas\\) residual. This is residual that comes from a transformation of the score residual. It enables us to check the influence of dropping any single observation on parameter estimates. We may suspect influential observations when the \\(dfbetas\\) residuals greater than 1. ggcoxdiagnostics(stroke_mv, type = &quot;dfbetas&quot;, point.size = 0, hline.col = &quot;black&quot;, sline.col = &quot;black&quot;) + geom_bar(stat = &quot;identity&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; 11.14 Plot the adjusted survival The function surv_adjustedcurves() calculates while the function ggadjustedcurves() plots adjusted survival curves for the coxph model. The main idea behind this function is to present expected survival curves calculated based on Cox model separately for subpopulations. ggadjustedcurves(stroke_mv, data = stroke) 11.15 Presentation and interpretation The Cox PH model that we believe explains the risk for death among hospitalized acute stroke patients can be presented as below: stroke_mv %&gt;% tbl_regression( exponentiate = TRUE, pvalue_fun = ~style_pvalue(.x, digits = 3) ) %&gt;% add_nevent(location = &#39;level&#39;) %&gt;% bold_p(t = 0.10) %&gt;% bold_labels() html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #ahskrhqsjy .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #ahskrhqsjy .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ahskrhqsjy .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #ahskrhqsjy .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; border-top-color: #FFFFFF; border-top-width: 0; } #ahskrhqsjy .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ahskrhqsjy .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ahskrhqsjy .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #ahskrhqsjy .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #ahskrhqsjy .gt_column_spanner_outer:first-child { padding-left: 0; } #ahskrhqsjy .gt_column_spanner_outer:last-child { padding-right: 0; } #ahskrhqsjy .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #ahskrhqsjy .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #ahskrhqsjy .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #ahskrhqsjy .gt_from_md > :first-child { margin-top: 0; } #ahskrhqsjy .gt_from_md > :last-child { margin-bottom: 0; } #ahskrhqsjy .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #ahskrhqsjy .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #ahskrhqsjy .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ahskrhqsjy .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #ahskrhqsjy .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ahskrhqsjy .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #ahskrhqsjy .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #ahskrhqsjy .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ahskrhqsjy .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ahskrhqsjy .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #ahskrhqsjy .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ahskrhqsjy .gt_sourcenote { font-size: 90%; padding: 4px; } #ahskrhqsjy .gt_left { text-align: left; } #ahskrhqsjy .gt_center { text-align: center; } #ahskrhqsjy .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #ahskrhqsjy .gt_font_normal { font-weight: normal; } #ahskrhqsjy .gt_font_bold { font-weight: bold; } #ahskrhqsjy .gt_font_italic { font-style: italic; } #ahskrhqsjy .gt_super { font-size: 65%; } #ahskrhqsjy .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 65%; } Characteristic Event N HR1 95% CI1 p-value stroke_type HS 31   IS 17 0.83 0.42, 1.64 0.601 gcs 48 0.85 0.79, 0.91 referral_from hospital 29   non-hospital 19 0.82 0.44, 1.55 0.547 1 HR = Hazard Ratio, CI = Confidence Interval 11.16 References For further understanding of the survival analysis concepts and application, excellent text include (Lemeshow_Stanley2008-03-07?)(survival-book?) "],["missing-data.html", "Chapter 12 Missing data 12.1 Introduction 12.2 Preliminaries 12.3 Exploring missing data 12.4 Handling missing data 12.5 Presentation 12.6 References", " Chapter 12 Missing data 12.1 Introduction 12.1.1 Background Missing data data is quite a common issue in research. The causes of missing data should always be investigated and more data should be collected if possible. There are three types of missing data: Missing completely at random (MCAR) Missing at random (MAR) Missing not at random (MNAR) Illustration of missing data mechanism for the three types of missing data. X represents an observed variable, Y represents a variable with part of missing values that have been observed, Z represents a cause of missing values and R represents a variable with part of missing values that have been missing. Missing data (R) is classified as MCAR if the cause of missing data (Z) is unrelated to the data (Y). For example, medical records that is lost due to flood and laboratory equipment malfunction. In other words, the data is missing totally by chance. We can relate this example to the figure above, the cause, Z is not related to the data itself that have been observed, Y, but only related to the data that is missing, R. MCAR is ideal and more convenient, though MAR is more common and realistic. Missing data (R) is said to be MAR if the cause of missing data (Z) is related to other observed value (X), but not to the variable itself (Y). For example, an older person is more likely complete a survey compared to the younger person. So, the missingness is related to age, a variable that has been observed. Similarly, if information on income is more likely to be missing for older individuals as they are more cautious to reveal a sensitive information as opposed to younger individuals. Thus, the missingness is related to age, which should be a variable that we have information on. So, this missingness is considered as MAR. Another example of MAR is missing a certain variable due to the old medical form do not request this information. However, a new updated medical form request this information. As the missingness is not related the variable itself and we have the information whether the patients use an old or a new medical form, we can classify this missingness as MAR. We can see from all the examples that the cause of missing data, Z is related to other variable, X, but not to the variable itself, Y as illustrated in the figure. Lastly, the missing data (R) is considered MNAR if the cause of missing data (Z) is related to the variable itself (Y) and to other variables (X) as well. Also, the missingness is considered MNAR if the causes completely unknown to us. In other words, we can not logically deduce that the missing data fit MCAR or MAR types. For example, missing a weight information for an obese individuals as the normal weighing scale may not able to weigh the individuals. Thus, the missing weight values (R) is considered MNAR as the missingness is related the variable itself (Y). However, we can never be sure of this without a further investigation about the mechanism of missingnness and its causes. MNAR is the most problematic type among the three. There are a few approaches to differentiate between MCAR and MAR-MNAR. However, an approach to differentiate between MAR and MNAR has yet to be proposed. Thus, we need to use a logical reasoning to differentiate between the two types. 12.1.2 Objectives At the end of the chapter, the reader will be able to: To perform a simple imputation To perform a single imputation To perform a multiple imputation 12.2 Preliminaries 12.2.1 Packages We will use these packages: mice: for the single and multiple imputation VIM: for missing data exploration naniar: for missing data exploration tidyverse: for data wrangling and manipulation gtsummary: to provide a nice result in a table library(mice) library(VIM) library(naniar) library(tidyverse) library(gtsummary) 12.2.2 Dataset We going to use the coronary dataset that we used previously in linear regression chapter. However, this dataset have been altered to generate a missing values in it. coroNA &lt;- read_csv(here::here(&#39;data&#39;, &quot;coronaryNA.csv&quot;)) Next, we going to transform the all the character variables into a factor. mutate_if() will recognise all character variables and transform it into a factor. coroNA &lt;- coroNA %&gt;% mutate_if(is.character, as.factor) summary(coroNA) ## age race chol id cad ## Min. :33.00 chinese:31 Min. :4.000 Min. : 1.0 cad : 37 ## 1st Qu.:43.00 indian :45 1st Qu.:5.362 1st Qu.: 901.5 no cad:163 ## Median :48.00 malay :49 Median :6.050 Median :2243.5 ## Mean :48.02 NA&#39;s :75 Mean :6.114 Mean :2218.3 ## 3rd Qu.:53.00 3rd Qu.:6.875 3rd Qu.:3346.8 ## Max. :62.00 Max. :9.350 Max. :4696.0 ## NA&#39;s :85 NA&#39;s :33 ## sbp dbp bmi gender ## Min. : 88.0 Min. : 56.00 Min. :28.99 man :100 ## 1st Qu.:115.0 1st Qu.: 72.00 1st Qu.:36.10 woman:100 ## Median :126.0 Median : 80.00 Median :37.80 ## Mean :130.2 Mean : 82.31 Mean :37.45 ## 3rd Qu.:144.0 3rd Qu.: 92.00 3rd Qu.:39.20 ## Max. :187.0 Max. :120.00 Max. :45.03 ## Missing data in R is denoted by NA. Further information about NA can be assessed by typing ?NA in the RStudio console. As seen above, the three variables; age, race and chol have missing values. We can further confirm this using a anyNA(). anyNA(coroNA) ## [1] TRUE True indicates a presence of missing values in our data. Thus, we can further explore the missing values in our data. 12.3 Exploring missing data The total percentage of missing values in our data is 10.7%. prop_miss(coroNA) ## [1] 0.1072222 Percentage of missing data by variable: miss_var_summary(coroNA) ## # A tibble: 9 x 3 ## variable n_miss pct_miss ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 age 85 42.5 ## 2 race 75 37.5 ## 3 chol 33 16.5 ## 4 id 0 0 ## 5 cad 0 0 ## 6 sbp 0 0 ## 7 dbp 0 0 ## 8 bmi 0 0 ## 9 gender 0 0 Both prop_miss() and miss_var_summary() are from naniar package. Subsequently, We can explore the pattern of missing data using aggr() function from VIM. The numbers and prop arguments indicate that we want the missing information on the y-axis of the plot to be in number not proportion. aggr(coroNA, numbers = TRUE, prop = FALSE) The labels on x-axis are according to the column names in the dataset. In term of pattern of missingness, about 43 observation are missing age values alone. Similarly, the pattern of missing data can be explore through md.pattern() from mice package. The rotate.names argument set to true to specify the direction of the variable names on top of the plot horizontally. md.pattern(coroNA, rotate.names = TRUE) ## id cad sbp dbp bmi gender chol race age ## 64 1 1 1 1 1 1 1 1 1 0 ## 43 1 1 1 1 1 1 1 1 0 1 ## 28 1 1 1 1 1 1 1 0 1 1 ## 32 1 1 1 1 1 1 1 0 0 2 ## 12 1 1 1 1 1 1 0 1 1 1 ## 6 1 1 1 1 1 1 0 1 0 2 ## 11 1 1 1 1 1 1 0 0 1 2 ## 4 1 1 1 1 1 1 0 0 0 3 ## 0 0 0 0 0 0 33 75 85 193 Additionally, we can assess the correlation: Between variables with missing values Between variables with missing values and variable with non-missing values First, we need to take variables with missing values only. Then, we code a missing value with 1 and non-missing value with 0. dummyNA &lt;- as.data.frame(abs(is.na(coroNA))) %&gt;% select(age, race, chol) # pick variable with missing values only head(dummyNA) ## age race chol ## 1 0 0 0 ## 2 0 0 0 ## 3 0 0 0 ## 4 1 0 0 ## 5 1 0 0 ## 6 0 0 0 We assess the correlation between variables with missing values. cor(dummyNA) %&gt;% round(digits = 2) ## age race chol ## age 1.00 0.09 -0.11 ## race 0.09 1.00 0.07 ## chol -0.11 0.07 1.00 There is no strong correlation between variables with missing values. We can conclude that the missing values in one variable is not related to the missing values in another variable. The second correlation is between variables with missing values and variable with non-missing values. First, we need to change the categorical variable into a numeric value to get a correlation for all the variables. cor(coroNA %&gt;% mutate_if(is.factor, as.numeric), dummyNA, use = &quot;pairwise.complete.obs&quot;) %&gt;% round(digits = 2) ## Warning in cor(coroNA %&gt;% mutate_if(is.factor, as.numeric), dummyNA, use = ## &quot;pairwise.complete.obs&quot;): the standard deviation is zero ## age race chol ## age NA 0.04 0.04 ## race 0.02 NA 0.01 ## chol -0.11 -0.06 NA ## id 0.01 -0.06 -0.07 ## cad 0.02 0.05 -0.17 ## sbp -0.07 0.04 0.17 ## dbp -0.04 0.04 0.17 ## bmi 0.13 -0.08 -0.76 ## gender 0.07 0.03 -0.07 We can safely ignore the warning generated in the result. Variables on the left side are variable with non-missing values and variables on the top of the columns are variables with missing values. There is a high correlation (-0.76) between bmi and chol indicating values of chol are more likely to be missing at lower values of bmi. Lastly, we can do a Littles test to determine if the missing data is MCAR or other types. The null hypothesis is the missing data is MCAR. Thus, a higher p value indicates probability of missing data is MCAR. The test shows that the missingness in our data is either MAR or MNAR. mcar_test(coroNA) ## # A tibble: 1 x 4 ## statistic df p.value missing.patterns ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 158. 51 8.25e-13 8 12.4 Handling missing data We going to cover four approaches to handling missing data: Listwise deletion Simple imputation: Mean substitution Median substitution Mode substitution Single imputation: Regression imputation Stochastic regression imputation Decision tree imputation Multiple imputation Each approach has their own caveats which we will cover in each section below. 12.4.1 Listwise deletion Listwise or case deletion is the default setting in R. By default, R will exclude all the rows with missing data. lw &lt;- lm(dbp ~ ., data = coroNA) summary(lw) ## ## Call: ## lm(formula = dbp ~ ., data = coroNA) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16.7770 -3.8094 -0.0874 3.5889 13.3479 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.6358468 20.5615343 1.684 0.0979 . ## age -0.2764718 0.2139740 -1.292 0.2018 ## raceindian 4.6080430 2.9423909 1.566 0.1232 ## racemalay -0.1318924 2.6075792 -0.051 0.9598 ## chol 0.7174697 0.8142408 0.881 0.3821 ## id 0.0000390 0.0006004 0.065 0.9484 ## cadno cad -1.2884879 2.2026942 -0.585 0.5610 ## sbp 0.5390141 0.0502732 10.722 5.48e-15 *** ## bmi -0.4189575 0.4086179 -1.025 0.3098 ## genderwoman 2.3084961 1.6749082 1.378 0.1738 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.218 on 54 degrees of freedom ## (136 observations deleted due to missingness) ## Multiple R-squared: 0.7581, Adjusted R-squared: 0.7177 ## F-statistic: 18.8 on 9 and 54 DF, p-value: 1.05e-13 As shown in the information above (at the bottom), 136 rows or observations were excluded due to missingness. Listwise deletion approach able to produce an unbiased result only when the missing data is MCAR and the amount of missing data is relatively small. 12.4.2 Simple imputation A simple imputation includes mean, median and mode substitution is a relatively easy approach. In this approach, the missing values is replaced by a mean or median for numerical variable, and mode for categorical variable. This simple imputation approach only appropriate if missing data is MCAR and the amount of missing data is relatively small. Mean substitution replace_na will replace the missing values in age with its mean. mean_sub &lt;- coroNA %&gt;% mutate(age = replace_na(age, mean(age, na.rm = T))) summary(mean_sub) ## age race chol id cad ## Min. :33.00 chinese:31 Min. :4.000 Min. : 1.0 cad : 37 ## 1st Qu.:46.00 indian :45 1st Qu.:5.362 1st Qu.: 901.5 no cad:163 ## Median :48.02 malay :49 Median :6.050 Median :2243.5 ## Mean :48.02 NA&#39;s :75 Mean :6.114 Mean :2218.3 ## 3rd Qu.:49.00 3rd Qu.:6.875 3rd Qu.:3346.8 ## Max. :62.00 Max. :9.350 Max. :4696.0 ## NA&#39;s :33 ## sbp dbp bmi gender ## Min. : 88.0 Min. : 56.00 Min. :28.99 man :100 ## 1st Qu.:115.0 1st Qu.: 72.00 1st Qu.:36.10 woman:100 ## Median :126.0 Median : 80.00 Median :37.80 ## Mean :130.2 Mean : 82.31 Mean :37.45 ## 3rd Qu.:144.0 3rd Qu.: 92.00 3rd Qu.:39.20 ## Max. :187.0 Max. :120.00 Max. :45.03 ## Median substitution replace_na will replace the missing values in age with its median. med_sub &lt;- coroNA %&gt;% mutate(age = replace_na(age, median(age, na.rm = T))) summary(med_sub) ## age race chol id cad ## Min. :33.00 chinese:31 Min. :4.000 Min. : 1.0 cad : 37 ## 1st Qu.:46.00 indian :45 1st Qu.:5.362 1st Qu.: 901.5 no cad:163 ## Median :48.00 malay :49 Median :6.050 Median :2243.5 ## Mean :48.01 NA&#39;s :75 Mean :6.114 Mean :2218.3 ## 3rd Qu.:49.00 3rd Qu.:6.875 3rd Qu.:3346.8 ## Max. :62.00 Max. :9.350 Max. :4696.0 ## NA&#39;s :33 ## sbp dbp bmi gender ## Min. : 88.0 Min. : 56.00 Min. :28.99 man :100 ## 1st Qu.:115.0 1st Qu.: 72.00 1st Qu.:36.10 woman:100 ## Median :126.0 Median : 80.00 Median :37.80 ## Mean :130.2 Mean : 82.31 Mean :37.45 ## 3rd Qu.:144.0 3rd Qu.: 92.00 3rd Qu.:39.20 ## Max. :187.0 Max. :120.00 Max. :45.03 ## Mode substitution We need to find the mode, the most frequent level or group in the variable. In race variable, it is malay. table(coroNA$race) ## ## chinese indian malay ## 31 45 49 replace_na will replace the missing values in race with its mode. mode_sub &lt;- coroNA %&gt;% mutate(race = replace_na(race, &quot;malay&quot;)) summary(mode_sub) ## age race chol id cad ## Min. :33.00 chinese: 31 Min. :4.000 Min. : 1.0 cad : 37 ## 1st Qu.:43.00 indian : 45 1st Qu.:5.362 1st Qu.: 901.5 no cad:163 ## Median :48.00 malay :124 Median :6.050 Median :2243.5 ## Mean :48.02 Mean :6.114 Mean :2218.3 ## 3rd Qu.:53.00 3rd Qu.:6.875 3rd Qu.:3346.8 ## Max. :62.00 Max. :9.350 Max. :4696.0 ## NA&#39;s :85 NA&#39;s :33 ## sbp dbp bmi gender ## Min. : 88.0 Min. : 56.00 Min. :28.99 man :100 ## 1st Qu.:115.0 1st Qu.: 72.00 1st Qu.:36.10 woman:100 ## Median :126.0 Median : 80.00 Median :37.80 ## Mean :130.2 Mean : 82.31 Mean :37.45 ## 3rd Qu.:144.0 3rd Qu.: 92.00 3rd Qu.:39.20 ## Max. :187.0 Max. :120.00 Max. :45.03 ## 12.4.3 Single imputation In single imputation, missing data is imputed by any methods producing a single set of a complete dataset. In facts, the simple imputation approaches (mean, median and mode) are part of single imputation techniques. Single imputation is better compared to the previous techniques that we have covered so far. This approach incorporate more information from other variables to impute the missing values. However, this approach produce a result with a small standard error, which reflect a false precision in the result. Additionally, a single imputation approach do not take into account uncertainty about the missing data (except for stochastic regression imputation). Additionally, this approach is applicable if the missing data is at least MAR. Regression imputation We will do a linear regression imputation for numerical variables (age and chol) and multinomial or polytomous logistic regression for categorical variable (since race has three levels; malay, chinese and indian). We run mice() with maxit = 0 (zero iteration) to get a model specification and further removed id variable as it is not useful for the analysis. Here, we do not actually run the imputation yet as the iteration is zero (maxit = 0). ini &lt;- mice(coroNA %&gt;% select(-id), m = 1, maxit = 0) ini ## Class: mids ## Number of multiple imputations: 1 ## Imputation methods: ## age race chol cad sbp dbp bmi gender ## &quot;pmm&quot; &quot;polyreg&quot; &quot;pmm&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## PredictorMatrix: ## age race chol cad sbp dbp bmi gender ## age 0 1 1 1 1 1 1 1 ## race 1 0 1 1 1 1 1 1 ## chol 1 1 0 1 1 1 1 1 ## cad 1 1 1 0 1 1 1 1 ## sbp 1 1 1 1 0 1 1 1 ## dbp 1 1 1 1 1 0 1 1 By default, mice() use predictive mean matching (pmm) for a numerical variable, binary logistic regression (logreg) for a categorical variable with two levels and multinomial or polytomous logistic regression (polyreg) for a categorical variable with more than two level. mice() will not assign any method for variable with non-missing values by default (denote by  in the section of imputation method above). We going to change the method for numerical variable (age and chol) to a linear regression. meth &lt;- ini$method meth[c(1,3)] &lt;- &quot;norm.predict&quot; meth ## age race chol cad sbp ## &quot;norm.predict&quot; &quot;polyreg&quot; &quot;norm.predict&quot; &quot;&quot; &quot;&quot; ## dbp bmi gender ## &quot;&quot; &quot;&quot; &quot;&quot; mice() function contains a few arguments: m: number of imputed sets (will be used in the multiple imputation later) method: to specify a method of imputation predictorMatrix: to specify predictors for imputation printFlag: print history on the R console seed: random number for reproducibility regImp &lt;- mice(coroNA %&gt;% select(-id), m = 1, method = meth, printFlag = F, seed = 123) regImp ## Class: mids ## Number of multiple imputations: 1 ## Imputation methods: ## age race chol cad sbp ## &quot;norm.predict&quot; &quot;polyreg&quot; &quot;norm.predict&quot; &quot;&quot; &quot;&quot; ## dbp bmi gender ## &quot;&quot; &quot;&quot; &quot;&quot; ## PredictorMatrix: ## age race chol cad sbp dbp bmi gender ## age 0 1 1 1 1 1 1 1 ## race 1 0 1 1 1 1 1 1 ## chol 1 1 0 1 1 1 1 1 ## cad 1 1 1 0 1 1 1 1 ## sbp 1 1 1 1 0 1 1 1 ## dbp 1 1 1 1 1 0 1 1 Here, we can see the summary of our imputation model: Number of multiple imputation Imputation methods PredictorMatrix We can further assess the full predictor matrix from the model. regImp$predictorMatrix ## age race chol cad sbp dbp bmi gender ## age 0 1 1 1 1 1 1 1 ## race 1 0 1 1 1 1 1 1 ## chol 1 1 0 1 1 1 1 1 ## cad 1 1 1 0 1 1 1 1 ## sbp 1 1 1 1 0 1 1 1 ## dbp 1 1 1 1 1 0 1 1 ## bmi 1 1 1 1 1 1 0 1 ## gender 1 1 1 1 1 1 1 0 The predictor matrix denotes the variable to be imputed at the left side and the predictors at the top of the column. 1 indicates a predictor while 0 zero indicates a non-predictor. This predictor matrix can be changed accordingly if needed by changing 1 to 0 (a predictor to a non-predictor) or vise versa. Noted that the diagonal is 0 as the variable is not allowed to impute itself. As an example, to impute missing values in age, all of the variables are used as predictors except for age itself as shown in the predictor matrix. So, the outcome variable (\\(\\hat{y}\\)) in linear regression equation will be the imputed variable. \\[ \\hat{y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_kx_k \\] So, the above equation for imputed age variable in the first row of the predictor matrix will be: \\[ age = \\beta_0 + \\beta_1(race) + \\beta_2(chol) + \\beta_3(cad) + \\beta_4(sbp) + \\beta_5(dbp) + \\beta_6(bmi) + \\beta_6(gender) \\] Although it seems that we impute all the missing values in each variable simultaneously, in actuality each imputation model will run separately. So, in our data, on top of the imputation model for age, we have another imputation models for race and chol variables. Fortunately, we do not have to concern much about which variable to be used as a predictor as mice() will automatically select the useful predictors for each variables with missing values. Further information on how mice do this automatic selection can assessed by typing quickpred() in the RStudio console. We can asses the imputed dataset as follows: coro_regImp &lt;- complete(regImp, 1) summary(coro_regImp) ## age race chol cad sbp ## Min. :33.00 chinese:56 Min. :4.000 cad : 37 Min. : 88.0 ## 1st Qu.:41.32 indian :67 1st Qu.:5.445 no cad:163 1st Qu.:115.0 ## Median :47.00 malay :77 Median :6.092 Median :126.0 ## Mean :47.68 Mean :6.144 Mean :130.2 ## 3rd Qu.:53.38 3rd Qu.:6.774 3rd Qu.:144.0 ## Max. :62.00 Max. :9.350 Max. :187.0 ## dbp bmi gender ## Min. : 56.00 Min. :28.99 man :100 ## 1st Qu.: 72.00 1st Qu.:36.10 woman:100 ## Median : 80.00 Median :37.80 ## Mean : 82.31 Mean :37.45 ## 3rd Qu.: 92.00 3rd Qu.:39.20 ## Max. :120.00 Max. :45.03 Stochastic regression imputation Stochastic regression imputation is an extension of regression imputation. This method attempts to account for the missing data uncertainty by adding a noise or extra variance. This method only applicable for numerical variable. First, we get a model specification. ini &lt;- mice(coroNA %&gt;% select(-id), m = 1, maxit = 0) ini ## Class: mids ## Number of multiple imputations: 1 ## Imputation methods: ## age race chol cad sbp dbp bmi gender ## &quot;pmm&quot; &quot;polyreg&quot; &quot;pmm&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## PredictorMatrix: ## age race chol cad sbp dbp bmi gender ## age 0 1 1 1 1 1 1 1 ## race 1 0 1 1 1 1 1 1 ## chol 1 1 0 1 1 1 1 1 ## cad 1 1 1 0 1 1 1 1 ## sbp 1 1 1 1 0 1 1 1 ## dbp 1 1 1 1 1 0 1 1 Then, we specify the imputation method to stochastic regression (norm.nob) for all numerical variables with missing values (age and chol). meth &lt;- ini$method meth[c(1,3)] &lt;- &quot;norm.nob&quot; meth ## age race chol cad sbp dbp bmi ## &quot;norm.nob&quot; &quot;polyreg&quot; &quot;norm.nob&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## gender ## &quot;&quot; Then, we run mice(). srImp &lt;- mice(coroNA %&gt;% select(-id), m = 1, method = meth, printFlag = F, seed = 123) srImp ## Class: mids ## Number of multiple imputations: 1 ## Imputation methods: ## age race chol cad sbp dbp bmi ## &quot;norm.nob&quot; &quot;polyreg&quot; &quot;norm.nob&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## gender ## &quot;&quot; ## PredictorMatrix: ## age race chol cad sbp dbp bmi gender ## age 0 1 1 1 1 1 1 1 ## race 1 0 1 1 1 1 1 1 ## chol 1 1 0 1 1 1 1 1 ## cad 1 1 1 0 1 1 1 1 ## sbp 1 1 1 1 0 1 1 1 ## dbp 1 1 1 1 1 0 1 1 Here is the imputed dataset. coro_srImp &lt;- complete(srImp, 1) summary(coro_srImp) ## age race chol cad sbp ## Min. :31.54 chinese:52 Min. :4.000 cad : 37 Min. : 88.0 ## 1st Qu.:42.16 indian :70 1st Qu.:5.390 no cad:163 1st Qu.:115.0 ## Median :46.70 malay :78 Median :6.105 Median :126.0 ## Mean :47.46 Mean :6.219 Mean :130.2 ## 3rd Qu.:52.25 3rd Qu.:7.005 3rd Qu.:144.0 ## Max. :66.41 Max. :9.600 Max. :187.0 ## dbp bmi gender ## Min. : 56.00 Min. :28.99 man :100 ## 1st Qu.: 72.00 1st Qu.:36.10 woman:100 ## Median : 80.00 Median :37.80 ## Mean : 82.31 Mean :37.45 ## 3rd Qu.: 92.00 3rd Qu.:39.20 ## Max. :120.00 Max. :45.03 Decision tree imputation Decision tree or classification and regression tree (CART) is a popular method in machine learning area. This method can be applied to impute a missing values for both numerical and categorical variables. First, we get a model specification. ini &lt;- mice(coroNA %&gt;% select(-id), m = 1, maxit = 0) ini ## Class: mids ## Number of multiple imputations: 1 ## Imputation methods: ## age race chol cad sbp dbp bmi gender ## &quot;pmm&quot; &quot;polyreg&quot; &quot;pmm&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## PredictorMatrix: ## age race chol cad sbp dbp bmi gender ## age 0 1 1 1 1 1 1 1 ## race 1 0 1 1 1 1 1 1 ## chol 1 1 0 1 1 1 1 1 ## cad 1 1 1 0 1 1 1 1 ## sbp 1 1 1 1 0 1 1 1 ## dbp 1 1 1 1 1 0 1 1 Then, we specify the imputation methods for all variable with missing values (age, race and chol) to decision tree. meth &lt;- ini$method meth[1:3] &lt;- &quot;cart&quot; meth ## age race chol cad sbp dbp bmi gender ## &quot;cart&quot; &quot;cart&quot; &quot;cart&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; Next, we run the imputation model. cartImp &lt;- mice(coroNA %&gt;% select(-id), m = 1, method = meth, printFlag = F, seed = 123) cartImp ## Class: mids ## Number of multiple imputations: 1 ## Imputation methods: ## age race chol cad sbp dbp bmi gender ## &quot;cart&quot; &quot;cart&quot; &quot;cart&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## PredictorMatrix: ## age race chol cad sbp dbp bmi gender ## age 0 1 1 1 1 1 1 1 ## race 1 0 1 1 1 1 1 1 ## chol 1 1 0 1 1 1 1 1 ## cad 1 1 1 0 1 1 1 1 ## sbp 1 1 1 1 0 1 1 1 ## dbp 1 1 1 1 1 0 1 1 Here is the imputed dataset. coro_cartImp &lt;- complete(cartImp, 1) summary(coro_cartImp) ## age race chol cad sbp ## Min. :33.00 chinese:56 Min. :4.000 cad : 37 Min. : 88.0 ## 1st Qu.:42.50 indian :70 1st Qu.:5.390 no cad:163 1st Qu.:115.0 ## Median :47.00 malay :74 Median :6.050 Median :126.0 ## Mean :47.92 Mean :6.085 Mean :130.2 ## 3rd Qu.:53.00 3rd Qu.:6.765 3rd Qu.:144.0 ## Max. :62.00 Max. :9.350 Max. :187.0 ## dbp bmi gender ## Min. : 56.00 Min. :28.99 man :100 ## 1st Qu.: 72.00 1st Qu.:36.10 woman:100 ## Median : 80.00 Median :37.80 ## Mean : 82.31 Mean :37.45 ## 3rd Qu.: 92.00 3rd Qu.:39.20 ## Max. :120.00 Max. :45.03 12.4.4 Multiple imputation Multiple imputation is an advanced approach to missing data. In this approach, several imputed datasets will be generated. Analyses will be run on each imputed datasets. Then, all the results will be combined into a pooled result. The advantage of multiple imputation is this approach takes into account uncertainty regarding the missing data, by which the single imputation approach may fail to do. Generally, this approach is applicable if the missing data is at least MAR. Flow of the multiple imputation is quite similar to the single imputation since we are using the same package. The number of imputed set, m by default is set to 5 in mice(). In general, the higher number of m is better, though this makes the computation longer. For moderate amount of missing data, m between 5 to 20 should be enough. Another recommendation is to set m to the average percentage of missing data. However, for this example we going to run the default values. miImp &lt;- mice(coroNA %&gt;% select(-id), m = 5, printFlag = F, seed = 123) miImp ## Class: mids ## Number of multiple imputations: 5 ## Imputation methods: ## age race chol cad sbp dbp bmi gender ## &quot;pmm&quot; &quot;polyreg&quot; &quot;pmm&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## PredictorMatrix: ## age race chol cad sbp dbp bmi gender ## age 0 1 1 1 1 1 1 1 ## race 1 0 1 1 1 1 1 1 ## chol 1 1 0 1 1 1 1 1 ## cad 1 1 1 0 1 1 1 1 ## sbp 1 1 1 1 0 1 1 1 ## dbp 1 1 1 1 1 0 1 1 We can see in the result, the number of imputations is 5. We can extract the first imputation set as follows: complete(miImp, 1) %&gt;% summary() ## age race chol cad sbp ## Min. :33.00 chinese:50 Min. :4.000 cad : 37 Min. : 88.0 ## 1st Qu.:43.00 indian :72 1st Qu.:5.170 no cad:163 1st Qu.:115.0 ## Median :48.00 malay :78 Median :5.968 Median :126.0 ## Mean :47.94 Mean :5.993 Mean :130.2 ## 3rd Qu.:53.00 3rd Qu.:6.703 3rd Qu.:144.0 ## Max. :62.00 Max. :9.350 Max. :187.0 ## dbp bmi gender ## Min. : 56.00 Min. :28.99 man :100 ## 1st Qu.: 72.00 1st Qu.:36.10 woman:100 ## Median : 80.00 Median :37.80 ## Mean : 82.31 Mean :37.45 ## 3rd Qu.: 92.00 3rd Qu.:39.20 ## Max. :120.00 Max. :45.03 Next, we need to check for convergence of the algorithm. plot(miImp) The line in the plot should be intermingled and free of any trend. The number of iteration can be further increased to make sure of this. miImp2 &lt;- mice.mids(miImp, maxit = 35, printFlag = F) plot(miImp2) Once the imputed datasets are obtained, an analysis (for example, a linear regression) can be run as follows: lr_mi &lt;- with(miImp, glm(dbp ~ age + race + chol + cad + sbp + bmi + gender)) pool(lr_mi) %&gt;% summary(conf.int = T) ## term estimate std.error statistic df p.value ## 1 (Intercept) 25.85805530 13.21741793 1.95636209 34.41840 0.05857610 ## 2 age -0.09649575 0.21229624 -0.45453348 10.34947 0.65882968 ## 3 raceindian -0.07034050 2.57995900 -0.02726419 12.32421 0.97868554 ## 4 racemalay 0.20029789 1.68514315 0.11886105 59.44295 0.90578621 ## 5 chol 1.33244955 0.52966992 2.51562245 89.92925 0.01365737 ## 6 cadno cad -1.43834422 1.43322822 -1.00356956 160.05725 0.31710092 ## 7 sbp 0.51358079 0.03015353 17.03219634 186.95165 0.00000000 ## 8 bmi -0.35959227 0.20258274 -1.77503907 136.19191 0.07812525 ## 9 genderwoman 1.30467626 1.04972267 1.24287709 185.09416 0.21548528 ## 2.5 % 97.5 % ## 1 -0.9909476 52.70705817 ## 2 -0.5673651 0.37437357 ## 3 -5.6752310 5.53454998 ## 4 -3.1711401 3.57173588 ## 5 0.2801565 2.38474258 ## 6 -4.2688212 1.39213272 ## 7 0.4540959 0.57306569 ## 8 -0.7602069 0.04102233 ## 9 -0.7662831 3.37563562 We can use mutate_if() from dplyr package to round up the numbers to two decimal points. pool(lr_mi) %&gt;% summary(conf.int = T) %&gt;% as.data.frame() %&gt;% mutate_if(is.numeric, round, 2) ## term estimate std.error statistic df p.value 2.5 % 97.5 % ## 1 (Intercept) 25.86 13.22 1.96 34.42 0.06 -0.99 52.71 ## 2 age -0.10 0.21 -0.45 10.35 0.66 -0.57 0.37 ## 3 raceindian -0.07 2.58 -0.03 12.32 0.98 -5.68 5.53 ## 4 racemalay 0.20 1.69 0.12 59.44 0.91 -3.17 3.57 ## 5 chol 1.33 0.53 2.52 89.93 0.01 0.28 2.38 ## 6 cadno cad -1.44 1.43 -1.00 160.06 0.32 -4.27 1.39 ## 7 sbp 0.51 0.03 17.03 186.95 0.00 0.45 0.57 ## 8 bmi -0.36 0.20 -1.78 136.19 0.08 -0.76 0.04 ## 9 genderwoman 1.30 1.05 1.24 185.09 0.22 -0.77 3.38 mice package has provided an easy flow to run the analysis for multiple imputation: mice(): impute the missing data with(): run a statistical analysis pool(): pool the results Additionally, a model comparison can be done as well. There are three methods available for the model comparison: D1(): multivariate Wald test D2(): pools test statistics from each analysis of the imputed datasets D3(): likelihood-ratio test statistics D2() is less powerful compared to the other two methods. lr_mi2 &lt;- with(miImp, glm(dbp ~ race + chol + cad + sbp + bmi + gender)) summary(D1(lr_mi, lr_mi2)) ## ## Models: ## model formula ## 1 dbp ~ age + race + chol + cad + sbp + bmi + gender ## 2 dbp ~ race + chol + cad + sbp + bmi + gender ## ## Comparisons: ## test statistic df1 df2 dfcom p.value riv ## 1 ~~ 2 0.2066007 1 4 191 0.6730188 1.379672 ## ## Number of imputations: 5 Method D1 Multivariate Wald test is not significant. Hence, removing age from the model does not reduce its predictive power. We can safely exclude age variable to aim for a parsimonious model. summary(D2(lr_mi, lr_mi2)) ## ## Models: ## model formula ## 1 dbp ~ age + race + chol + cad + sbp + bmi + gender ## 2 dbp ~ race + chol + cad + sbp + bmi + gender ## ## Comparisons: ## test statistic df1 df2 dfcom p.value riv ## 1 ~~ 2 -0.002254427 1 16.1953 NA 1 0.9879772 ## ## Number of imputations: 5 Method D2 (wald) summary(D3(lr_mi, lr_mi2)) ## ## Models: ## model formula ## 1 dbp ~ age + race + chol + cad + sbp + bmi + gender ## 2 dbp ~ race + chol + cad + sbp + bmi + gender ## ## Comparisons: ## test statistic df1 df2 dfcom p.value riv ## 1 ~~ 2 0.1946719 1 10.83101 191 0.6677344 1.549127 ## ## Number of imputations: 5 Method D3 Also, we get a similar result from the remaining two methods for the model comparison. However, this may not always be the case. Generally, D1() and D3() are preferred and equally good for a sample size more than 200. However, for a small sample size (n &lt; 200), D1() is better. Besides, D2() should be used with cautious especially in large datasets with many missing values as it may produce a false positive estimate. 12.5 Presentation gtsummary package can be used for all the analyses run on the approaches of handling missing data that we have covered in this chapter. Here is an example to get a nice table using tbl_regression() from a linear regression model run on the multiple imputation approach. tbl_regression(lr_mi2) html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #okvtewmmhk .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #okvtewmmhk .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #okvtewmmhk .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #okvtewmmhk .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; border-top-color: #FFFFFF; border-top-width: 0; } #okvtewmmhk .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #okvtewmmhk .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #okvtewmmhk .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #okvtewmmhk .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #okvtewmmhk .gt_column_spanner_outer:first-child { padding-left: 0; } #okvtewmmhk .gt_column_spanner_outer:last-child { padding-right: 0; } #okvtewmmhk .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #okvtewmmhk .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; } #okvtewmmhk .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #okvtewmmhk .gt_from_md > :first-child { margin-top: 0; } #okvtewmmhk .gt_from_md > :last-child { margin-bottom: 0; } #okvtewmmhk .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #okvtewmmhk .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; } #okvtewmmhk .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #okvtewmmhk .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; } #okvtewmmhk .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #okvtewmmhk .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #okvtewmmhk .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #okvtewmmhk .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #okvtewmmhk .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #okvtewmmhk .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; } #okvtewmmhk .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #okvtewmmhk .gt_sourcenote { font-size: 90%; padding: 4px; } #okvtewmmhk .gt_left { text-align: left; } #okvtewmmhk .gt_center { text-align: center; } #okvtewmmhk .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #okvtewmmhk .gt_font_normal { font-weight: normal; } #okvtewmmhk .gt_font_bold { font-weight: bold; } #okvtewmmhk .gt_font_italic { font-style: italic; } #okvtewmmhk .gt_super { font-size: 65%; } #okvtewmmhk .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 65%; } Characteristic Beta 95% CI1 p-value race chinese   indian -0.86 -4.1, 2.3 0.6 malay 0.73 -2.0, 3.5 0.6 chol 1.3 0.29, 2.3 0.012 cad cad   no cad -1.3 -4.0, 1.5 0.4 sbp 0.51 0.45, 0.57 bmi -0.36 -0.77, 0.04 0.076 gender man   woman 1.4 -0.67, 3.4 0.2 1 CI = Confidence Interval 12.6 References "],["references-5.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
